{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f6d2c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9159718d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\SOFTWARE\\Anaconda3\\lib\\site-packages\\gym\\core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "D:\\SOFTWARE\\Anaconda3\\lib\\site-packages\\gym\\wrappers\\step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "numberOfActions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "670f116c",
   "metadata": {},
   "outputs": [],
   "source": [
    "actionSpace = [0, 1, 2]\n",
    "numberOfStates = 20 ** 2\n",
    "GAMMA = 0.9\n",
    "ALPHA = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "294b078b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createBins():\n",
    "\n",
    "    bins = np.zeros((2, 20))\n",
    "    bins[0] = np.linspace(-1.20, 0.6, 20)\n",
    "    bins[1] = np.linspace(-0.07, 0.07, 20)\n",
    "    return bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa543096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assignBins(observation, bins):\n",
    "    state = np.zeros(2)\n",
    "    for i in range(2):\n",
    "        state[i] = np.digitize(observation[i], bins[i])\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fceb6d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_dict(d):\n",
    "    max_v = float('-inf')\n",
    "    for key, val in d.items():\n",
    "        if val > max_v:\n",
    "            max_v = val\n",
    "            max_key = key\n",
    "    return max_key, max_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24d315c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initilizing the action value function\n",
    "def initQ():\n",
    "  Q = {}\n",
    "\n",
    "  for i in range(20):\n",
    "    for j in range(20):\n",
    "      Q[(i,j)] = {}\n",
    "      for action in range(env.action_space.n):\n",
    "        Q[(i,j)][action] = float(0)\n",
    "  return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c60c7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initilize a random deterministic target policy\n",
    "def initTargetPolicy():\n",
    "  pi = {}\n",
    "\n",
    "  for i in range(20):\n",
    "    for j in range(20):\n",
    "      pi[(i,j)]= np.random.choice(actionSpace)\n",
    "  return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f7c7af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initReturns():\n",
    "    returns = {}\n",
    "    \n",
    "    for i in range(20):\n",
    "        for j in range(20):\n",
    "            returns[(i,j)] = {}\n",
    "            for action in range(env.action_space.n):\n",
    "                returns[(i,j)][action] = []\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "46ad3910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Expected_SARSA(env,Q, bins,eps):\n",
    "    obs = env.reset()\n",
    "    state = tuple(assignBins(obs, bins))\n",
    "    # prev_screen = env.render(mode='rgb_array')\n",
    "    # plt.imshow(prev_screen)\n",
    "    count = 0\n",
    "    complete = 0\n",
    "    while True:\n",
    "        count += 1\n",
    "        if np.random.uniform() < eps:\n",
    "            action = env.action_space.sample()  # epsilon greedy\n",
    "        else:\n",
    "            action = max_dict(Q[state])[0]\n",
    "\n",
    "        obs, reward, done, info = env.step(action)\n",
    "\n",
    "\n",
    "        if done == True:\n",
    "            if count < 200:\n",
    "                reward = 1000\n",
    "                complete = 1\n",
    "\n",
    "        expected_q = 0\n",
    "        greedy_actions = 0\n",
    "        new_state = tuple(assignBins(obs, bins))\n",
    "        max_action, max_q = max_dict(Q[new_state])\n",
    "        \n",
    "        for i in actionSpace:\n",
    "            if Q[new_state][i] == max_q:\n",
    "                greedy_actions += 1\n",
    "\n",
    "        non_greedy_action_probability = eps / len(actionSpace)\n",
    "        greedy_action_probability = ((1 - eps) / greedy_actions) + non_greedy_action_probability\n",
    "\n",
    "        for i in actionSpace:\n",
    "            if Q[new_state][i] == max_q:\n",
    "                expected_q += Q[new_state][i] * greedy_action_probability\n",
    "            else:\n",
    "                expected_q += Q[new_state][i] * non_greedy_action_probability\n",
    "\n",
    "\n",
    "        Q[state][action] += ALPHA * (reward + GAMMA * expected_q - Q[state][action])\n",
    "        state = new_state\n",
    "        if done == True:\n",
    "            break;\n",
    "    return complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "60c93653",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_and_plot_parameters(cumulative_completion, training_episodes):\n",
    "    title = f'total episodes vs completed episodes'\n",
    "    print(f'Evaluated {title}')\n",
    "    line, = plt.plot(\n",
    "        np.arange(1, training_episodes + 1),\n",
    "        cumulative_completion,\n",
    "        label=title)\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "086fa1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = createBins()\n",
    "Q = initQ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c4c9b8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "SARSA_completeList = []\n",
    "completed = 0\n",
    "eps = 0.05\n",
    "mem = 0\n",
    "training_episodes = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f1f3af73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 100, and completed over past 100 episode 0\n",
      "i = 200, and completed over past 100 episode 0\n",
      "i = 300, and completed over past 100 episode 0\n",
      "i = 400, and completed over past 100 episode 0\n",
      "i = 500, and completed over past 100 episode 0\n",
      "i = 600, and completed over past 100 episode 0\n",
      "i = 700, and completed over past 100 episode 0\n",
      "i = 800, and completed over past 100 episode 0\n",
      "i = 900, and completed over past 100 episode 1\n",
      "i = 1000, and completed over past 100 episode 0\n",
      "i = 1100, and completed over past 100 episode 2\n",
      "i = 1200, and completed over past 100 episode 2\n",
      "i = 1300, and completed over past 100 episode 2\n",
      "i = 1400, and completed over past 100 episode 1\n",
      "i = 1500, and completed over past 100 episode 46\n",
      "i = 1600, and completed over past 100 episode 25\n",
      "i = 1700, and completed over past 100 episode 21\n",
      "i = 1800, and completed over past 100 episode 6\n",
      "i = 1900, and completed over past 100 episode 32\n",
      "i = 2000, and completed over past 100 episode 74\n",
      "i = 2100, and completed over past 100 episode 77\n",
      "i = 2200, and completed over past 100 episode 63\n",
      "i = 2300, and completed over past 100 episode 26\n",
      "i = 2400, and completed over past 100 episode 45\n",
      "i = 2500, and completed over past 100 episode 49\n",
      "i = 2600, and completed over past 100 episode 91\n",
      "i = 2700, and completed over past 100 episode 84\n",
      "i = 2800, and completed over past 100 episode 80\n",
      "i = 2900, and completed over past 100 episode 65\n",
      "i = 3000, and completed over past 100 episode 36\n",
      "i = 3100, and completed over past 100 episode 79\n",
      "i = 3200, and completed over past 100 episode 77\n",
      "i = 3300, and completed over past 100 episode 89\n",
      "i = 3400, and completed over past 100 episode 89\n",
      "i = 3500, and completed over past 100 episode 83\n",
      "i = 3600, and completed over past 100 episode 88\n",
      "i = 3700, and completed over past 100 episode 51\n",
      "i = 3800, and completed over past 100 episode 68\n",
      "i = 3900, and completed over past 100 episode 48\n",
      "i = 4000, and completed over past 100 episode 65\n",
      "i = 4100, and completed over past 100 episode 23\n",
      "i = 4200, and completed over past 100 episode 74\n",
      "i = 4300, and completed over past 100 episode 93\n",
      "i = 4400, and completed over past 100 episode 96\n",
      "i = 4500, and completed over past 100 episode 98\n",
      "i = 4600, and completed over past 100 episode 93\n",
      "i = 4700, and completed over past 100 episode 92\n",
      "i = 4800, and completed over past 100 episode 89\n",
      "i = 4900, and completed over past 100 episode 86\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'onPolicy_SARSA_completeList' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [25]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m     completed \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m complete\n\u001b[0;32m      8\u001b[0m     SARSA_completeList\u001b[38;5;241m.\u001b[39mappend(completed)\n\u001b[1;32m---> 10\u001b[0m evaluate_and_plot_parameters(\u001b[43monPolicy_SARSA_completeList\u001b[49m, training_episodes)\n\u001b[0;32m     11\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'onPolicy_SARSA_completeList' is not defined"
     ]
    }
   ],
   "source": [
    "for i in range(training_episodes):\n",
    "    complete = Expected_SARSA(env, Q, bins, eps)\n",
    "    if i % 100 == 0 and i != 0:\n",
    "        print(\"i = \" + str(i) + \", and completed over past 100 episode \" + str(completed - mem))\n",
    "        mem = completed\n",
    "\n",
    "    completed += complete\n",
    "    SARSA_completeList.append(completed)\n",
    "\n",
    "evaluate_and_plot_parameters(SARSA_completeList, training_episodes)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7ff9620c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.92\n"
     ]
    }
   ],
   "source": [
    "eps = 0\n",
    "avg = 0\n",
    "for i in range(100):\n",
    "    complete = Expected_SARSA(env, Q, bins, eps)\n",
    "    avg += complete\n",
    "print(avg / 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ee8c5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
