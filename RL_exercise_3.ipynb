{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VvHe_zMDvNT"
   },
   "source": [
    "**Hruday Vishal Kanna Anand**\n",
    "\n",
    "**1006874517**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "1XDvssQd64Pf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The system cannot find the path specified.\n"
     ]
    }
   ],
   "source": [
    "!apt-get install -y xvfb python-opengl > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "F5esgX013vPe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The system cannot find the path specified.\n"
     ]
    }
   ],
   "source": [
    "!pip install gym pyvirtualdisplay > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Qbi2xaFo31Sj"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display as ipythondisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FGqXqJxoAsHG",
    "outputId": "76c66fed-d07a-401f-ea36-9b4901c8a8b7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyvirtualdisplay.display.Display at 0x7f79f803a590>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(400, 300))\n",
    "display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "id": "6L4YayzR4FYj",
    "outputId": "6c0d530b-ea13-441a-cb94-6813268f3162"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations that were run: 11\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#starter code\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "env.reset()\n",
    "prev_screen = env.render(mode='rgb_array')\n",
    "plt.imshow(prev_screen)\n",
    "\n",
    "for i in range(5000):\n",
    "  action = env.action_space.sample()\n",
    "  print(\"step i\",i,\"action=\",action)\n",
    "  obs, reward, done, info = env.step(action)\n",
    "  print(\"obs=\",obs,\"reward=\",reward,\"done=\",done,\"info=\",info)\n",
    "  screen = env.render(mode='rgb_array')\n",
    "  \n",
    "  plt.imshow(screen)\n",
    "  ipythondisplay.clear_output(wait=True)\n",
    "  ipythondisplay.display(plt.gcf())\n",
    "  \n",
    "  if done:\n",
    "    break\n",
    "    \n",
    "ipythondisplay.clear_output(wait=True)\n",
    "env.close()\n",
    "print(\"Iterations that were run:\",i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "09RlWdRlm1wz"
   },
   "outputs": [],
   "source": [
    "#defining the action space and the number of states and actions\n",
    "actionSpace = [0,1]\n",
    "numStates = 11 ** 4\n",
    "numActions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Otp_w3QO-75b"
   },
   "outputs": [],
   "source": [
    "# descretizing the observation space so it is easier for the model to handle the problem\n",
    "def createBins():\n",
    "  bins = np.zeros((4, 10))\n",
    "  bins[0] = np.linspace(-1.5, 1.5, 10)\n",
    "  bins[1] = np.linspace(-2.5, 2.5, 10)\n",
    "  bins[2] = np.linspace(-.5, .5, 10)\n",
    "  bins[3] = np.linspace(-3, 3, 10)\n",
    "\n",
    "  return bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "AIqlaDop_kK4"
   },
   "outputs": [],
   "source": [
    "# function to assign a observation to a perticular descrete state\n",
    "def assignBins(obs, bins):\n",
    "  state = np.zeros(4)\n",
    "  for i in range(4):\n",
    "    state[i] = np.digitize(obs[i], bins[i])\n",
    "  return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "qoAEsYvRGQpK"
   },
   "outputs": [],
   "source": [
    "#initilizing the action value function\n",
    "def initQ():\n",
    "  Q = {}\n",
    "\n",
    "  for i in range(11):\n",
    "    for j in range(11):\n",
    "      for k in range(11):\n",
    "        for l in range(11):\n",
    "\n",
    "          Q[(i,j,k,l)] = {}\n",
    "          for action in range(env.action_space.n):\n",
    "            Q[(i,j,k,l)][action] = float(0)\n",
    "  return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "vM6csDjyGTfw"
   },
   "outputs": [],
   "source": [
    "#initilize a random deterministic target policy\n",
    "def initTargetPolicy():\n",
    "  pi = {}\n",
    "\n",
    "  for i in range(11):\n",
    "    for j in range(11):\n",
    "      for k in range(11):\n",
    "        for l in range(11):\n",
    "          pi[(i,j,k,l)]= np.random.choice(actionSpace)\n",
    "  return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ML3iK2Qn8DVx"
   },
   "outputs": [],
   "source": [
    "#initilize a behaviour soft policy based on the target policy\n",
    "def initBehaviourPolicy(epsilon,pi):\n",
    "  b = {}\n",
    "\n",
    "  for i in range(11):\n",
    "    for j in range(11):\n",
    "      for k in range(11):\n",
    "        for l in range(11):\n",
    "          #if np.random.choice(actionSpace, p=[0.9,0.1])==0:\n",
    "            if pi[(i,j,k,l)]==0:  \n",
    "              b[(i,j,k,l)]= [1-epsilon,epsilon]\n",
    "            else:\n",
    "              b[(i,j,k,l)]= [epsilon,1-epsilon]\n",
    "            '''else:\n",
    "            if pi[(i,j,k,l)]==0:\n",
    "              b[(i,j,k,l)]= [epsilon,1-epsilon]\n",
    "            else:'''\n",
    "\n",
    "  return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "e44J4iH0BO5F"
   },
   "outputs": [],
   "source": [
    "bins= createBins()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "j25IEXqQM7Gh"
   },
   "outputs": [],
   "source": [
    "q= initQ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "iIrLrbE102Sk"
   },
   "outputs": [],
   "source": [
    "pi=initTargetPolicy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "NfTnuURxAaFj"
   },
   "outputs": [],
   "source": [
    "b=initBehaviourPolicy(0.1,pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "BM9oidyN1Kav"
   },
   "outputs": [],
   "source": [
    "# A function to handle the running of a single episode it maintains a lists of states, actions and rewards that it returns \n",
    "def episode(env, policy,bins):\n",
    "  obs = env.reset()\n",
    "  rewards = []\n",
    "  actions = []\n",
    "  states = []\n",
    "  while True:\n",
    "    state = tuple(assignBins(obs, bins))\n",
    "    action = np.random.choice(actionSpace, p=policy[state])\n",
    "    actions.append(action)\n",
    "    states.append(state)\n",
    "\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    rewards.append(reward)\n",
    "    if done == True:\n",
    "        break\n",
    "\n",
    "  return states, actions, rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FRgRvnW6MXfB"
   },
   "source": [
    "## a. On-policy SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vLBiLsOTXMvf",
    "outputId": "6fda79ce-215d-47a8-de3b-e50f3ac0a6cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0\n",
      "step i 0 action= 1 \n",
      "\n",
      "Next observation= [-0.00136394  0.22394651 -0.04525426 -0.32896236] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 1 action= 1 \n",
      "\n",
      "Next observation= [ 0.00311499  0.4196825  -0.05183351 -0.63556594] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 2 action= 1 \n",
      "\n",
      "Next observation= [ 0.01150864  0.61548764 -0.06454483 -0.9441114 ] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 3 action= 1 \n",
      "\n",
      "Next observation= [ 0.02381839  0.8114169  -0.08342706 -1.2563564 ] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 4 action= 1 \n",
      "\n",
      "Next observation= [ 0.04004673  1.0075018  -0.10855418 -1.57396   ] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 5 action= 1 \n",
      "\n",
      "Next observation= [ 0.06019676  1.2037381  -0.14003338 -1.8984338 ] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 6 action= 1 \n",
      "\n",
      "Next observation= [ 0.08427153  1.4000715  -0.17800206 -2.2310867 ] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 7 action= 0 \n",
      "\n",
      "Next observation= [ 0.11227296  1.2070317  -0.2226238  -1.9981589 ] reward= 1.0 done= True info= {'TimeLimit.truncated': False} \n",
      "\n",
      "episode: 1\n",
      "step i 0 action= 1 \n",
      "\n",
      "Next observation= [-0.04693015  0.18629952 -0.03289926 -0.33419755] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 1 action= 0 \n",
      "\n",
      "Next observation= [-0.04320415 -0.00833913 -0.03958321 -0.05206821] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 2 action= 0 \n",
      "\n",
      "Next observation= [-0.04337094 -0.2028718  -0.04062457  0.22786781] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 3 action= 1 \n",
      "\n",
      "Next observation= [-0.04742837 -0.00719355 -0.03606721 -0.07734774] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 4 action= 1 \n",
      "\n",
      "Next observation= [-0.04757224  0.18842639 -0.03761417 -0.38118827] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 5 action= 0 \n",
      "\n",
      "Next observation= [-0.04380372 -0.00614183 -0.04523794 -0.10059834] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 6 action= 1 \n",
      "\n",
      "Next observation= [-0.04392655  0.18959828 -0.0472499  -0.40720356] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 7 action= 0 \n",
      "\n",
      "Next observation= [-0.04013459 -0.00482295 -0.05539397 -0.12978348] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 8 action= 1 \n",
      "\n",
      "Next observation= [-0.04023105  0.19104697 -0.05798964 -0.4394152 ] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 9 action= 0 \n",
      "\n",
      "Next observation= [-0.03641011 -0.00320837 -0.06677794 -0.1655614 ] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 10 action= 0 \n",
      "\n",
      "Next observation= [-0.03647428 -0.19731401 -0.07008918  0.10533001] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 11 action= 0 \n",
      "\n",
      "Next observation= [-0.04042055 -0.3913652  -0.06798258  0.37510276] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 12 action= 1 \n",
      "\n",
      "Next observation= [-0.04824786 -0.19534679 -0.06048052  0.06178284] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 13 action= 0 \n",
      "\n",
      "Next observation= [-0.05215479 -0.3895518  -0.05924486  0.33478728] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 14 action= 1 \n",
      "\n",
      "Next observation= [-0.05994583 -0.19363889 -0.05254912  0.02402572] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 15 action= 1 \n",
      "\n",
      "Next observation= [-0.06381861  0.00219574 -0.0520686  -0.28476307] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 16 action= 1 \n",
      "\n",
      "Next observation= [-0.06377469  0.19802016 -0.05776386 -0.59340286] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 17 action= 0 \n",
      "\n",
      "Next observation= [-0.05981429  0.00375233 -0.06963192 -0.31946027] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 18 action= 0 \n",
      "\n",
      "Next observation= [-0.05973924 -0.19031242 -0.07602113 -0.04952382] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 19 action= 0 \n",
      "\n",
      "Next observation= [-0.0635455  -0.38426661 -0.0770116   0.21823855] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 20 action= 1 \n",
      "\n",
      "Next observation= [-0.07123082 -0.1881331  -0.07264683 -0.09770879] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 21 action= 0 \n",
      "\n",
      "Next observation= [-0.07499348 -0.38214275 -0.07460101  0.1711987 ] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 22 action= 1 \n",
      "\n",
      "Next observation= [-0.08263634 -0.1860367  -0.07117704 -0.14405455] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 23 action= 0 \n",
      "\n",
      "Next observation= [-0.08635707 -0.38007095 -0.07405812  0.12535149] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 24 action= 1 \n",
      "\n",
      "Next observation= [-0.0939585  -0.18397047 -0.07155109 -0.18974616] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 25 action= 0 \n",
      "\n",
      "Next observation= [-0.09763791 -0.37799972 -0.07534602  0.07953496] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 26 action= 1 \n",
      "\n",
      "Next observation= [-0.1051979  -0.18188307 -0.07375532 -0.23593618] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 27 action= 0 \n",
      "\n",
      "Next observation= [-0.10883556 -0.375878   -0.07847404  0.03260071] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 28 action= 1 \n",
      "\n",
      "Next observation= [-0.11635312 -0.17972353 -0.07782203 -0.2837732 ] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 29 action= 0 \n",
      "\n",
      "Next observation= [-0.11994759 -0.37365425 -0.08349749 -0.01661415] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 30 action= 1 \n",
      "\n",
      "Next observation= [-0.12742068 -0.17744026 -0.08382978 -0.3344295 ] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 31 action= 0 \n",
      "\n",
      "Next observation= [-0.13096948 -0.37127528 -0.09051836 -0.06931507] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 32 action= 1 \n",
      "\n",
      "Next observation= [-0.13839498 -0.17498004 -0.09190466 -0.38912857] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 33 action= 0 \n",
      "\n",
      "Next observation= [-0.1418946  -0.36868554 -0.09968724 -0.12677851] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 34 action= 1 \n",
      "\n",
      "Next observation= [-0.1492683  -0.17228727 -0.10222281 -0.44917285] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 35 action= 1 \n",
      "\n",
      "Next observation= [-0.15271404  0.02412078 -0.11120626 -0.7722482 ] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 36 action= 0 \n",
      "\n",
      "Next observation= [-0.15223163 -0.16930968 -0.12665123 -0.51652205] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 37 action= 0 \n",
      "\n",
      "Next observation= [-0.15561782 -0.36244205 -0.13698167 -0.26627985] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 38 action= 1 \n",
      "\n",
      "Next observation= [-0.16286667 -0.165658   -0.14230727 -0.5988377 ] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 39 action= 0 \n",
      "\n",
      "Next observation= [-0.16617982 -0.35853222 -0.15428402 -0.35414812] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 40 action= 0 \n",
      "\n",
      "Next observation= [-0.17335047 -0.5511624  -0.16136698 -0.11381476] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 41 action= 1 \n",
      "\n",
      "Next observation= [-0.18437372 -0.35414037 -0.16364329 -0.45274472] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 42 action= 0 \n",
      "\n",
      "Next observation= [-0.19145653 -0.5466159  -0.17269817 -0.21578522] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 43 action= 0 \n",
      "\n",
      "Next observation= [-0.20238884 -0.7389024  -0.17701387  0.01783278] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 44 action= 1 \n",
      "\n",
      "Next observation= [-0.21716689 -0.5417419  -0.17665721 -0.32505742] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 45 action= 1 \n",
      "\n",
      "Next observation= [-0.22800173 -0.3446024  -0.18315837 -0.6678319 ] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 46 action= 1 \n",
      "\n",
      "Next observation= [-0.23489377 -0.14746982 -0.19651501 -1.0121328 ] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 47 action= 1 \n",
      "\n",
      "Next observation= [-0.23784317  0.04965342 -0.21675767 -1.3595308 ] reward= 1.0 done= True info= {'TimeLimit.truncated': False} \n",
      "\n",
      "episode: 2\n",
      "step i 0 action= 1 \n",
      "\n",
      "Next observation= [-0.00523971  0.20937158 -0.00360936 -0.2508915 ] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 1 action= 1 \n",
      "\n",
      "Next observation= [-0.00105228  0.4045449  -0.00862719 -0.5447107 ] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 2 action= 1 \n",
      "\n",
      "Next observation= [ 0.00703862  0.599787   -0.0195214  -0.84009933] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 3 action= 1 \n",
      "\n",
      "Next observation= [ 0.01903436  0.79516995 -0.03632339 -1.1388569 ] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 4 action= 0 \n",
      "\n",
      "Next observation= [ 0.03493775  0.6005413  -0.05910053 -0.8577832 ] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 5 action= 1 \n",
      "\n",
      "Next observation= [ 0.04694858  0.79641646 -0.07625619 -1.1684484 ] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 6 action= 0 \n",
      "\n",
      "Next observation= [ 0.06287691  0.60236484 -0.09962516 -0.90061444] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 7 action= 1 \n",
      "\n",
      "Next observation= [ 0.07492421  0.7986853  -0.11763745 -1.2228764 ] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 8 action= 0 \n",
      "\n",
      "Next observation= [ 0.09089791  0.60525864 -0.14209497 -0.96924734] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 9 action= 1 \n",
      "\n",
      "Next observation= [ 0.10300308  0.8019727  -0.16147992 -1.30298   ] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 10 action= 1 \n",
      "\n",
      "Next observation= [ 0.11904254  0.998732   -0.18753952 -1.6415483 ] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 11 action= 0 \n",
      "\n",
      "Next observation= [ 0.13901718  0.80623645 -0.22037049 -1.4126818 ] reward= 1.0 done= True info= {'TimeLimit.truncated': False} \n",
      "\n",
      "episode: 3\n",
      "step i 0 action= 1 \n",
      "\n",
      "Next observation= [-0.02221981  0.18578622 -0.02897152 -0.2972193 ] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 1 action= 1 \n",
      "\n",
      "Next observation= [-0.01850409  0.38130894 -0.03491591 -0.59889674] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 2 action= 1 \n",
      "\n",
      "Next observation= [-0.01087791  0.57690156 -0.04689384 -0.90237004] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 3 action= 1 \n",
      "\n",
      "Next observation= [ 6.6012068e-04  7.7262634e-01 -6.4941242e-02 -1.2094163e+00] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 4 action= 1 \n",
      "\n",
      "Next observation= [ 0.01611265  0.96852404 -0.08912957 -1.5217227 ] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 5 action= 1 \n",
      "\n",
      "Next observation= [ 0.03548313  1.1646028  -0.11956402 -1.8408426 ] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 6 action= 1 \n",
      "\n",
      "Next observation= [ 0.05877518  1.3608245  -0.15638088 -2.168142  ] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 7 action= 0 \n",
      "\n",
      "Next observation= [ 0.08599167  1.1675377  -0.19974372 -1.9275385 ] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 8 action= 1 \n",
      "\n",
      "Next observation= [ 0.10934243  1.364163   -0.23829448 -2.2749474 ] reward= 1.0 done= True info= {'TimeLimit.truncated': False} \n",
      "\n",
      "episode: 4\n",
      "step i 0 action= 1 \n",
      "\n",
      "Next observation= [ 0.00888675  0.15497625 -0.03246423 -0.34757474] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 1 action= 0 \n",
      "\n",
      "Next observation= [ 0.01198627 -0.03966927 -0.03941572 -0.06530312] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 2 action= 0 \n",
      "\n",
      "Next observation= [ 0.01119289 -0.23420459 -0.04072179  0.21468799] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 3 action= 0 \n",
      "\n",
      "Next observation= [ 0.0065088  -0.42872143 -0.03642803  0.49425244] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 4 action= 0 \n",
      "\n",
      "Next observation= [-0.00206563 -0.6233112  -0.02654298  0.775236  ] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 5 action= 0 \n",
      "\n",
      "Next observation= [-0.01453186 -0.8180582  -0.01103826  1.0594509 ] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 6 action= 0 \n",
      "\n",
      "Next observation= [-0.03089302 -1.0130322   0.01015076  1.3486489 ] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 7 action= 1 \n",
      "\n",
      "Next observation= [-0.05115366 -0.81803924  0.03712374  1.0591588 ] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 8 action= 0 \n",
      "\n",
      "Next observation= [-0.06751445 -1.0136328   0.05830691  1.3632588 ] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 9 action= 1 \n",
      "\n",
      "Next observation= [-0.08778711 -0.8192878   0.08557209  1.0893693 ] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 10 action= 1 \n",
      "\n",
      "Next observation= [-0.10417286 -0.6253918   0.10735948  0.8247169 ] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 11 action= 0 \n",
      "\n",
      "Next observation= [-0.1166807  -0.8218055   0.12385382  1.1491443 ] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 12 action= 1 \n",
      "\n",
      "Next observation= [-0.13311681 -0.6284984   0.1468367   0.8977248 ] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 13 action= 0 \n",
      "\n",
      "Next observation= [-0.14568678 -0.8252728   0.1647912   1.2327251 ] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 14 action= 1 \n",
      "\n",
      "Next observation= [-0.16219223 -0.6326078   0.1894457   0.9958725 ] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 15 action= 1 \n",
      "\n",
      "Next observation= [-0.17484438 -0.44045463  0.20936315  0.768164  ] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 16 action= 1 \n",
      "\n",
      "Next observation= [-0.18365347 -0.24873461  0.22472644  0.54796773] reward= 1.0 done= True info= {'TimeLimit.truncated': False} \n",
      "\n",
      "episode: 5\n",
      "step i 0 action= 1 \n",
      "\n",
      "Next observation= [ 0.04766948  0.15557316 -0.00860489 -0.33683726] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 1 action= 0 \n",
      "\n",
      "Next observation= [ 0.05078094 -0.03942529 -0.01534164 -0.04688024] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 2 action= 1 \n",
      "\n",
      "Next observation= [ 0.04999243  0.15591325 -0.01627924 -0.34436384] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 3 action= 0 \n",
      "\n",
      "Next observation= [ 0.0531107  -0.03897338 -0.02316652 -0.05685851] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 4 action= 1 \n",
      "\n",
      "Next observation= [ 0.05233124  0.15647295 -0.02430369 -0.3567597 ] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 5 action= 0 \n",
      "\n",
      "Next observation= [ 0.05546069 -0.03829519 -0.03143888 -0.07183833] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 6 action= 1 \n",
      "\n",
      "Next observation= [ 0.05469479  0.15726306 -0.03287565 -0.37427226] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 7 action= 0 \n",
      "\n",
      "Next observation= [ 0.05784005 -0.03737686 -0.0403611  -0.09213384] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 8 action= 1 \n",
      "\n",
      "Next observation= [ 0.05709251  0.15829965 -0.04220377 -0.39727253] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 9 action= 1 \n",
      "\n",
      "Next observation= [ 0.06025851  0.35399416 -0.05014922 -0.7029571 ] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 10 action= 1 \n",
      "\n",
      "Next observation= [ 0.06733839  0.54977393 -0.06420837 -1.0109953 ] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 11 action= 1 \n",
      "\n",
      "Next observation= [ 0.07833387  0.74569124 -0.08442827 -1.3231299 ] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 12 action= 1 \n",
      "\n",
      "Next observation= [ 0.09324769  0.9417722  -0.11089087 -1.6409962 ] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 13 action= 1 \n",
      "\n",
      "Next observation= [ 0.11208314  1.1380049  -0.14371079 -1.9660724 ] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 14 action= 1 \n",
      "\n",
      "Next observation= [ 0.13484323  1.3343248  -0.18303224 -2.2996225 ] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 15 action= 1 \n",
      "\n",
      "Next observation= [ 0.16152973  1.5305988  -0.2290247  -2.642627  ] reward= 1.0 done= True info= {'TimeLimit.truncated': False} \n",
      "\n",
      "episode: 6\n",
      "step i 0 action= 1 \n",
      "\n",
      "Next observation= [ 0.03862407  0.2067213  -0.0353603  -0.2601276 ] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 1 action= 1 \n",
      "\n",
      "Next observation= [ 0.04275849  0.4023297  -0.04056285 -0.56375057] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 2 action= 1 \n",
      "\n",
      "Next observation= [ 0.05080509  0.59799665 -0.05183787 -0.8689318 ] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 3 action= 1 \n",
      "\n",
      "Next observation= [ 0.06276502  0.7937841  -0.0692165  -1.1774518 ] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 4 action= 1 \n",
      "\n",
      "Next observation= [ 0.07864071  0.9897334  -0.09276554 -1.4910055 ] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 5 action= 1 \n",
      "\n",
      "Next observation= [ 0.09843537  1.1858541  -0.12258565 -1.8111556 ] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 6 action= 1 \n",
      "\n",
      "Next observation= [ 0.12215246  1.3821105  -0.15880877 -2.139281  ] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 7 action= 0 \n",
      "\n",
      "Next observation= [ 0.14979467  1.1888756  -0.20159438 -1.8995697 ] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 8 action= 1 \n",
      "\n",
      "Next observation= [ 0.17357217  1.3855292  -0.23958577 -2.2474446 ] reward= 1.0 done= True info= {'TimeLimit.truncated': False} \n",
      "\n",
      "episode: 7\n",
      "step i 0 action= 1 \n",
      "\n",
      "Next observation= [ 0.03843489  0.22559373 -0.02815187 -0.27665073] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 1 action= 1 \n",
      "\n",
      "Next observation= [ 0.04294676  0.42110577 -0.03368488 -0.57807815] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 2 action= 1 \n",
      "\n",
      "Next observation= [ 0.05136888  0.61668324 -0.04524644 -0.8811794 ] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 3 action= 1 \n",
      "\n",
      "Next observation= [ 0.06370255  0.8123896  -0.06287003 -1.1877365 ] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 4 action= 1 \n",
      "\n",
      "Next observation= [ 0.07995034  1.0082678  -0.08662476 -1.4994448 ] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 5 action= 1 \n",
      "\n",
      "Next observation= [ 0.10011569  1.2043288  -0.11661366 -1.8178694 ] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 6 action= 1 \n",
      "\n",
      "Next observation= [ 0.12420227  1.4005386  -0.15297104 -2.144392  ] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 7 action= 1 \n",
      "\n",
      "Next observation= [ 0.15221304  1.5968032  -0.19585888 -2.4801495 ] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 8 action= 0 \n",
      "\n",
      "Next observation= [ 0.1841491   1.4037858  -0.24546188 -2.253374  ] reward= 1.0 done= True info= {'TimeLimit.truncated': False} \n",
      "\n",
      "episode: 8\n",
      "step i 0 action= 1 \n",
      "\n",
      "Next observation= [-0.04024832  0.1748731  -0.04603596 -0.33489037] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 1 action= 0 \n",
      "\n",
      "Next observation= [-0.03675086 -0.01956447 -0.05273376 -0.0570728 ] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 2 action= 1 \n",
      "\n",
      "Next observation= [-0.03714215  0.17627238 -0.05387522 -0.36591625] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 3 action= 0 \n",
      "\n",
      "Next observation= [-0.0336167  -0.01804422 -0.06119354 -0.09069592] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 4 action= 0 \n",
      "\n",
      "Next observation= [-0.03397758 -0.2122381  -0.06300746  0.18207   ] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 5 action= 0 \n",
      "\n",
      "Next observation= [-0.03822235 -0.40640453 -0.05936606  0.45422977] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 6 action= 1 \n",
      "\n",
      "Next observation= [-0.04635043 -0.21049559 -0.05028147  0.14344068] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 7 action= 1 \n",
      "\n",
      "Next observation= [-0.05056035 -0.01469095 -0.04741265 -0.16467161] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 8 action= 1 \n",
      "\n",
      "Next observation= [-0.05085417  0.18107653 -0.05070608 -0.47192693] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 9 action= 1 \n",
      "\n",
      "Next observation= [-0.04723264  0.37687662 -0.06014462 -0.7801508 ] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 10 action= 1 \n",
      "\n",
      "Next observation= [-0.0396951   0.57277155 -0.07574764 -1.0911337 ] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 11 action= 1 \n",
      "\n",
      "Next observation= [-0.02823967  0.7688057  -0.09757032 -1.4065902 ] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 12 action= 1 \n",
      "\n",
      "Next observation= [-0.01286356  0.9649938  -0.12570211 -1.7281129 ] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 13 action= 0 \n",
      "\n",
      "Next observation= [ 0.00643632  0.7715129  -0.16026437 -1.4770406 ] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 14 action= 1 \n",
      "\n",
      "Next observation= [ 0.02186657  0.968188   -0.1898052  -1.815189  ] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 15 action= 1 \n",
      "\n",
      "Next observation= [ 0.04123033  1.1648484  -0.22610897 -2.1603503 ] reward= 1.0 done= True info= {'TimeLimit.truncated': False} \n",
      "\n",
      "episode: 9\n",
      "step i 0 action= 1 \n",
      "\n",
      "Next observation= [ 0.00969925  0.2294264  -0.04609015 -0.32378024] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 1 action= 1 \n",
      "\n",
      "Next observation= [ 0.01428778  0.4251733  -0.05256575 -0.6306345 ] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 2 action= 1 \n",
      "\n",
      "Next observation= [ 0.02279125  0.6209878  -0.06517845 -0.9393977 ] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 3 action= 1 \n",
      "\n",
      "Next observation= [ 0.035211   0.816925  -0.0839664 -1.2518283] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 4 action= 0 \n",
      "\n",
      "Next observation= [ 0.0515495   0.62297314 -0.10900296 -0.9865827 ] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 5 action= 0 \n",
      "\n",
      "Next observation= [ 0.06400897  0.42946637 -0.12873462 -0.7300286 ] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 6 action= 1 \n",
      "\n",
      "Next observation= [ 0.07259829  0.6261104  -0.1433352  -1.0602974 ] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 7 action= 1 \n",
      "\n",
      "Next observation= [ 0.0851205   0.8228096  -0.16454114 -1.3943168 ] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 8 action= 1 \n",
      "\n",
      "Next observation= [ 0.10157669  1.019552   -0.19242747 -1.7336017 ] reward= 1.0 done= False info= {} \n",
      "\n",
      "step i 9 action= 1 \n",
      "\n",
      "Next observation= [ 0.12196773  1.2162788  -0.22709951 -2.0794706 ] reward= 1.0 done= True info= {'TimeLimit.truncated': False} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "actions=[]\n",
    "states=[]\n",
    "rewards=[]\n",
    "\n",
    "#constants chosen by us\n",
    "num_episodes=10\n",
    "alpha = 0.5\n",
    "gamma = 0.7\n",
    "\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "  obs = env.reset()\n",
    "  actions=[]\n",
    "  states=[]\n",
    "  rewards=[]\n",
    "  state = tuple(assignBins(obs, bins))\n",
    "  action = np.random.choice(actionSpace, p=b[state])\n",
    "  print(\"episode:\",i_episode)\n",
    "\n",
    "  for i in range(1000):\n",
    "    # Take a step\n",
    "    states.append(state)\n",
    "    print(\"step i\",i,\"action=\",action,\"\\n\")\n",
    "    actions.append(action)\n",
    "    next_obs, reward, done, info = env.step(action)\n",
    "    print(\"Next observation=\",next_obs,\"reward=\",reward,\"done=\",done,\"info=\",info,\"\\n\")\n",
    "    rewards.append(reward)\n",
    "    next_state=tuple(assignBins(next_obs, bins))\n",
    "    \n",
    "    next_action = np.random.choice(actionSpace, p=b[next_state])\n",
    "    \n",
    "    \n",
    "    # TD Update\n",
    "    td_target = reward + gamma * q[next_state][next_action]\n",
    "    td_delta = td_target - q[state][action]\n",
    "    q[state][action] += alpha * td_delta\n",
    "\n",
    "\n",
    "    if done:\n",
    "        break\n",
    "        \n",
    "    action = next_action\n",
    "    state = next_state   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9IHOqY7GR5Qm"
   },
   "source": [
    "## b. Off-policy Q learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IqZ-F_kEhzET",
    "outputId": "ce33fcf2-920f-4594-98a8-dfc79b0615d5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\SOFTWARE\\Anaconda3\\lib\\site-packages\\gym\\core.py:49: DeprecationWarning: \u001b[33mWARN: You are calling render method, but you didn't specified the argument render_mode at environment initialization. To maintain backward compatibility, the environment will render in human mode.\n",
      "If you want to render in human mode, initialize the environment in this way: gym.make('EnvName', render_mode='human') and don't call the render method.\n",
      "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step i 0 action= 1 \n",
      "\n",
      "Next observation= [-0.02520478  0.21160643  0.04565999 -0.29999405] reward= 1.0 done= False \n",
      "\n",
      "step i 1 action= 1 \n",
      "\n",
      "Next observation= [-0.02097265  0.40604883  0.03966011 -0.57793427] reward= 1.0 done= False \n",
      "\n",
      "step i 2 action= 1 \n",
      "\n",
      "Next observation= [-0.01285168  0.60059315  0.02810142 -0.85786426] reward= 1.0 done= False \n",
      "\n",
      "step i 3 action= 1 \n",
      "\n",
      "Next observation= [-8.3981425e-04  7.9532123e-01  1.0944136e-02 -1.1415803e+00] reward= 1.0 done= False \n",
      "\n",
      "step i 4 action= 1 \n",
      "\n",
      "Next observation= [ 0.01506661  0.9902984  -0.01188747 -1.4308112 ] reward= 1.0 done= False \n",
      "\n",
      "step i 5 action= 1 \n",
      "\n",
      "Next observation= [ 0.03487258  1.1855651  -0.04050369 -1.7271852 ] reward= 1.0 done= False \n",
      "\n",
      "step i 6 action= 0 \n",
      "\n",
      "Next observation= [ 0.05858388  0.9909289  -0.0750474  -1.4473752 ] reward= 1.0 done= False \n",
      "\n",
      "step i 7 action= 1 \n",
      "\n",
      "Next observation= [ 0.07840246  1.1868893  -0.1039949  -1.7625316 ] reward= 1.0 done= False \n",
      "\n",
      "step i 8 action= 1 \n",
      "\n",
      "Next observation= [ 0.10214024  1.3830227  -0.13924554 -2.0856616 ] reward= 1.0 done= False \n",
      "\n",
      "step i 9 action= 1 \n",
      "\n",
      "Next observation= [ 0.12980069  1.5792502  -0.18095876 -2.4179602 ] reward= 1.0 done= False \n",
      "\n",
      "step i 10 action= 0 \n",
      "\n",
      "Next observation= [ 0.1613857   1.3860984  -0.22931796 -2.1858752 ] reward= 1.0 done= True \n",
      "\n",
      "Iteration No: 1 TimeSteps: 10  Success: 0 Best Q: 1 Learning rate: 0.5 Total reward: 11.0\n",
      "Episode 1 terminated after 10 Iterations \n",
      "step i 0 action= 1 \n",
      "\n",
      "Next observation= [ 0.02602025  0.19757733  0.03489972 -0.3064122 ] reward= 1.0 done= False \n",
      "\n",
      "step i 1 action= 1 \n",
      "\n",
      "Next observation= [ 0.0299718   0.39218503  0.02877147 -0.5878876 ] reward= 1.0 done= False \n",
      "\n",
      "step i 2 action= 1 \n",
      "\n",
      "Next observation= [ 0.0378155   0.5868925   0.01701372 -0.87137026] reward= 1.0 done= False \n",
      "\n",
      "step i 3 action= 1 \n",
      "\n",
      "Next observation= [ 4.9553353e-02  7.8177899e-01 -4.1368694e-04 -1.1586559e+00] reward= 1.0 done= False \n",
      "\n",
      "step i 4 action= 0 \n",
      "\n",
      "Next observation= [ 0.06518893  0.5866624  -0.0235868  -0.8661027 ] reward= 1.0 done= False \n",
      "\n",
      "step i 5 action= 1 \n",
      "\n",
      "Next observation= [ 0.07692218  0.7820973  -0.04090886 -1.1661073 ] reward= 1.0 done= False \n",
      "\n",
      "step i 6 action= 0 \n",
      "\n",
      "Next observation= [ 0.09256413  0.5875309  -0.064231   -0.8865258 ] reward= 1.0 done= False \n",
      "\n",
      "step i 7 action= 1 \n",
      "\n",
      "Next observation= [ 0.10431474  0.78346324 -0.08196152 -1.1986891 ] reward= 1.0 done= False \n",
      "\n",
      "step i 8 action= 1 \n",
      "\n",
      "Next observation= [ 0.11998401  0.97954446 -0.10593531 -1.5158933 ] reward= 1.0 done= False \n",
      "\n",
      "step i 9 action= 1 \n",
      "\n",
      "Next observation= [ 0.1395749   1.1757767  -0.13625316 -1.8396784 ] reward= 1.0 done= False \n",
      "\n",
      "step i 10 action= 1 \n",
      "\n",
      "Next observation= [ 0.16309044  1.3721151  -0.17304674 -2.171391  ] reward= 1.0 done= False \n",
      "\n",
      "step i 11 action= 1 \n",
      "\n",
      "Next observation= [ 0.19053273  1.5684516  -0.21647456 -2.5121195 ] reward= 1.0 done= True \n",
      "\n",
      "Iteration No: 2 TimeSteps: 11  Success: 0 Best Q: 1 Learning rate: 0.5 Total reward: 12.0\n",
      "Episode 2 terminated after 11 Iterations \n",
      "step i 0 action= 1 \n",
      "\n",
      "Next observation= [ 0.04326977  0.18489487 -0.01457196 -0.31919074] reward= 1.0 done= False \n",
      "\n",
      "step i 1 action= 0 \n",
      "\n",
      "Next observation= [ 0.04696767 -0.01001655 -0.02095577 -0.03113865] reward= 1.0 done= False \n",
      "\n",
      "step i 2 action= 1 \n",
      "\n",
      "Next observation= [ 0.04676734  0.18539956 -0.02157855 -0.33035898] reward= 1.0 done= False \n",
      "\n",
      "step i 3 action= 1 \n",
      "\n",
      "Next observation= [ 0.05047533  0.3808219  -0.02818573 -0.6297679 ] reward= 1.0 done= False \n",
      "\n",
      "step i 4 action= 1 \n",
      "\n",
      "Next observation= [ 0.05809177  0.5763256  -0.04078108 -0.93119246] reward= 1.0 done= False \n",
      "\n",
      "step i 5 action= 1 \n",
      "\n",
      "Next observation= [ 0.06961828  0.7719735  -0.05940494 -1.2364066 ] reward= 1.0 done= False \n",
      "\n",
      "step i 6 action= 1 \n",
      "\n",
      "Next observation= [ 0.08505775  0.96780634 -0.08413307 -1.5470924 ] reward= 1.0 done= False \n",
      "\n",
      "step i 7 action= 1 \n",
      "\n",
      "Next observation= [ 0.10441387  1.1638317  -0.11507492 -1.8647964 ] reward= 1.0 done= False \n",
      "\n",
      "step i 8 action= 1 \n",
      "\n",
      "Next observation= [ 0.12769051  1.3600106  -0.15237084 -2.190876  ] reward= 1.0 done= False \n",
      "\n",
      "step i 9 action= 1 \n",
      "\n",
      "Next observation= [ 0.15489072  1.5562426  -0.19618836 -2.5264375 ] reward= 1.0 done= False \n",
      "\n",
      "step i 10 action= 0 \n",
      "\n",
      "Next observation= [ 0.18601558  1.3631855  -0.24671711 -2.2997172 ] reward= 1.0 done= True \n",
      "\n",
      "Iteration No: 3 TimeSteps: 10  Success: 0 Best Q: 1 Learning rate: 0.5 Total reward: 11.0\n",
      "Episode 3 terminated after 10 Iterations \n",
      "step i 0 action= 1 \n",
      "\n",
      "Next observation= [-0.04881884  0.14538072  0.00746757 -0.25540537] reward= 1.0 done= False \n",
      "\n",
      "step i 1 action= 1 \n",
      "\n",
      "Next observation= [-0.04591122  0.34039527  0.00235946 -0.54572356] reward= 1.0 done= False \n",
      "\n",
      "step i 2 action= 1 \n",
      "\n",
      "Next observation= [-0.03910332  0.53548396 -0.00855501 -0.83766216] reward= 1.0 done= False \n",
      "\n",
      "step i 3 action= 1 \n",
      "\n",
      "Next observation= [-0.02839364  0.7307217  -0.02530826 -1.1330231 ] reward= 1.0 done= False \n",
      "\n",
      "step i 4 action= 0 \n",
      "\n",
      "Next observation= [-0.0137792   0.53594    -0.04796872 -0.848384  ] reward= 1.0 done= False \n",
      "\n",
      "step i 5 action= 1 \n",
      "\n",
      "Next observation= [-0.0030604   0.73168224 -0.0649364  -1.1557571 ] reward= 1.0 done= False \n",
      "\n",
      "step i 6 action= 1 \n",
      "\n",
      "Next observation= [ 0.01157324  0.9275879  -0.08805154 -1.4680741 ] reward= 1.0 done= False \n",
      "\n",
      "step i 7 action= 1 \n",
      "\n",
      "Next observation= [ 0.030125    1.1236703  -0.11741302 -1.786912  ] reward= 1.0 done= False \n",
      "\n",
      "step i 8 action= 1 \n",
      "\n",
      "Next observation= [ 0.05259841  1.3198987  -0.15315126 -2.1136682 ] reward= 1.0 done= False \n",
      "\n",
      "step i 9 action= 1 \n",
      "\n",
      "Next observation= [ 0.07899638  1.5161837  -0.19542463 -2.4495    ] reward= 1.0 done= False \n",
      "\n",
      "step i 10 action= 0 \n",
      "\n",
      "Next observation= [ 0.10932005  1.3231895  -0.24441463 -2.222609  ] reward= 1.0 done= True \n",
      "\n",
      "Iteration No: 4 TimeSteps: 10  Success: 0 Best Q: 1 Learning rate: 0.5 Total reward: 11.0\n",
      "Episode 4 terminated after 10 Iterations \n",
      "step i 0 action= 1 \n",
      "\n",
      "Next observation= [-0.023021    0.22649217  0.03164266 -0.2508596 ] reward= 1.0 done= False \n",
      "\n",
      "step i 1 action= 0 \n",
      "\n",
      "Next observation= [-0.01849115  0.03093297  0.02662547  0.05163373] reward= 1.0 done= False \n",
      "\n",
      "step i 2 action= 1 \n",
      "\n",
      "Next observation= [-0.0178725   0.22566323  0.02765814 -0.23253116] reward= 1.0 done= False \n",
      "\n",
      "step i 3 action= 1 \n",
      "\n",
      "Next observation= [-0.01335923  0.42037928  0.02300752 -0.5163631 ] reward= 1.0 done= False \n",
      "\n",
      "step i 4 action= 1 \n",
      "\n",
      "Next observation= [-0.00495164  0.6151698   0.01268026 -0.801708  ] reward= 1.0 done= False \n",
      "\n",
      "step i 5 action= 1 \n",
      "\n",
      "Next observation= [ 0.00735175  0.81011564 -0.0033539  -1.0903752 ] reward= 1.0 done= False \n",
      "\n",
      "step i 6 action= 0 \n",
      "\n",
      "Next observation= [ 0.02355406  0.61503804 -0.02516141 -0.7987465 ] reward= 1.0 done= False \n",
      "\n",
      "step i 7 action= 1 \n",
      "\n",
      "Next observation= [ 0.03585482  0.810496   -0.04113634 -1.0992373 ] reward= 1.0 done= False \n",
      "\n",
      "step i 8 action= 0 \n",
      "\n",
      "Next observation= [ 0.05206474  0.6159389  -0.06312109 -0.8197393 ] reward= 1.0 done= False \n",
      "\n",
      "step i 9 action= 1 \n",
      "\n",
      "Next observation= [ 0.06438352  0.8118653  -0.07951587 -1.1315889 ] reward= 1.0 done= False \n",
      "\n",
      "step i 10 action= 1 \n",
      "\n",
      "Next observation= [ 0.08062083  1.007933   -0.10214765 -1.4481143 ] reward= 1.0 done= False \n",
      "\n",
      "step i 11 action= 1 \n",
      "\n",
      "Next observation= [ 0.10077949  1.2041519  -0.13110994 -1.7708875 ] reward= 1.0 done= False \n",
      "\n",
      "step i 12 action= 1 \n",
      "\n",
      "Next observation= [ 0.12486253  1.4004872  -0.16652769 -2.1012988 ] reward= 1.0 done= False \n",
      "\n",
      "step i 13 action= 1 \n",
      "\n",
      "Next observation= [ 0.15287226  1.5968447  -0.20855366 -2.4404936 ] reward= 1.0 done= False \n",
      "\n",
      "step i 14 action= 0 \n",
      "\n",
      "Next observation= [ 0.18480916  1.4040294  -0.25736353 -2.218409  ] reward= 1.0 done= True \n",
      "\n",
      "Iteration No: 5 TimeSteps: 14  Success: 0 Best Q: 1 Learning rate: 0.5 Total reward: 15.0\n",
      "Episode 5 terminated after 14 Iterations \n",
      "step i 0 action= 1 \n",
      "\n",
      "Next observation= [ 0.03622602  0.23916773  0.01537425 -0.31758973] reward= 1.0 done= False \n",
      "\n",
      "step i 1 action= 1 \n",
      "\n",
      "Next observation= [ 0.04100937  0.43406737  0.00902245 -0.60538477] reward= 1.0 done= False \n",
      "\n",
      "step i 2 action= 1 \n",
      "\n",
      "Next observation= [ 0.04969072  0.629062   -0.00308525 -0.8952123 ] reward= 1.0 done= False \n",
      "\n",
      "step i 3 action= 0 \n",
      "\n",
      "Next observation= [ 0.06227196  0.433982   -0.02098949 -0.6035007 ] reward= 1.0 done= False \n",
      "\n",
      "step i 4 action= 1 \n",
      "\n",
      "Next observation= [ 0.0709516   0.62939113 -0.0330595  -0.90272033] reward= 1.0 done= False \n",
      "\n",
      "step i 5 action= 1 \n",
      "\n",
      "Next observation= [ 0.08353943  0.824945   -0.05111391 -1.2056085 ] reward= 1.0 done= False \n",
      "\n",
      "step i 6 action= 0 \n",
      "\n",
      "Next observation= [ 0.10003832  0.63051945 -0.07522608 -0.9293721 ] reward= 1.0 done= False \n",
      "\n",
      "step i 7 action= 1 \n",
      "\n",
      "Next observation= [ 0.11264871  0.8265718  -0.09381352 -1.2447146 ] reward= 1.0 done= False \n",
      "\n",
      "step i 8 action= 1 \n",
      "\n",
      "Next observation= [ 0.12918015  1.0227637  -0.11870781 -1.5652492 ] reward= 1.0 done= False \n",
      "\n",
      "step i 9 action= 1 \n",
      "\n",
      "Next observation= [ 0.14963542  1.2190877  -0.1500128  -1.8924807 ] reward= 1.0 done= False \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step i 10 action= 1 \n",
      "\n",
      "Next observation= [ 0.17401718  1.4154857  -0.18786241 -2.2277079 ] reward= 1.0 done= False \n",
      "\n",
      "step i 11 action= 0 \n",
      "\n",
      "Next observation= [ 0.2023269   1.2225832  -0.23241657 -1.9983522 ] reward= 1.0 done= True \n",
      "\n",
      "Iteration No: 6 TimeSteps: 11  Success: 0 Best Q: 1 Learning rate: 0.5 Total reward: 12.0\n",
      "Episode 6 terminated after 11 Iterations \n",
      "step i 0 action= 0 \n",
      "\n",
      "Next observation= [ 0.03780091 -0.18443191 -0.04946313  0.28336358] reward= 1.0 done= False \n",
      "\n",
      "step i 1 action= 1 \n",
      "\n",
      "Next observation= [ 0.03411227  0.01135936 -0.04379586 -0.02450038] reward= 1.0 done= False \n",
      "\n",
      "step i 2 action= 1 \n",
      "\n",
      "Next observation= [ 0.03433946  0.20708112 -0.04428586 -0.3306734 ] reward= 1.0 done= False \n",
      "\n",
      "step i 3 action= 1 \n",
      "\n",
      "Next observation= [ 0.03848108  0.40280458 -0.05089933 -0.6369865 ] reward= 1.0 done= False \n",
      "\n",
      "step i 4 action= 1 \n",
      "\n",
      "Next observation= [ 0.04653718  0.598598   -0.06363906 -0.94525427] reward= 1.0 done= False \n",
      "\n",
      "step i 5 action= 1 \n",
      "\n",
      "Next observation= [ 0.05850913  0.79451674 -0.08254415 -1.2572347 ] reward= 1.0 done= False \n",
      "\n",
      "step i 6 action= 1 \n",
      "\n",
      "Next observation= [ 0.07439947  0.9905925  -0.10768884 -1.5745873 ] reward= 1.0 done= False \n",
      "\n",
      "step i 7 action= 1 \n",
      "\n",
      "Next observation= [ 0.09421132  1.1868211  -0.13918059 -1.8988245 ] reward= 1.0 done= False \n",
      "\n",
      "step i 8 action= 1 \n",
      "\n",
      "Next observation= [ 0.11794774  1.3831489  -0.17715707 -2.2312558 ] reward= 1.0 done= False \n",
      "\n",
      "step i 9 action= 0 \n",
      "\n",
      "Next observation= [ 0.14561072  1.1900976  -0.2217822  -1.998023  ] reward= 1.0 done= True \n",
      "\n",
      "Iteration No: 7 TimeSteps: 9  Success: 0 Best Q: 1 Learning rate: 0.5 Total reward: 10.0\n",
      "Episode 7 terminated after 9 Iterations \n",
      "step i 0 action= 1 \n",
      "\n",
      "Next observation= [ 0.0259546   0.16356063  0.02983999 -0.24660134] reward= 1.0 done= False \n",
      "\n",
      "step i 1 action= 1 \n",
      "\n",
      "Next observation= [ 0.02922582  0.35824397  0.02490796 -0.5297247 ] reward= 1.0 done= False \n",
      "\n",
      "step i 2 action= 1 \n",
      "\n",
      "Next observation= [ 0.0363907   0.5530068   0.01431347 -0.81445616] reward= 1.0 done= False \n",
      "\n",
      "step i 3 action= 1 \n",
      "\n",
      "Next observation= [ 0.04745083  0.7479299  -0.00197566 -1.1026028 ] reward= 1.0 done= False \n",
      "\n",
      "step i 4 action= 0 \n",
      "\n",
      "Next observation= [ 0.06240943  0.552834   -0.02402771 -0.8105403 ] reward= 1.0 done= False \n",
      "\n",
      "step i 5 action= 1 \n",
      "\n",
      "Next observation= [ 0.07346611  0.7482767  -0.04023852 -1.1106833 ] reward= 1.0 done= False \n",
      "\n",
      "step i 6 action= 0 \n",
      "\n",
      "Next observation= [ 0.08843164  0.5537059  -0.06245219 -0.8308902 ] reward= 1.0 done= False \n",
      "\n",
      "step i 7 action= 1 \n",
      "\n",
      "Next observation= [ 0.09950576  0.74962324 -0.07906999 -1.1425424 ] reward= 1.0 done= False \n",
      "\n",
      "step i 8 action= 1 \n",
      "\n",
      "Next observation= [ 0.11449823  0.9456844  -0.10192084 -1.4589375 ] reward= 1.0 done= False \n",
      "\n",
      "step i 9 action= 1 \n",
      "\n",
      "Next observation= [ 0.13341191  1.141898   -0.1310996  -1.7816435 ] reward= 1.0 done= False \n",
      "\n",
      "step i 10 action= 1 \n",
      "\n",
      "Next observation= [ 0.15624987  1.3382283  -0.16673246 -2.1120448 ] reward= 1.0 done= False \n",
      "\n",
      "step i 11 action= 1 \n",
      "\n",
      "Next observation= [ 0.18301444  1.5345794  -0.20897336 -2.4512796 ] reward= 1.0 done= False \n",
      "\n",
      "step i 12 action= 1 \n",
      "\n",
      "Next observation= [ 0.21370603  1.7307779  -0.25799894 -2.8001666 ] reward= 1.0 done= True \n",
      "\n",
      "Iteration No: 8 TimeSteps: 12  Success: 0 Best Q: 1 Learning rate: 0.5 Total reward: 13.0\n",
      "Episode 8 terminated after 12 Iterations \n",
      "step i 0 action= 1 \n",
      "\n",
      "Next observation= [-0.00230631  0.21141434 -0.00825999 -0.26624677] reward= 1.0 done= False \n",
      "\n",
      "step i 1 action= 1 \n",
      "\n",
      "Next observation= [ 0.00192198  0.4066532  -0.01358493 -0.5615235 ] reward= 1.0 done= False \n",
      "\n",
      "step i 2 action= 1 \n",
      "\n",
      "Next observation= [ 0.01005504  0.60196316 -0.0248154  -0.8584552 ] reward= 1.0 done= False \n",
      "\n",
      "step i 3 action= 1 \n",
      "\n",
      "Next observation= [ 0.0220943  0.7974142 -0.0419845 -1.1588365] reward= 1.0 done= False \n",
      "\n",
      "step i 4 action= 0 \n",
      "\n",
      "Next observation= [ 0.03804259  0.6028637  -0.06516123 -0.8796078 ] reward= 1.0 done= False \n",
      "\n",
      "step i 5 action= 1 \n",
      "\n",
      "Next observation= [ 0.05009986  0.7988076  -0.08275338 -1.1920437 ] reward= 1.0 done= False \n",
      "\n",
      "step i 6 action= 1 \n",
      "\n",
      "Next observation= [ 0.06607601  0.9948983  -0.10659426 -1.509475  ] reward= 1.0 done= False \n",
      "\n",
      "step i 7 action= 1 \n",
      "\n",
      "Next observation= [ 0.08597398  1.1911384  -0.13678376 -1.8334438 ] reward= 1.0 done= False \n",
      "\n",
      "step i 8 action= 1 \n",
      "\n",
      "Next observation= [ 0.10979675  1.3874834  -0.17345263 -2.1652994 ] reward= 1.0 done= False \n",
      "\n",
      "step i 9 action= 1 \n",
      "\n",
      "Next observation= [ 0.13754642  1.583826   -0.21675862 -2.5061338 ] reward= 1.0 done= True \n",
      "\n",
      "Iteration No: 9 TimeSteps: 9  Success: 0 Best Q: 1 Learning rate: 0.5 Total reward: 10.0\n",
      "Episode 9 terminated after 9 Iterations \n",
      "step i 0 action= 0 \n",
      "\n",
      "Next observation= [ 0.03266889 -0.22962579  0.01766573  0.29160374] reward= 1.0 done= False \n",
      "\n",
      "step i 1 action= 1 \n",
      "\n",
      "Next observation= [ 0.02807637 -0.03476013  0.0234978   0.0045443 ] reward= 1.0 done= False \n",
      "\n",
      "step i 2 action= 1 \n",
      "\n",
      "Next observation= [ 0.02738117  0.16001709  0.02358869 -0.28063315] reward= 1.0 done= False \n",
      "\n",
      "step i 3 action= 1 \n",
      "\n",
      "Next observation= [ 0.03058152  0.35479474  0.01797603 -0.5657839 ] reward= 1.0 done= False \n",
      "\n",
      "step i 4 action= 1 \n",
      "\n",
      "Next observation= [ 0.03767741  0.54965997  0.00666035 -0.8527499 ] reward= 1.0 done= False \n",
      "\n",
      "step i 5 action= 1 \n",
      "\n",
      "Next observation= [ 0.04867061  0.7446905  -0.01039465 -1.143331  ] reward= 1.0 done= False \n",
      "\n",
      "step i 6 action= 0 \n",
      "\n",
      "Next observation= [ 0.06356442  0.54970586 -0.03326127 -0.8539259 ] reward= 1.0 done= False \n",
      "\n",
      "step i 7 action= 1 \n",
      "\n",
      "Next observation= [ 0.07455853  0.745265   -0.05033978 -1.1568793 ] reward= 1.0 done= False \n",
      "\n",
      "step i 8 action= 0 \n",
      "\n",
      "Next observation= [ 0.08946384  0.5508341  -0.07347737 -0.8803961 ] reward= 1.0 done= False \n",
      "\n",
      "step i 9 action= 0 \n",
      "\n",
      "Next observation= [ 0.10048052  0.35678315 -0.09108529 -0.61168796] reward= 1.0 done= False \n",
      "\n",
      "step i 10 action= 1 \n",
      "\n",
      "Next observation= [ 0.10761618  0.5530522  -0.10331906 -0.93161315] reward= 1.0 done= False \n",
      "\n",
      "step i 11 action= 1 \n",
      "\n",
      "Next observation= [ 0.11867723  0.7494052  -0.12195132 -1.2548938 ] reward= 1.0 done= False \n",
      "\n",
      "step i 12 action= 1 \n",
      "\n",
      "Next observation= [ 0.13366532  0.9458592  -0.14704919 -1.5831512 ] reward= 1.0 done= False \n",
      "\n",
      "step i 13 action= 1 \n",
      "\n",
      "Next observation= [ 0.15258251  1.1423928  -0.17871222 -1.9178468 ] reward= 1.0 done= False \n",
      "\n",
      "step i 14 action= 1 \n",
      "\n",
      "Next observation= [ 0.17543037  1.3389312  -0.21706915 -2.2602212 ] reward= 1.0 done= True \n",
      "\n",
      "Iteration No: 10 TimeSteps: 14  Success: 0 Best Q: 1 Learning rate: 0.5 Total reward: 15.0\n",
      "Episode 10 terminated after 14 Iterations \n",
      "step i 0 action= 1 \n",
      "\n",
      "Next observation= [-0.0044422   0.20237955  0.02366302 -0.28869304] reward= 1.0 done= False \n",
      "\n",
      "step i 1 action= 1 \n",
      "\n",
      "Next observation= [-3.9461369e-04  3.9715621e-01  1.7889163e-02 -5.7381994e-01] reward= 1.0 done= False \n",
      "\n",
      "step i 2 action= 1 \n",
      "\n",
      "Next observation= [ 0.00754851  0.59202284  0.00641276 -0.860814  ] reward= 1.0 done= False \n",
      "\n",
      "step i 3 action= 1 \n",
      "\n",
      "Next observation= [ 0.01938897  0.78705686 -0.01080352 -1.1514736 ] reward= 1.0 done= False \n",
      "\n",
      "step i 4 action= 0 \n",
      "\n",
      "Next observation= [ 0.03513011  0.59207755 -0.03383299 -0.86219794] reward= 1.0 done= False \n",
      "\n",
      "step i 5 action= 0 \n",
      "\n",
      "Next observation= [ 0.04697166  0.3974322  -0.05107695 -0.580342  ] reward= 1.0 done= False \n",
      "\n",
      "step i 6 action= 1 \n",
      "\n",
      "Next observation= [ 0.0549203   0.59323126 -0.06268379 -0.8886677 ] reward= 1.0 done= False \n",
      "\n",
      "step i 7 action= 1 \n",
      "\n",
      "Next observation= [ 0.06678493  0.78914535 -0.08045714 -1.2003785 ] reward= 1.0 done= False \n",
      "\n",
      "step i 8 action= 1 \n",
      "\n",
      "Next observation= [ 0.08256783  0.98521054 -0.10446472 -1.5171539 ] reward= 1.0 done= False \n",
      "\n",
      "step i 9 action= 1 \n",
      "\n",
      "Next observation= [ 0.10227204  1.1814295  -0.1348078  -1.8405347 ] reward= 1.0 done= False \n",
      "\n",
      "step i 10 action= 1 \n",
      "\n",
      "Next observation= [ 0.12590064  1.3777578  -0.17161849 -2.1718688 ] reward= 1.0 done= False \n",
      "\n",
      "step i 11 action= 1 \n",
      "\n",
      "Next observation= [ 0.1534558   1.5740881  -0.21505585 -2.5122464 ] reward= 1.0 done= True \n",
      "\n",
      "Iteration No: 11 TimeSteps: 11  Success: 0 Best Q: 1 Learning rate: 0.5 Total reward: 12.0\n",
      "Episode 11 terminated after 11 Iterations \n",
      "step i 0 action= 1 \n",
      "\n",
      "Next observation= [ 0.0075331   0.17785871  0.00312947 -0.26915625] reward= 1.0 done= False \n",
      "\n",
      "step i 1 action= 1 \n",
      "\n",
      "Next observation= [ 0.01109027  0.37293586 -0.00225366 -0.5608505 ] reward= 1.0 done= False \n",
      "\n",
      "step i 2 action= 1 \n",
      "\n",
      "Next observation= [ 0.01854899  0.56808937 -0.01347067 -0.85424256] reward= 1.0 done= False \n",
      "\n",
      "step i 3 action= 1 \n",
      "\n",
      "Next observation= [ 0.02991078  0.7633923  -0.03055552 -1.1511307 ] reward= 1.0 done= False \n",
      "\n",
      "step i 4 action= 0 \n",
      "\n",
      "Next observation= [ 0.04517863  0.5686821  -0.05357813 -0.8681836 ] reward= 1.0 done= False \n",
      "\n",
      "step i 5 action= 1 \n",
      "\n",
      "Next observation= [ 0.05655227  0.7644905  -0.07094181 -1.1772192 ] reward= 1.0 done= False \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step i 6 action= 1 \n",
      "\n",
      "Next observation= [ 0.07184207  0.9604586  -0.09448618 -1.4912713 ] reward= 1.0 done= False \n",
      "\n",
      "step i 7 action= 1 \n",
      "\n",
      "Next observation= [ 0.09105125  1.1565951  -0.12431161 -1.8119015 ] reward= 1.0 done= False \n",
      "\n",
      "step i 8 action= 1 \n",
      "\n",
      "Next observation= [ 0.11418315  1.3528637  -0.16054964 -2.140486  ] reward= 1.0 done= False \n",
      "\n",
      "step i 9 action= 1 \n",
      "\n",
      "Next observation= [ 0.14124042  1.549167   -0.20335937 -2.4781535 ] reward= 1.0 done= False \n",
      "\n",
      "step i 10 action= 0 \n",
      "\n",
      "Next observation= [ 0.17222376  1.3562475  -0.25292242 -2.2541137 ] reward= 1.0 done= True \n",
      "\n",
      "Iteration No: 12 TimeSteps: 10  Success: 0 Best Q: 1 Learning rate: 0.5 Total reward: 11.0\n",
      "Episode 12 terminated after 10 Iterations \n",
      "step i 0 action= 1 \n",
      "\n",
      "Next observation= [ 0.04833493  0.18046212 -0.04212419 -0.32398608] reward= 1.0 done= False \n",
      "\n",
      "step i 1 action= 1 \n",
      "\n",
      "Next observation= [ 0.05194417  0.37615776 -0.04860391 -0.62965   ] reward= 1.0 done= False \n",
      "\n",
      "step i 2 action= 1 \n",
      "\n",
      "Next observation= [ 0.05946733  0.5719231  -0.06119692 -0.9372351 ] reward= 1.0 done= False \n",
      "\n",
      "step i 3 action= 1 \n",
      "\n",
      "Next observation= [ 0.07090579  0.76781446 -0.07994162 -1.2485029 ] reward= 1.0 done= False \n",
      "\n",
      "step i 4 action= 1 \n",
      "\n",
      "Next observation= [ 0.08626208  0.9638651  -0.10491167 -1.5651174 ] reward= 1.0 done= False \n",
      "\n",
      "step i 5 action= 1 \n",
      "\n",
      "Next observation= [ 0.10553938  1.1600729  -0.13621402 -1.8885984 ] reward= 1.0 done= False \n",
      "\n",
      "step i 6 action= 1 \n",
      "\n",
      "Next observation= [ 0.12874083  1.3563869  -0.17398599 -2.220265  ] reward= 1.0 done= False \n",
      "\n",
      "step i 7 action= 0 \n",
      "\n",
      "Next observation= [ 0.15586857  1.163301   -0.21839128 -1.9859029 ] reward= 1.0 done= True \n",
      "\n",
      "Iteration No: 13 TimeSteps: 7  Success: 0 Best Q: 1 Learning rate: 0.5 Total reward: 8.0\n",
      "Episode 13 terminated after 7 Iterations \n",
      "step i 0 action= 1 \n",
      "\n",
      "Next observation= [ 0.02764826  0.17417641  0.00471995 -0.26499888] reward= 1.0 done= False \n",
      "\n",
      "step i 1 action= 1 \n",
      "\n",
      "Next observation= [ 0.03113179  0.3692307  -0.00058002 -0.55618936] reward= 1.0 done= False \n",
      "\n",
      "step i 2 action= 1 \n",
      "\n",
      "Next observation= [ 0.0385164   0.5643608  -0.01170381 -0.849055  ] reward= 1.0 done= False \n",
      "\n",
      "step i 3 action= 1 \n",
      "\n",
      "Next observation= [ 0.04980361  0.7596404  -0.02868491 -1.1453952 ] reward= 1.0 done= False \n",
      "\n",
      "step i 4 action= 0 \n",
      "\n",
      "Next observation= [ 0.06499642  0.5649046  -0.05159281 -0.8618439 ] reward= 1.0 done= False \n",
      "\n",
      "step i 5 action= 0 \n",
      "\n",
      "Next observation= [ 0.07629452  0.3705217  -0.06882969 -0.58581907] reward= 1.0 done= False \n",
      "\n",
      "step i 6 action= 1 \n",
      "\n",
      "Next observation= [ 0.08370495  0.5665368  -0.08054607 -0.8993654 ] reward= 1.0 done= False \n",
      "\n",
      "step i 7 action= 0 \n",
      "\n",
      "Next observation= [ 0.09503569  0.37259337 -0.09853338 -0.6330484 ] reward= 1.0 done= False \n",
      "\n",
      "step i 8 action= 1 \n",
      "\n",
      "Next observation= [ 0.10248755  0.56894183 -0.11119435 -0.9550645 ] reward= 1.0 done= False \n",
      "\n",
      "step i 9 action= 1 \n",
      "\n",
      "Next observation= [ 0.11386639  0.7653696  -0.13029563 -1.2805104 ] reward= 1.0 done= False \n",
      "\n",
      "step i 10 action= 1 \n",
      "\n",
      "Next observation= [ 0.12917379  0.96188855 -0.15590584 -1.6109887 ] reward= 1.0 done= False \n",
      "\n",
      "step i 11 action= 1 \n",
      "\n",
      "Next observation= [ 0.14841156  1.1584704  -0.18812563 -1.9479359 ] reward= 1.0 done= False \n",
      "\n",
      "step i 12 action= 1 \n",
      "\n",
      "Next observation= [ 0.17158096  1.3550317  -0.22708434 -2.2925591 ] reward= 1.0 done= True \n",
      "\n",
      "Iteration No: 14 TimeSteps: 12  Success: 0 Best Q: 1 Learning rate: 0.5 Total reward: 13.0\n",
      "Episode 14 terminated after 12 Iterations \n",
      "step i 0 action= 1 \n",
      "\n",
      "Next observation= [ 0.0272353   0.24359347 -0.02491009 -0.3347102 ] reward= 1.0 done= False \n",
      "\n",
      "step i 1 action= 0 \n",
      "\n",
      "Next observation= [ 0.03210717  0.04883473 -0.03160429 -0.04998554] reward= 1.0 done= False \n",
      "\n",
      "step i 2 action= 1 \n",
      "\n",
      "Next observation= [ 0.03308387  0.24439527 -0.032604   -0.35246998] reward= 1.0 done= False \n",
      "\n",
      "step i 3 action= 0 \n",
      "\n",
      "Next observation= [ 0.03797177  0.04975175 -0.0396534  -0.07024375] reward= 1.0 done= False \n",
      "\n",
      "step i 4 action= 1 \n",
      "\n",
      "Next observation= [ 0.03896681  0.2454191  -0.04105828 -0.3751691 ] reward= 1.0 done= False \n",
      "\n",
      "step i 5 action= 0 \n",
      "\n",
      "Next observation= [ 0.04387519  0.05090367 -0.04856166 -0.0957096 ] reward= 1.0 done= False \n",
      "\n",
      "step i 6 action= 1 \n",
      "\n",
      "Next observation= [ 0.04489326  0.24668679 -0.05047585 -0.40330958] reward= 1.0 done= False \n",
      "\n",
      "step i 7 action= 1 \n",
      "\n",
      "Next observation= [ 0.049827    0.4424869  -0.05854204 -0.7114693 ] reward= 1.0 done= False \n",
      "\n",
      "step i 8 action= 1 \n",
      "\n",
      "Next observation= [ 0.05867674  0.6383686  -0.07277143 -1.02199   ] reward= 1.0 done= False \n",
      "\n",
      "step i 9 action= 1 \n",
      "\n",
      "Next observation= [ 0.07144411  0.83438057 -0.09321123 -1.3366058 ] reward= 1.0 done= False \n",
      "\n",
      "step i 10 action= 1 \n",
      "\n",
      "Next observation= [ 0.08813172  1.030545   -0.11994334 -1.6569395 ] reward= 1.0 done= False \n",
      "\n",
      "step i 11 action= 0 \n",
      "\n",
      "Next observation= [ 0.10874262  0.8370091  -0.15308213 -1.4039001 ] reward= 1.0 done= False \n",
      "\n",
      "step i 12 action= 1 \n",
      "\n",
      "Next observation= [ 0.1254828   1.0336645  -0.18116014 -1.7402643 ] reward= 1.0 done= False \n",
      "\n",
      "step i 13 action= 1 \n",
      "\n",
      "Next observation= [ 0.14615609  1.2303286  -0.21596542 -2.083403  ] reward= 1.0 done= True \n",
      "\n",
      "Iteration No: 15 TimeSteps: 13  Success: 0 Best Q: 1 Learning rate: 0.5 Total reward: 14.0\n",
      "Episode 15 terminated after 13 Iterations \n",
      "step i 0 action= 1 \n",
      "\n",
      "Next observation= [ 0.02446376  0.2363669  -0.00278001 -0.25044355] reward= 1.0 done= False \n",
      "\n",
      "step i 1 action= 1 \n",
      "\n",
      "Next observation= [ 0.0291911   0.43152842 -0.00778888 -0.54400206] reward= 1.0 done= False \n",
      "\n",
      "step i 2 action= 1 \n",
      "\n",
      "Next observation= [ 0.03782167  0.626759   -0.01866892 -0.8391289 ] reward= 1.0 done= False \n",
      "\n",
      "step i 3 action= 1 \n",
      "\n",
      "Next observation= [ 0.05035685  0.8221308  -0.0354515  -1.1376239 ] reward= 1.0 done= False \n",
      "\n",
      "step i 4 action= 0 \n",
      "\n",
      "Next observation= [ 0.06679946  0.62749    -0.05820398 -0.8562667 ] reward= 1.0 done= False \n",
      "\n",
      "step i 5 action= 1 \n",
      "\n",
      "Next observation= [ 0.07934926  0.82335466 -0.07532931 -1.1666685 ] reward= 1.0 done= False \n",
      "\n",
      "step i 6 action= 1 \n",
      "\n",
      "Next observation= [ 0.09581636  1.0193717  -0.09866268 -1.4819862 ] reward= 1.0 done= False \n",
      "\n",
      "step i 7 action= 0 \n",
      "\n",
      "Next observation= [ 0.11620379  0.8255821  -0.12830241 -1.2216752 ] reward= 1.0 done= False \n",
      "\n",
      "step i 8 action= 1 \n",
      "\n",
      "Next observation= [ 0.13271543  1.0221022  -0.1527359  -1.5516499 ] reward= 1.0 done= False \n",
      "\n",
      "step i 9 action= 1 \n",
      "\n",
      "Next observation= [ 0.15315747  1.2186904  -0.18376891 -1.8878293 ] reward= 1.0 done= False \n",
      "\n",
      "step i 10 action= 0 \n",
      "\n",
      "Next observation= [ 0.17753129  1.0259805  -0.22152549 -1.6573563 ] reward= 1.0 done= True \n",
      "\n",
      "Iteration No: 16 TimeSteps: 10  Success: 0 Best Q: 1 Learning rate: 0.5 Total reward: 11.0\n",
      "Episode 16 terminated after 10 Iterations \n",
      "step i 0 action= 1 \n",
      "\n",
      "Next observation= [-0.0138625   0.1580348   0.02728364 -0.2717277 ] reward= 1.0 done= False \n",
      "\n",
      "step i 1 action= 1 \n",
      "\n",
      "Next observation= [-0.0107018   0.352757    0.02184908 -0.55568194] reward= 1.0 done= False \n",
      "\n",
      "step i 2 action= 1 \n",
      "\n",
      "Next observation= [-0.00364666  0.5475655   0.01073545 -0.8414018 ] reward= 1.0 done= False \n",
      "\n",
      "step i 3 action= 1 \n",
      "\n",
      "Next observation= [ 0.00730465  0.7425393  -0.00609259 -1.1306895 ] reward= 1.0 done= False \n",
      "\n",
      "step i 4 action= 0 \n",
      "\n",
      "Next observation= [ 0.02215544  0.54749763 -0.02870638 -0.8399236 ] reward= 1.0 done= False \n",
      "\n",
      "step i 5 action= 1 \n",
      "\n",
      "Next observation= [ 0.03310539  0.7429995  -0.04550485 -1.1414942 ] reward= 1.0 done= False \n",
      "\n",
      "step i 6 action= 0 \n",
      "\n",
      "Next observation= [ 0.04796538  0.5485009  -0.06833474 -0.86342204] reward= 1.0 done= False \n",
      "\n",
      "step i 7 action= 1 \n",
      "\n",
      "Next observation= [ 0.0589354   0.7444833  -0.08560317 -1.1767843 ] reward= 1.0 done= False \n",
      "\n",
      "step i 8 action= 1 \n",
      "\n",
      "Next observation= [ 0.07382506  0.9406065  -0.10913886 -1.4950285 ] reward= 1.0 done= False \n",
      "\n",
      "step i 9 action= 1 \n",
      "\n",
      "Next observation= [ 0.0926372   1.1368734  -0.13903943 -1.8197004 ] reward= 1.0 done= False \n",
      "\n",
      "step i 10 action= 1 \n",
      "\n",
      "Next observation= [ 0.11537466  1.33324    -0.17543344 -2.152154  ] reward= 1.0 done= False \n",
      "\n",
      "step i 11 action= 0 \n",
      "\n",
      "Next observation= [ 0.14203946  1.1402243  -0.21847652 -1.9183874 ] reward= 1.0 done= True \n",
      "\n",
      "Iteration No: 17 TimeSteps: 11  Success: 0 Best Q: 1 Learning rate: 0.5 Total reward: 12.0\n",
      "Episode 17 terminated after 11 Iterations \n",
      "step i 0 action= 1 \n",
      "\n",
      "Next observation= [-0.04003306  0.18885389 -0.03280807 -0.2928037 ] reward= 1.0 done= False \n",
      "\n",
      "step i 1 action= 1 \n",
      "\n",
      "Next observation= [-0.03625599  0.38442788 -0.03866414 -0.5956507 ] reward= 1.0 done= False \n",
      "\n",
      "step i 2 action= 0 \n",
      "\n",
      "Next observation= [-0.02856743  0.18986776 -0.05057715 -0.31539303] reward= 1.0 done= False \n",
      "\n",
      "step i 3 action= 1 \n",
      "\n",
      "Next observation= [-0.02477007  0.3856723  -0.05688501 -0.6235876 ] reward= 1.0 done= False \n",
      "\n",
      "step i 4 action= 1 \n",
      "\n",
      "Next observation= [-0.01705663  0.5815404  -0.06935676 -0.9336297 ] reward= 1.0 done= False \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step i 5 action= 1 \n",
      "\n",
      "Next observation= [-0.00542582  0.7775261  -0.08802935 -1.247276  ] reward= 1.0 done= False \n",
      "\n",
      "step i 6 action= 1 \n",
      "\n",
      "Next observation= [ 0.0101247   0.97365963 -0.11297487 -1.5661843 ] reward= 1.0 done= False \n",
      "\n",
      "step i 7 action= 1 \n",
      "\n",
      "Next observation= [ 0.0295979   1.1699357  -0.14429857 -1.8918656 ] reward= 1.0 done= False \n",
      "\n",
      "step i 8 action= 1 \n",
      "\n",
      "Next observation= [ 0.05299661  1.3662993  -0.18213588 -2.2256265 ] reward= 1.0 done= False \n",
      "\n",
      "step i 9 action= 0 \n",
      "\n",
      "Next observation= [ 0.08032259  1.1733199  -0.2266484  -1.994198  ] reward= 1.0 done= True \n",
      "\n",
      "Iteration No: 18 TimeSteps: 9  Success: 0 Best Q: 1 Learning rate: 0.5 Total reward: 10.0\n",
      "Episode 18 terminated after 9 Iterations \n",
      "step i 0 action= 1 \n",
      "\n",
      "Next observation= [ 0.00816384  0.17659704  0.02942573 -0.26209903] reward= 1.0 done= False \n",
      "\n",
      "step i 1 action= 1 \n",
      "\n",
      "Next observation= [ 0.01169578  0.37128687  0.02418375 -0.54535747] reward= 1.0 done= False \n",
      "\n",
      "step i 2 action= 1 \n",
      "\n",
      "Next observation= [ 0.01912152  0.5660608   0.0132766  -0.8303236 ] reward= 1.0 done= False \n",
      "\n",
      "step i 3 action= 1 \n",
      "\n",
      "Next observation= [ 0.03044273  0.7609988  -0.00332987 -1.1188016 ] reward= 1.0 done= False \n",
      "\n",
      "step i 4 action= 0 \n",
      "\n",
      "Next observation= [ 0.04566271  0.5659207  -0.0257059  -0.827165  ] reward= 1.0 done= False \n",
      "\n",
      "step i 5 action= 1 \n",
      "\n",
      "Next observation= [ 0.05698112  0.76138455 -0.0422492  -1.1278206 ] reward= 1.0 done= False \n",
      "\n",
      "step i 6 action= 0 \n",
      "\n",
      "Next observation= [ 0.07220881  0.56684077 -0.06480561 -0.84868294] reward= 1.0 done= False \n",
      "\n",
      "step i 7 action= 1 \n",
      "\n",
      "Next observation= [ 0.08354563  0.7627839  -0.08177927 -1.1610202 ] reward= 1.0 done= False \n",
      "\n",
      "step i 8 action= 1 \n",
      "\n",
      "Next observation= [ 0.09880131  0.9588703  -0.10499968 -1.478183  ] reward= 1.0 done= False \n",
      "\n",
      "step i 9 action= 1 \n",
      "\n",
      "Next observation= [ 0.11797871  1.1551058  -0.13456334 -1.8017286 ] reward= 1.0 done= False \n",
      "\n",
      "step i 10 action= 1 \n",
      "\n",
      "Next observation= [ 0.14108083  1.351451   -0.17059791 -2.1330261 ] reward= 1.0 done= False \n",
      "\n",
      "step i 11 action= 0 \n",
      "\n",
      "Next observation= [ 0.16810985  1.158382   -0.21325843 -1.8975396 ] reward= 1.0 done= True \n",
      "\n",
      "Iteration No: 19 TimeSteps: 11  Success: 0 Best Q: 1 Learning rate: 0.5 Total reward: 12.0\n",
      "Episode 19 terminated after 11 Iterations \n",
      "step i 0 action= 1 \n",
      "\n",
      "Next observation= [-0.01502879  0.16314632 -0.03459866 -0.3027346 ] reward= 1.0 done= False \n",
      "\n",
      "step i 1 action= 1 \n",
      "\n",
      "Next observation= [-0.01176587  0.35874385 -0.04065335 -0.6061253 ] reward= 1.0 done= False \n",
      "\n",
      "step i 2 action= 1 \n",
      "\n",
      "Next observation= [-0.00459099  0.55441    -0.05277586 -0.91133076] reward= 1.0 done= False \n",
      "\n",
      "step i 3 action= 1 \n",
      "\n",
      "Next observation= [ 0.00649721  0.7502048  -0.07100247 -1.220123  ] reward= 1.0 done= False \n",
      "\n",
      "step i 4 action= 1 \n",
      "\n",
      "Next observation= [ 0.02150131  0.9461664  -0.09540493 -1.534182  ] reward= 1.0 done= False \n",
      "\n",
      "step i 5 action= 1 \n",
      "\n",
      "Next observation= [ 0.04042463  1.1422993  -0.12608857 -1.85505   ] reward= 1.0 done= False \n",
      "\n",
      "step i 6 action= 1 \n",
      "\n",
      "Next observation= [ 0.06327062  1.3385612  -0.16318958 -2.1840775 ] reward= 1.0 done= False \n",
      "\n",
      "step i 7 action= 1 \n",
      "\n",
      "Next observation= [ 0.09004185  1.534847   -0.20687112 -2.5223596 ] reward= 1.0 done= False \n",
      "\n",
      "step i 8 action= 0 \n",
      "\n",
      "Next observation= [ 0.12073878  1.3419285  -0.25731832 -2.299539  ] reward= 1.0 done= True \n",
      "\n",
      "Iteration No: 20 TimeSteps: 8  Success: 0 Best Q: 1 Learning rate: 0.5 Total reward: 9.0\n",
      "Episode 20 terminated after 8 Iterations \n",
      "step i 0 action= 1 \n",
      "\n",
      "Next observation= [-0.00534055  0.24376452 -0.01289862 -0.33118182] reward= 1.0 done= False \n",
      "\n",
      "step i 1 action= 1 \n",
      "\n",
      "Next observation= [-4.6526102e-04  4.3906766e-01 -1.9522255e-02 -6.2790430e-01] reward= 1.0 done= False \n",
      "\n",
      "step i 2 action= 1 \n",
      "\n",
      "Next observation= [ 0.00831609  0.6344566  -0.03208034 -0.92667097] reward= 1.0 done= False \n",
      "\n",
      "step i 3 action= 1 \n",
      "\n",
      "Next observation= [ 0.02100522  0.8299967  -0.05061376 -1.2292602 ] reward= 1.0 done= False \n",
      "\n",
      "step i 4 action= 1 \n",
      "\n",
      "Next observation= [ 0.03760516  1.025732   -0.07519896 -1.5373614 ] reward= 1.0 done= False \n",
      "\n",
      "step i 5 action= 1 \n",
      "\n",
      "Next observation= [ 0.0581198   1.2216742  -0.10594619 -1.8525317 ] reward= 1.0 done= False \n",
      "\n",
      "step i 6 action= 1 \n",
      "\n",
      "Next observation= [ 0.08255328  1.4177897  -0.14299683 -2.1761453 ] reward= 1.0 done= False \n",
      "\n",
      "step i 7 action= 1 \n",
      "\n",
      "Next observation= [ 0.11090908  1.6139847  -0.18651973 -2.509332  ] reward= 1.0 done= False \n",
      "\n",
      "step i 8 action= 0 \n",
      "\n",
      "Next observation= [ 0.14318877  1.4208229  -0.23670638 -2.2791343 ] reward= 1.0 done= True \n",
      "\n",
      "Iteration No: 21 TimeSteps: 8  Success: 0 Best Q: 1 Learning rate: 0.5 Total reward: 9.0\n",
      "Episode 21 terminated after 8 Iterations \n",
      "step i 0 action= 1 \n",
      "\n",
      "Next observation= [ 0.03349089  0.24203032 -0.03585014 -0.2914635 ] reward= 1.0 done= False \n",
      "\n",
      "step i 1 action= 1 \n",
      "\n",
      "Next observation= [ 0.0383315   0.43764463 -0.04167942 -0.5952341 ] reward= 1.0 done= False \n",
      "\n",
      "step i 2 action= 1 \n",
      "\n",
      "Next observation= [ 0.04708439  0.6333244  -0.0535841  -0.90074897] reward= 1.0 done= False \n",
      "\n",
      "step i 3 action= 1 \n",
      "\n",
      "Next observation= [ 0.05975088  0.8291298  -0.07159907 -1.2097818 ] reward= 1.0 done= False \n",
      "\n",
      "step i 4 action= 1 \n",
      "\n",
      "Next observation= [ 0.07633348  1.0250996  -0.09579471 -1.5240154 ] reward= 1.0 done= False \n",
      "\n",
      "step i 5 action= 1 \n",
      "\n",
      "Next observation= [ 0.09683546  1.221239   -0.12627502 -1.8449962 ] reward= 1.0 done= False \n",
      "\n",
      "step i 6 action= 1 \n",
      "\n",
      "Next observation= [ 0.12126025  1.4175067  -0.16317494 -2.17408   ] reward= 1.0 done= False \n",
      "\n",
      "step i 7 action= 1 \n",
      "\n",
      "Next observation= [ 0.14961039  1.6137993  -0.20665655 -2.5123684 ] reward= 1.0 done= False \n",
      "\n",
      "step i 8 action= 0 \n",
      "\n",
      "Next observation= [ 0.18188636  1.4208882  -0.25690392 -2.2894843 ] reward= 1.0 done= True \n",
      "\n",
      "Iteration No: 22 TimeSteps: 8  Success: 0 Best Q: 1 Learning rate: 0.5 Total reward: 9.0\n",
      "Episode 22 terminated after 8 Iterations \n",
      "step i 0 action= 1 \n",
      "\n",
      "Next observation= [-0.03632042  0.23868603  0.03298566 -0.26368868] reward= 1.0 done= False \n",
      "\n",
      "step i 1 action= 1 \n",
      "\n",
      "Next observation= [-0.0315467   0.433322    0.02771189 -0.5457878 ] reward= 1.0 done= False \n",
      "\n",
      "step i 2 action= 0 \n",
      "\n",
      "Next observation= [-0.02288026  0.23782186  0.01679613 -0.24450395] reward= 1.0 done= False \n",
      "\n",
      "step i 3 action= 0 \n",
      "\n",
      "Next observation= [-0.01812382  0.04246408  0.01190605  0.05342921] reward= 1.0 done= False \n",
      "\n",
      "step i 4 action= 1 \n",
      "\n",
      "Next observation= [-0.01727454  0.23741332  0.01297464 -0.2354736 ] reward= 1.0 done= False \n",
      "\n",
      "step i 5 action= 1 \n",
      "\n",
      "Next observation= [-0.01252627  0.4323475   0.00826517 -0.5240359 ] reward= 1.0 done= False \n",
      "\n",
      "step i 6 action= 1 \n",
      "\n",
      "Next observation= [-0.00387932  0.6273522  -0.00221555 -0.81410295] reward= 1.0 done= False \n",
      "\n",
      "step i 7 action= 1 \n",
      "\n",
      "Next observation= [ 0.00866772  0.8225044  -0.01849761 -1.107482  ] reward= 1.0 done= False \n",
      "\n",
      "step i 8 action= 1 \n",
      "\n",
      "Next observation= [ 0.02511781  1.0178646  -0.04064725 -1.40591   ] reward= 1.0 done= False \n",
      "\n",
      "step i 9 action= 1 \n",
      "\n",
      "Next observation= [ 0.0454751   1.2134668  -0.06876545 -1.7110181 ] reward= 1.0 done= False \n",
      "\n",
      "step i 10 action= 1 \n",
      "\n",
      "Next observation= [ 0.06974444  1.409308   -0.10298581 -2.0242865 ] reward= 1.0 done= False \n",
      "\n",
      "step i 11 action= 1 \n",
      "\n",
      "Next observation= [ 0.0979306   1.6053338  -0.14347154 -2.3469918 ] reward= 1.0 done= False \n",
      "\n",
      "step i 12 action= 1 \n",
      "\n",
      "Next observation= [ 0.13003728  1.8014235  -0.19041137 -2.6801405 ] reward= 1.0 done= False \n",
      "\n",
      "step i 13 action= 0 \n",
      "\n",
      "Next observation= [ 0.16606574  1.6081471  -0.24401419 -2.451109  ] reward= 1.0 done= True \n",
      "\n",
      "Iteration No: 23 TimeSteps: 13  Success: 0 Best Q: 1 Learning rate: 0.5 Total reward: 14.0\n",
      "Episode 23 terminated after 13 Iterations \n",
      "step i 0 action= 1 \n",
      "\n",
      "Next observation= [-0.02034935  0.17815427 -0.0021962  -0.2622093 ] reward= 1.0 done= False \n",
      "\n",
      "step i 1 action= 1 \n",
      "\n",
      "Next observation= [-0.01678626  0.3733075  -0.00744039 -0.55558413] reward= 1.0 done= False \n",
      "\n",
      "step i 2 action= 1 \n",
      "\n",
      "Next observation= [-0.00932011  0.5685331  -0.01855207 -0.8506019 ] reward= 1.0 done= False \n",
      "\n",
      "step i 3 action= 1 \n",
      "\n",
      "Next observation= [ 0.00205055  0.7639031  -0.03556411 -1.1490604 ] reward= 1.0 done= False \n",
      "\n",
      "step i 4 action= 0 \n",
      "\n",
      "Next observation= [ 0.01732861  0.5692629  -0.05854531 -0.8677384 ] reward= 1.0 done= False \n",
      "\n",
      "step i 5 action= 1 \n",
      "\n",
      "Next observation= [ 0.02871387  0.7651305  -0.07590009 -1.178239  ] reward= 1.0 done= False \n",
      "\n",
      "step i 6 action= 1 \n",
      "\n",
      "Next observation= [ 0.04401648  0.96115166 -0.09946486 -1.4937173 ] reward= 1.0 done= False \n",
      "\n",
      "step i 7 action= 1 \n",
      "\n",
      "Next observation= [ 0.06323951  1.157333   -0.1293392  -1.8157294 ] reward= 1.0 done= False \n",
      "\n",
      "step i 8 action= 1 \n",
      "\n",
      "Next observation= [ 0.08638617  1.3536354  -0.1656538  -2.1456432 ] reward= 1.0 done= False \n",
      "\n",
      "step i 9 action= 1 \n",
      "\n",
      "Next observation= [ 0.11345888  1.5499587  -0.20856667 -2.4845767 ] reward= 1.0 done= False \n",
      "\n",
      "step i 10 action= 0 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next observation= [ 0.14445806  1.3570999  -0.2582582  -2.2624328 ] reward= 1.0 done= True \n",
      "\n",
      "Iteration No: 24 TimeSteps: 10  Success: 0 Best Q: 1 Learning rate: 0.5 Total reward: 11.0\n",
      "Episode 24 terminated after 10 Iterations \n",
      "step i 0 action= 0 \n",
      "\n",
      "Next observation= [ 0.01112959 -0.20843975  0.0199705   0.29078838] reward= 1.0 done= False \n",
      "\n",
      "step i 1 action= 1 \n",
      "\n",
      "Next observation= [ 0.0069608  -0.01360817  0.02578627  0.00447022] reward= 1.0 done= False \n",
      "\n",
      "step i 2 action= 1 \n",
      "\n",
      "Next observation= [ 0.00668863  0.18113466  0.02587568 -0.2799666 ] reward= 1.0 done= False \n",
      "\n",
      "step i 3 action= 1 \n",
      "\n",
      "Next observation= [ 0.01031133  0.37587813  0.02027634 -0.56437737] reward= 1.0 done= False \n",
      "\n",
      "step i 4 action= 0 \n",
      "\n",
      "Next observation= [ 0.01782889  0.18047763  0.0089888  -0.26537606] reward= 1.0 done= False \n",
      "\n",
      "step i 5 action= 1 \n",
      "\n",
      "Next observation= [ 0.02143844  0.37547016  0.00368127 -0.55521035] reward= 1.0 done= False \n",
      "\n",
      "step i 6 action= 1 \n",
      "\n",
      "Next observation= [ 0.02894785  0.5705402  -0.00742293 -0.8467312 ] reward= 1.0 done= False \n",
      "\n",
      "step i 7 action= 0 \n",
      "\n",
      "Next observation= [ 0.04035865  0.37552032 -0.02435756 -0.5563917 ] reward= 1.0 done= False \n",
      "\n",
      "step i 8 action= 1 \n",
      "\n",
      "Next observation= [ 0.04786906  0.5709756  -0.03548539 -0.8566481 ] reward= 1.0 done= False \n",
      "\n",
      "step i 9 action= 1 \n",
      "\n",
      "Next observation= [ 0.05928857  0.76656264 -0.05261835 -1.1602745 ] reward= 1.0 done= False \n",
      "\n",
      "step i 10 action= 0 \n",
      "\n",
      "Next observation= [ 0.07461982  0.5721642  -0.07582384 -0.884543  ] reward= 1.0 done= False \n",
      "\n",
      "step i 11 action= 1 \n",
      "\n",
      "Next observation= [ 0.0860631   0.76822925 -0.0935147  -1.2000664 ] reward= 1.0 done= False \n",
      "\n",
      "step i 12 action= 1 \n",
      "\n",
      "Next observation= [ 0.10142769  0.9644282  -0.11751603 -1.5205323 ] reward= 1.0 done= False \n",
      "\n",
      "step i 13 action= 1 \n",
      "\n",
      "Next observation= [ 0.12071625  1.160758   -0.14792667 -1.8474661 ] reward= 1.0 done= False \n",
      "\n",
      "step i 14 action= 0 \n",
      "\n",
      "Next observation= [ 0.14393142  0.9675431  -0.184876   -1.6041409 ] reward= 1.0 done= False \n",
      "\n",
      "step i 15 action= 0 \n",
      "\n",
      "Next observation= [ 0.16328228  0.77502704 -0.21695882 -1.3743323 ] reward= 1.0 done= True \n",
      "\n",
      "Iteration No: 25 TimeSteps: 15  Success: 0 Best Q: 1 Learning rate: 0.5 Total reward: 16.0\n",
      "Episode 25 terminated after 15 Iterations \n",
      "step i 0 action= 1 \n",
      "\n",
      "Next observation= [ 0.04853258  0.24461691  0.02952136 -0.32746688] reward= 1.0 done= False \n",
      "\n",
      "step i 1 action= 1 \n",
      "\n",
      "Next observation= [ 0.05342492  0.4393064   0.02297202 -0.61069584] reward= 1.0 done= False \n",
      "\n",
      "step i 2 action= 1 \n",
      "\n",
      "Next observation= [ 0.06221105  0.63409984  0.0107581  -0.89605576] reward= 1.0 done= False \n",
      "\n",
      "step i 3 action= 1 \n",
      "\n",
      "Next observation= [ 0.07489304  0.8290743  -0.00716301 -1.1853377 ] reward= 1.0 done= False \n",
      "\n",
      "step i 4 action= 0 \n",
      "\n",
      "Next observation= [ 0.09147453  0.634046   -0.03086977 -0.89490867] reward= 1.0 done= False \n",
      "\n",
      "step i 5 action= 0 \n",
      "\n",
      "Next observation= [ 0.10415545  0.43935594 -0.04876794 -0.61208695] reward= 1.0 done= False \n",
      "\n",
      "step i 6 action= 1 \n",
      "\n",
      "Next observation= [ 0.11294257  0.6351243  -0.06100968 -0.9197225 ] reward= 1.0 done= False \n",
      "\n",
      "step i 7 action= 1 \n",
      "\n",
      "Next observation= [ 0.12564506  0.83101547 -0.07940413 -1.2309383 ] reward= 1.0 done= False \n",
      "\n",
      "step i 8 action= 1 \n",
      "\n",
      "Next observation= [ 0.14226536  1.0270638  -0.1040229  -1.5474046 ] reward= 1.0 done= False \n",
      "\n",
      "step i 9 action= 1 \n",
      "\n",
      "Next observation= [ 0.16280665  1.2232696  -0.134971   -1.8706499 ] reward= 1.0 done= False \n",
      "\n",
      "step i 10 action= 1 \n",
      "\n",
      "Next observation= [ 0.18727204  1.4195843  -0.17238398 -2.202005  ] reward= 1.0 done= False \n",
      "\n",
      "step i 11 action= 0 \n",
      "\n",
      "Next observation= [ 0.21566372  1.2264901  -0.21642408 -1.9670869 ] reward= 1.0 done= True \n",
      "\n",
      "Iteration No: 26 TimeSteps: 11  Success: 0 Best Q: 1 Learning rate: 0.5 Total reward: 12.0\n",
      "Episode 26 terminated after 11 Iterations \n",
      "step i 0 action= 1 \n",
      "\n",
      "Next observation= [ 0.02729074  0.20005956  0.04056065 -0.28437057] reward= 1.0 done= False \n",
      "\n",
      "step i 1 action= 1 \n",
      "\n",
      "Next observation= [ 0.03129193  0.39458025  0.03487324 -0.56399006] reward= 1.0 done= False \n",
      "\n",
      "step i 2 action= 1 \n",
      "\n",
      "Next observation= [ 0.03918353  0.58919597  0.02359344 -0.8454855 ] reward= 1.0 done= False \n",
      "\n",
      "step i 3 action= 0 \n",
      "\n",
      "Next observation= [ 0.05096745  0.3937602   0.00668372 -0.5454776 ] reward= 1.0 done= False \n",
      "\n",
      "step i 4 action= 1 \n",
      "\n",
      "Next observation= [ 0.05884266  0.5887876  -0.00422583 -0.8360472 ] reward= 1.0 done= False \n",
      "\n",
      "step i 5 action= 1 \n",
      "\n",
      "Next observation= [ 0.07061841  0.783967   -0.02094677 -1.1300561 ] reward= 1.0 done= False \n",
      "\n",
      "step i 6 action= 0 \n",
      "\n",
      "Next observation= [ 0.08629775  0.5891256  -0.04354789 -0.8440159 ] reward= 1.0 done= False \n",
      "\n",
      "step i 7 action= 1 \n",
      "\n",
      "Next observation= [ 0.09808026  0.7848139  -0.06042821 -1.1500691 ] reward= 1.0 done= False \n",
      "\n",
      "step i 8 action= 1 \n",
      "\n",
      "Next observation= [ 0.11377654  0.98067015 -0.08342959 -1.4610723 ] reward= 1.0 done= False \n",
      "\n",
      "step i 9 action= 1 \n",
      "\n",
      "Next observation= [ 0.13338995  1.1767099  -0.11265104 -1.778609  ] reward= 1.0 done= False \n",
      "\n",
      "step i 10 action= 1 \n",
      "\n",
      "Next observation= [ 0.15692414  1.3729054  -0.14822322 -2.1040864 ] reward= 1.0 done= False \n",
      "\n",
      "step i 11 action= 1 \n",
      "\n",
      "Next observation= [ 0.18438224  1.5691711  -0.19030495 -2.438675  ] reward= 1.0 done= False \n",
      "\n",
      "step i 12 action= 1 \n",
      "\n",
      "Next observation= [ 0.21576567  1.7653457  -0.23907845 -2.7832372 ] reward= 1.0 done= True \n",
      "\n",
      "Iteration No: 27 TimeSteps: 12  Success: 0 Best Q: 1 Learning rate: 0.5 Total reward: 13.0\n",
      "Episode 27 terminated after 12 Iterations \n",
      "step i 0 action= 1 \n",
      "\n",
      "Next observation= [ 0.03665048  0.16648713  0.04579364 -0.27626336] reward= 1.0 done= False \n",
      "\n",
      "step i 1 action= 1 \n",
      "\n",
      "Next observation= [ 0.03998022  0.36092684  0.04026837 -0.5541585 ] reward= 1.0 done= False \n",
      "\n",
      "step i 2 action= 1 \n",
      "\n",
      "Next observation= [ 0.04719875  0.55546093  0.0291852  -0.8338874 ] reward= 1.0 done= False \n",
      "\n",
      "step i 3 action= 1 \n",
      "\n",
      "Next observation= [ 0.05830797  0.7501722   0.01250745 -1.1172507 ] reward= 1.0 done= False \n",
      "\n",
      "step i 4 action= 0 \n",
      "\n",
      "Next observation= [ 0.07331142  0.55488837 -0.00983756 -0.8206708 ] reward= 1.0 done= False \n",
      "\n",
      "step i 5 action= 1 \n",
      "\n",
      "Next observation= [ 0.08440918  0.7501435  -0.02625098 -1.1164316 ] reward= 1.0 done= False \n",
      "\n",
      "step i 6 action= 1 \n",
      "\n",
      "Next observation= [ 0.09941205  0.94560003 -0.04857961 -1.4172322 ] reward= 1.0 done= False \n",
      "\n",
      "step i 7 action= 1 \n",
      "\n",
      "Next observation= [ 0.11832406  1.1412886  -0.07692425 -1.7246956 ] reward= 1.0 done= False \n",
      "\n",
      "step i 8 action= 1 \n",
      "\n",
      "Next observation= [ 0.14114982  1.3372018  -0.11141816 -2.0402896 ] reward= 1.0 done= False \n",
      "\n",
      "step i 9 action= 1 \n",
      "\n",
      "Next observation= [ 0.16789386  1.5332795  -0.15222396 -2.3652718 ] reward= 1.0 done= False \n",
      "\n",
      "step i 10 action= 0 \n",
      "\n",
      "Next observation= [ 0.19855945  1.339805   -0.1995294  -2.122997  ] reward= 1.0 done= False \n",
      "\n",
      "step i 11 action= 0 \n",
      "\n",
      "Next observation= [ 0.22535555  1.1471515  -0.24198933 -1.8980235 ] reward= 1.0 done= True \n",
      "\n",
      "Iteration No: 28 TimeSteps: 11  Success: 0 Best Q: 1 Learning rate: 0.5 Total reward: 12.0\n",
      "Episode 28 terminated after 11 Iterations \n",
      "step i 0 action= 1 \n",
      "\n",
      "Next observation= [-0.02627388  0.2011711  -0.01969004 -0.31246665] reward= 1.0 done= False \n",
      "\n",
      "step i 1 action= 1 \n",
      "\n",
      "Next observation= [-0.02225046  0.39656794 -0.02593937 -0.6112936 ] reward= 1.0 done= False \n",
      "\n",
      "step i 2 action= 1 \n",
      "\n",
      "Next observation= [-0.0143191   0.5920427  -0.03816525 -0.91203237] reward= 1.0 done= False \n",
      "\n",
      "step i 3 action= 1 \n",
      "\n",
      "Next observation= [-0.00247824  0.78765965 -0.05640589 -1.216462  ] reward= 1.0 done= False \n",
      "\n",
      "step i 4 action= 0 \n",
      "\n",
      "Next observation= [ 0.01327495  0.5933087  -0.08073513 -0.9419738 ] reward= 1.0 done= False \n",
      "\n",
      "step i 5 action= 1 \n",
      "\n",
      "Next observation= [ 0.02514112  0.78942037 -0.0995746  -1.2588934 ] reward= 1.0 done= False \n",
      "\n",
      "step i 6 action= 1 \n",
      "\n",
      "Next observation= [ 0.04092953  0.9856653  -0.12475248 -1.5810293 ] reward= 1.0 done= False \n",
      "\n",
      "step i 7 action= 1 \n",
      "\n",
      "Next observation= [ 0.06064283  1.182032   -0.15637305 -1.9098724 ] reward= 1.0 done= False \n",
      "\n",
      "step i 8 action= 1 \n",
      "\n",
      "Next observation= [ 0.08428348  1.3784574  -0.19457051 -2.246702  ] reward= 1.0 done= False \n",
      "\n",
      "step i 9 action= 1 \n",
      "\n",
      "Next observation= [ 0.11185262  1.5748106  -0.23950455 -2.5925179 ] reward= 1.0 done= True \n",
      "\n",
      "Iteration No: 29 TimeSteps: 9  Success: 0 Best Q: 1 Learning rate: 0.5 Total reward: 10.0\n",
      "Episode 29 terminated after 9 Iterations \n",
      "step i 0 action= 1 \n",
      "\n",
      "Next observation= [-0.0087069   0.2370574  -0.01594255 -0.3201651 ] reward= 1.0 done= False \n",
      "\n",
      "step i 1 action= 1 \n",
      "\n",
      "Next observation= [-0.00396575  0.43240273 -0.02234585 -0.6178327 ] reward= 1.0 done= False \n",
      "\n",
      "step i 2 action= 1 \n",
      "\n",
      "Next observation= [ 0.0046823   0.6278296  -0.0347025  -0.91746897] reward= 1.0 done= False \n",
      "\n",
      "step i 3 action= 1 \n",
      "\n",
      "Next observation= [ 0.01723889  0.8234031  -0.05305188 -1.2208531 ] reward= 1.0 done= False \n",
      "\n",
      "step i 4 action= 0 \n",
      "\n",
      "Next observation= [ 0.03370696  0.62900347 -0.07746895 -0.94525385] reward= 1.0 done= False \n",
      "\n",
      "step i 5 action= 1 \n",
      "\n",
      "Next observation= [ 0.04628703  0.8250786  -0.09637402 -1.2612375 ] reward= 1.0 done= False \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step i 6 action= 1 \n",
      "\n",
      "Next observation= [ 0.0627886   1.0212919  -0.12159877 -1.5824819 ] reward= 1.0 done= False \n",
      "\n",
      "step i 7 action= 1 \n",
      "\n",
      "Next observation= [ 0.08321443  1.2176329  -0.15324841 -1.9104806 ] reward= 1.0 done= False \n",
      "\n",
      "step i 8 action= 1 \n",
      "\n",
      "Next observation= [ 0.10756709  1.41404    -0.19145802 -2.2465174 ] reward= 1.0 done= False \n",
      "\n",
      "step i 9 action= 0 \n",
      "\n",
      "Next observation= [ 0.1358479   1.2211713  -0.23638837 -2.018446  ] reward= 1.0 done= True \n",
      "\n",
      "Iteration No: 30 TimeSteps: 9  Success: 0 Best Q: 1 Learning rate: 0.5 Total reward: 10.0\n",
      "Episode 30 terminated after 9 Iterations \n",
      "step i 0 action= 1 \n",
      "\n",
      "Next observation= [ 0.04366491  0.1986678   0.02225965 -0.28823408] reward= 1.0 done= False \n",
      "\n",
      "step i 1 action= 1 \n",
      "\n",
      "Next observation= [ 0.04763826  0.39346537  0.01649497 -0.5738143 ] reward= 1.0 done= False \n",
      "\n",
      "step i 2 action= 1 \n",
      "\n",
      "Next observation= [ 0.05550757  0.5883522   0.00501869 -0.86125547] reward= 1.0 done= False \n",
      "\n",
      "step i 3 action= 1 \n",
      "\n",
      "Next observation= [ 0.06727462  0.7834055  -0.01220642 -1.1523561 ] reward= 1.0 done= False \n",
      "\n",
      "step i 4 action= 0 \n",
      "\n",
      "Next observation= [ 0.08294272  0.5884449  -0.03525355 -0.8635256 ] reward= 1.0 done= False \n",
      "\n",
      "step i 5 action= 1 \n",
      "\n",
      "Next observation= [ 0.09471162  0.7840286  -0.05252406 -1.1670814 ] reward= 1.0 done= False \n",
      "\n",
      "step i 6 action= 0 \n",
      "\n",
      "Next observation= [ 0.1103922   0.5896279  -0.07586569 -0.8913175 ] reward= 1.0 done= False \n",
      "\n",
      "step i 7 action= 1 \n",
      "\n",
      "Next observation= [ 0.12218475  0.7856926  -0.09369203 -1.2068516 ] reward= 1.0 done= False \n",
      "\n",
      "step i 8 action= 1 \n",
      "\n",
      "Next observation= [ 0.13789861  0.9818918  -0.11782907 -1.5273649 ] reward= 1.0 done= False \n",
      "\n",
      "step i 9 action= 1 \n",
      "\n",
      "Next observation= [ 0.15753645  1.178222   -0.14837636 -1.8543797 ] reward= 1.0 done= False \n",
      "\n",
      "step i 10 action= 1 \n",
      "\n",
      "Next observation= [ 0.18110088  1.3746307  -0.18546396 -2.1892185 ] reward= 1.0 done= False \n",
      "\n",
      "step i 11 action= 0 \n",
      "\n",
      "Next observation= [ 0.20859349  1.1817257  -0.22924833 -1.9590377 ] reward= 1.0 done= True \n",
      "\n",
      "Iteration No: 31 TimeSteps: 11  Success: 0 Best Q: 1 Learning rate: 0.5 Total reward: 12.0\n",
      "Episode 31 terminated after 11 Iterations \n",
      "step i 0 action= 1 \n",
      "\n",
      "Next observation= [-0.02773013  0.19723012  0.0264786  -0.33186975] reward= 1.0 done= False \n",
      "\n",
      "step i 1 action= 1 \n",
      "\n",
      "Next observation= [-0.02378553  0.39196536  0.01984121 -0.6160864 ] reward= 1.0 done= False \n",
      "\n",
      "step i 2 action= 1 \n",
      "\n",
      "Next observation= [-0.01594622  0.58680457  0.00751948 -0.9024548 ] reward= 1.0 done= False \n",
      "\n",
      "step i 3 action= 1 \n",
      "\n",
      "Next observation= [-0.00421013  0.7818239  -0.01052962 -1.1927648 ] reward= 1.0 done= False \n",
      "\n",
      "step i 4 action= 0 \n",
      "\n",
      "Next observation= [ 0.01142635  0.58683985 -0.03438491 -0.9034006 ] reward= 1.0 done= False \n",
      "\n",
      "step i 5 action= 1 \n",
      "\n",
      "Next observation= [ 0.02316314  0.78241026 -0.05245293 -1.20669   ] reward= 1.0 done= False \n",
      "\n",
      "step i 6 action= 0 \n",
      "\n",
      "Next observation= [ 0.03881135  0.5880038  -0.07658672 -0.93089545] reward= 1.0 done= False \n",
      "\n",
      "step i 7 action= 1 \n",
      "\n",
      "Next observation= [ 0.05057142  0.78407115 -0.09520464 -1.2466289 ] reward= 1.0 done= False \n",
      "\n",
      "step i 8 action= 1 \n",
      "\n",
      "Next observation= [ 0.06625285  0.9802764  -0.12013721 -1.5675519 ] reward= 1.0 done= False \n",
      "\n",
      "step i 9 action= 1 \n",
      "\n",
      "Next observation= [ 0.08585837  1.1766111  -0.15148824 -1.8951666 ] reward= 1.0 done= False \n",
      "\n",
      "step i 10 action= 1 \n",
      "\n",
      "Next observation= [ 0.1093906   1.3730165  -0.18939158 -2.230768  ] reward= 1.0 done= False \n",
      "\n",
      "step i 11 action= 1 \n",
      "\n",
      "Next observation= [ 0.13685092  1.5693669  -0.23400694 -2.575376  ] reward= 1.0 done= True \n",
      "\n",
      "Iteration No: 32 TimeSteps: 11  Success: 0 Best Q: 1 Learning rate: 0.5 Total reward: 12.0\n",
      "Episode 32 terminated after 11 Iterations \n",
      "step i 0 action= 1 \n",
      "\n",
      "Next observation= [ 0.04273061  0.16530423 -0.03482564 -0.26507756] reward= 1.0 done= False \n",
      "\n",
      "step i 1 action= 1 \n",
      "\n",
      "Next observation= [ 0.04603669  0.3609055  -0.04012719 -0.5685382 ] reward= 1.0 done= False \n",
      "\n",
      "step i 2 action= 1 \n",
      "\n",
      "Next observation= [ 0.0532548   0.5565666  -0.05149795 -0.87358785] reward= 1.0 done= False \n",
      "\n",
      "step i 3 action= 1 \n",
      "\n",
      "Next observation= [ 0.06438614  0.75234956 -0.06896971 -1.1820066 ] reward= 1.0 done= False \n",
      "\n",
      "step i 4 action= 1 \n",
      "\n",
      "Next observation= [ 0.07943312  0.9482955  -0.09260985 -1.4954877 ] reward= 1.0 done= False \n",
      "\n",
      "step i 5 action= 0 \n",
      "\n",
      "Next observation= [ 0.09839904  0.7544136  -0.1225196  -1.2330996 ] reward= 1.0 done= False \n",
      "\n",
      "step i 6 action= 1 \n",
      "\n",
      "Next observation= [ 0.1134873   0.95087916 -0.14718159 -1.5615195 ] reward= 1.0 done= False \n",
      "\n",
      "step i 7 action= 1 \n",
      "\n",
      "Next observation= [ 0.1325049   1.1474234  -0.17841198 -1.8962637 ] reward= 1.0 done= False \n",
      "\n",
      "step i 8 action= 0 \n",
      "\n",
      "Next observation= [ 0.15545335  0.9546279  -0.21633725 -1.6638362 ] reward= 1.0 done= True \n",
      "\n",
      "Iteration No: 33 TimeSteps: 8  Success: 0 Best Q: 1 Learning rate: 0.5 Total reward: 9.0\n",
      "Episode 33 terminated after 8 Iterations \n",
      "step i 0 action= 1 \n",
      "\n",
      "Next observation= [ 0.01879534  0.22704588  0.00448381 -0.26446143] reward= 1.0 done= False \n",
      "\n",
      "step i 1 action= 0 \n",
      "\n",
      "Next observation= [ 0.02333625  0.03186021 -0.00080541  0.02963235] reward= 1.0 done= False \n",
      "\n",
      "step i 2 action= 1 \n",
      "\n",
      "Next observation= [ 2.3973459e-02  2.2699371e-01 -2.1276767e-04 -2.6330459e-01] reward= 1.0 done= False \n",
      "\n",
      "step i 3 action= 0 \n",
      "\n",
      "Next observation= [ 0.02851333  0.03187479 -0.00547886  0.02931122] reward= 1.0 done= False \n",
      "\n",
      "step i 4 action= 1 \n",
      "\n",
      "Next observation= [ 0.02915083  0.22707488 -0.00489264 -0.2650953 ] reward= 1.0 done= False \n",
      "\n",
      "step i 5 action= 1 \n",
      "\n",
      "Next observation= [ 0.03369233  0.42226633 -0.01019454 -0.5593174 ] reward= 1.0 done= False \n",
      "\n",
      "step i 6 action= 1 \n",
      "\n",
      "Next observation= [ 0.04213765  0.61752987 -0.02138089 -0.8551946 ] reward= 1.0 done= False \n",
      "\n",
      "step i 7 action= 1 \n",
      "\n",
      "Next observation= [ 0.05448825  0.81293654 -0.03848478 -1.1545231 ] reward= 1.0 done= False \n",
      "\n",
      "step i 8 action= 0 \n",
      "\n",
      "Next observation= [ 0.07074698  0.61833704 -0.06157525 -0.87415177] reward= 1.0 done= False \n",
      "\n",
      "step i 9 action= 1 \n",
      "\n",
      "Next observation= [ 0.08311372  0.8142397  -0.07905828 -1.1855406 ] reward= 1.0 done= False \n",
      "\n",
      "step i 10 action= 1 \n",
      "\n",
      "Next observation= [ 0.09939852  1.010293   -0.10276909 -1.5019208 ] reward= 1.0 done= False \n",
      "\n",
      "step i 11 action= 1 \n",
      "\n",
      "Next observation= [ 0.11960438  1.2065016  -0.13280751 -1.8248419 ] reward= 1.0 done= False \n",
      "\n",
      "step i 12 action= 1 \n",
      "\n",
      "Next observation= [ 0.14373441  1.4028238  -0.16930434 -2.1556628 ] reward= 1.0 done= False \n",
      "\n",
      "step i 13 action= 0 \n",
      "\n",
      "Next observation= [ 0.17179088  1.209721   -0.2124176  -1.919688  ] reward= 1.0 done= True \n",
      "\n",
      "Iteration No: 34 TimeSteps: 13  Success: 0 Best Q: 1 Learning rate: 0.5 Total reward: 14.0\n",
      "Episode 34 terminated after 13 Iterations \n",
      "step i 0 action= 1 \n",
      "\n",
      "Next observation= [ 0.03163803  0.20239308 -0.02354336 -0.32319805] reward= 1.0 done= False \n",
      "\n",
      "step i 1 action= 0 \n",
      "\n",
      "Next observation= [ 0.03568589  0.00761415 -0.03000732 -0.03803174] reward= 1.0 done= False \n",
      "\n",
      "step i 2 action= 1 \n",
      "\n",
      "Next observation= [ 0.03583818  0.20315327 -0.03076795 -0.3400292 ] reward= 1.0 done= False \n",
      "\n",
      "step i 3 action= 0 \n",
      "\n",
      "Next observation= [ 0.03990124  0.00848231 -0.03756854 -0.05720531] reward= 1.0 done= False \n",
      "\n",
      "step i 4 action= 1 \n",
      "\n",
      "Next observation= [ 0.04007089  0.20412223 -0.03871264 -0.36150068] reward= 1.0 done= False \n",
      "\n",
      "step i 5 action= 0 \n",
      "\n",
      "Next observation= [ 0.04415333  0.00957131 -0.04594266 -0.08127161] reward= 1.0 done= False \n",
      "\n",
      "step i 6 action= 1 \n",
      "\n",
      "Next observation= [ 0.04434476  0.20532072 -0.04756809 -0.3880883 ] reward= 1.0 done= False \n",
      "\n",
      "step i 7 action= 1 \n",
      "\n",
      "Next observation= [ 0.04845117  0.40108445 -0.05532985 -0.69538146] reward= 1.0 done= False \n",
      "\n",
      "step i 8 action= 1 \n",
      "\n",
      "Next observation= [ 0.05647286  0.59692836 -0.06923749 -1.0049565 ] reward= 1.0 done= False \n",
      "\n",
      "step i 9 action= 1 \n",
      "\n",
      "Next observation= [ 0.06841143  0.7929033  -0.08933661 -1.3185542 ] reward= 1.0 done= False \n",
      "\n",
      "step i 10 action= 0 \n",
      "\n",
      "Next observation= [ 0.0842695  0.5990174 -0.1157077 -1.0551151] reward= 1.0 done= False \n",
      "\n",
      "step i 11 action= 0 \n",
      "\n",
      "Next observation= [ 0.09624985  0.40560332 -0.13681    -0.80087614] reward= 1.0 done= False \n",
      "\n",
      "step i 12 action= 0 \n",
      "\n",
      "Next observation= [ 0.10436191  0.21259637 -0.15282752 -0.5541676 ] reward= 1.0 done= False \n",
      "\n",
      "step i 13 action= 0 \n",
      "\n",
      "Next observation= [ 0.10861384  0.01991335 -0.16391088 -0.31326842] reward= 1.0 done= False \n",
      "\n",
      "step i 14 action= 0 \n",
      "\n",
      "Next observation= [ 0.1090121  -0.17254041 -0.17017624 -0.07643134] reward= 1.0 done= False \n",
      "\n",
      "step i 15 action= 0 \n",
      "\n",
      "Next observation= [ 0.1055613  -0.364866   -0.17170487  0.15809909] reward= 1.0 done= False \n",
      "\n",
      "step i 16 action= 1 \n",
      "\n",
      "Next observation= [ 0.09826398 -0.16775487 -0.16854289 -0.1834533 ] reward= 1.0 done= False \n",
      "\n",
      "step i 17 action= 0 \n",
      "\n",
      "Next observation= [ 0.09490888 -0.36011463 -0.17221196  0.05168046] reward= 1.0 done= False \n",
      "\n",
      "step i 18 action= 1 \n",
      "\n",
      "Next observation= [ 0.08770659 -0.16299544 -0.17117834 -0.29000512] reward= 1.0 done= False \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step i 19 action= 0 \n",
      "\n",
      "Next observation= [ 0.08444668 -0.35531577 -0.17697844 -0.05582187] reward= 1.0 done= False \n",
      "\n",
      "step i 20 action= 1 \n",
      "\n",
      "Next observation= [ 0.07734036 -0.15815604 -0.17809488 -0.3987025 ] reward= 1.0 done= False \n",
      "\n",
      "step i 21 action= 0 \n",
      "\n",
      "Next observation= [ 0.07417724 -0.3503636  -0.18606894 -0.16703494] reward= 1.0 done= False \n",
      "\n",
      "step i 22 action= 1 \n",
      "\n",
      "Next observation= [ 0.06716997 -0.15313326 -0.18940963 -0.51216304] reward= 1.0 done= False \n",
      "\n",
      "step i 23 action= 0 \n",
      "\n",
      "Next observation= [ 0.06410731 -0.34515342 -0.1996529  -0.2846381 ] reward= 1.0 done= False \n",
      "\n",
      "step i 24 action= 1 \n",
      "\n",
      "Next observation= [ 0.05720424 -0.14782725 -0.20534566 -0.63305646] reward= 1.0 done= False \n",
      "\n",
      "step i 25 action= 0 \n",
      "\n",
      "Next observation= [ 0.05424769 -0.33958268 -0.21800679 -0.41141453] reward= 1.0 done= True \n",
      "\n",
      "Iteration No: 35 TimeSteps: 25  Success: 0 Best Q: 1 Learning rate: 0.5 Total reward: 26.0\n",
      "Episode 35 terminated after 25 Iterations \n",
      "step i 0 action= 1 \n",
      "\n",
      "Next observation= [-0.02567902  0.15384853 -0.00727252 -0.3311735 ] reward= 1.0 done= False \n",
      "\n",
      "step i 1 action= 0 \n",
      "\n",
      "Next observation= [-0.02260205 -0.04116916 -0.01389599 -0.04079282] reward= 1.0 done= False \n",
      "\n",
      "step i 2 action= 1 \n",
      "\n",
      "Next observation= [-0.02342544  0.15414928 -0.01471185 -0.33782747] reward= 1.0 done= False \n",
      "\n",
      "step i 3 action= 0 \n",
      "\n",
      "Next observation= [-0.02034245 -0.04076027 -0.0214684  -0.04981992] reward= 1.0 done= False \n",
      "\n",
      "step i 4 action= 1 \n",
      "\n",
      "Next observation= [-0.02115766  0.15466283 -0.0224648  -0.34919825] reward= 1.0 done= False \n",
      "\n",
      "step i 5 action= 0 \n",
      "\n",
      "Next observation= [-0.0180644  -0.04013253 -0.02944876 -0.06368303] reward= 1.0 done= False \n",
      "\n",
      "step i 6 action= 1 \n",
      "\n",
      "Next observation= [-0.01886705  0.155399   -0.03072242 -0.36550984] reward= 1.0 done= False \n",
      "\n",
      "step i 7 action= 0 \n",
      "\n",
      "Next observation= [-0.01575907 -0.03927319 -0.03803262 -0.08267032] reward= 1.0 done= False \n",
      "\n",
      "step i 8 action= 1 \n",
      "\n",
      "Next observation= [-0.01654453  0.15637273 -0.03968602 -0.38710588] reward= 1.0 done= False \n",
      "\n",
      "step i 9 action= 0 \n",
      "\n",
      "Next observation= [-0.01341708 -0.03816406 -0.04742814 -0.10719508] reward= 1.0 done= False \n",
      "\n",
      "step i 10 action= 1 \n",
      "\n",
      "Next observation= [-0.01418036  0.15760434 -0.04957204 -0.4144561 ] reward= 1.0 done= False \n",
      "\n",
      "step i 11 action= 0 \n",
      "\n",
      "Next observation= [-0.01102827 -0.03678123 -0.05786116 -0.13780417] reward= 1.0 done= False \n",
      "\n",
      "step i 12 action= 0 \n",
      "\n",
      "Next observation= [-0.0117639  -0.23102875 -0.06061725  0.13607784] reward= 1.0 done= False \n",
      "\n",
      "step i 13 action= 0 \n",
      "\n",
      "Next observation= [-0.01638447 -0.42523244 -0.05789569  0.40903777] reward= 1.0 done= False \n",
      "\n",
      "step i 14 action= 1 \n",
      "\n",
      "Next observation= [-0.02488912 -0.22933947 -0.04971494  0.09867881] reward= 1.0 done= False \n",
      "\n",
      "step i 15 action= 1 \n",
      "\n",
      "Next observation= [-0.02947591 -0.03354156 -0.04774136 -0.20926535] reward= 1.0 done= False \n",
      "\n",
      "step i 16 action= 1 \n",
      "\n",
      "Next observation= [-0.03014674  0.16222937 -0.05192667 -0.5166178 ] reward= 1.0 done= False \n",
      "\n",
      "step i 17 action= 0 \n",
      "\n",
      "Next observation= [-0.02690216 -0.03212442 -0.06225902 -0.24073963] reward= 1.0 done= False \n",
      "\n",
      "step i 18 action= 0 \n",
      "\n",
      "Next observation= [-0.02754464 -0.22630431 -0.06707381  0.03167355] reward= 1.0 done= False \n",
      "\n",
      "step i 19 action= 1 \n",
      "\n",
      "Next observation= [-0.03207073 -0.03028779 -0.06644034 -0.281395  ] reward= 1.0 done= False \n",
      "\n",
      "step i 20 action= 0 \n",
      "\n",
      "Next observation= [-0.03267648 -0.22440222 -0.07206824 -0.01038488] reward= 1.0 done= False \n",
      "\n",
      "step i 21 action= 0 \n",
      "\n",
      "Next observation= [-0.03716453 -0.41842058 -0.07227594  0.2587175 ] reward= 1.0 done= False \n",
      "\n",
      "step i 22 action= 1 \n",
      "\n",
      "Next observation= [-0.04553294 -0.22234522 -0.06710159 -0.05585831] reward= 1.0 done= False \n",
      "\n",
      "step i 23 action= 0 \n",
      "\n",
      "Next observation= [-0.04997985 -0.41644406 -0.06821876  0.21492168] reward= 1.0 done= False \n",
      "\n",
      "step i 24 action= 1 \n",
      "\n",
      "Next observation= [-0.05830873 -0.22041652 -0.06392033 -0.09847647] reward= 1.0 done= False \n",
      "\n",
      "step i 25 action= 1 \n",
      "\n",
      "Next observation= [-0.06271706 -0.02443947 -0.06588985 -0.4106215 ] reward= 1.0 done= False \n",
      "\n",
      "step i 26 action= 0 \n",
      "\n",
      "Next observation= [-0.06320585 -0.21856844 -0.07410228 -0.1394175 ] reward= 1.0 done= False \n",
      "\n",
      "step i 27 action= 0 \n",
      "\n",
      "Next observation= [-0.06757721 -0.41255516 -0.07689063  0.12899788] reward= 1.0 done= False \n",
      "\n",
      "step i 28 action= 1 \n",
      "\n",
      "Next observation= [-0.07582832 -0.21642074 -0.07431068 -0.18691805] reward= 1.0 done= False \n",
      "\n",
      "step i 29 action= 0 \n",
      "\n",
      "Next observation= [-0.08015674 -0.4104052  -0.07804903  0.08142834] reward= 1.0 done= False \n",
      "\n",
      "step i 30 action= 1 \n",
      "\n",
      "Next observation= [-0.08836484 -0.2142562  -0.07642047 -0.23482257] reward= 1.0 done= False \n",
      "\n",
      "step i 31 action= 0 \n",
      "\n",
      "Next observation= [-0.09264996 -0.4082078  -0.08111692  0.03280999] reward= 1.0 done= False \n",
      "\n",
      "step i 32 action= 1 \n",
      "\n",
      "Next observation= [-0.10081412 -0.21202196 -0.08046072 -0.2843234 ] reward= 1.0 done= False \n",
      "\n",
      "step i 33 action= 0 \n",
      "\n",
      "Next observation= [-0.10505456 -0.40590966 -0.08614719 -0.01806268] reward= 1.0 done= False \n",
      "\n",
      "step i 34 action= 1 \n",
      "\n",
      "Next observation= [-0.11317275 -0.20966466 -0.08650845 -0.33663452] reward= 1.0 done= False \n",
      "\n",
      "step i 35 action= 0 \n",
      "\n",
      "Next observation= [-0.11736605 -0.40345582 -0.09324113 -0.07243655] reward= 1.0 done= False \n",
      "\n",
      "step i 36 action= 1 \n",
      "\n",
      "Next observation= [-0.12543516 -0.2071294  -0.09468986 -0.39302018] reward= 1.0 done= False \n",
      "\n",
      "step i 37 action= 0 \n",
      "\n",
      "Next observation= [-0.12957776 -0.40078905 -0.10255027 -0.13162927] reward= 1.0 done= False \n",
      "\n",
      "step i 38 action= 1 \n",
      "\n",
      "Next observation= [-0.13759352 -0.20435902 -0.10518286 -0.4548233 ] reward= 1.0 done= False \n",
      "\n",
      "step i 39 action= 0 \n",
      "\n",
      "Next observation= [-0.14168072 -0.39784873 -0.11427932 -0.19705953] reward= 1.0 done= False \n",
      "\n",
      "step i 40 action= 0 \n",
      "\n",
      "Next observation= [-0.14963768 -0.5911664  -0.11822051  0.05750045] reward= 1.0 done= False \n",
      "\n",
      "step i 41 action= 1 \n",
      "\n",
      "Next observation= [-0.16146101 -0.39456522 -0.1170705  -0.27001882] reward= 1.0 done= False \n",
      "\n",
      "step i 42 action= 1 \n",
      "\n",
      "Next observation= [-0.16935232 -0.19798414 -0.12247088 -0.5972122 ] reward= 1.0 done= False \n",
      "\n",
      "step i 43 action= 1 \n",
      "\n",
      "Next observation= [-0.173312   -0.0013805  -0.13441512 -0.9258253 ] reward= 1.0 done= False \n",
      "\n",
      "step i 44 action= 1 \n",
      "\n",
      "Next observation= [-0.1733396   0.19527555 -0.15293163 -1.2575477 ] reward= 1.0 done= False \n",
      "\n",
      "step i 45 action= 1 \n",
      "\n",
      "Next observation= [-0.1694341   0.39198765 -0.17808259 -1.5939589 ] reward= 1.0 done= False \n",
      "\n",
      "step i 46 action= 0 \n",
      "\n",
      "Next observation= [-0.16159435  0.19936919 -0.20996176 -1.3616805 ] reward= 1.0 done= True \n",
      "\n",
      "Iteration No: 36 TimeSteps: 46  Success: 1 Best Q: 1 Learning rate: 0.5 Total reward: 47.0\n",
      "Episode 36 terminated after 46 Iterations \n",
      "step i 0 action= 1 \n",
      "\n",
      "Next observation= [-0.0383501   0.20579514 -0.00645405 -0.26855537] reward= 1.0 done= False \n",
      "\n",
      "step i 1 action= 1 \n",
      "\n",
      "Next observation= [-0.0342342   0.4010086  -0.01182516 -0.56326693] reward= 1.0 done= False \n",
      "\n",
      "step i 2 action= 1 \n",
      "\n",
      "Next observation= [-0.02621403  0.59629446 -0.0230905  -0.85965174] reward= 1.0 done= False \n",
      "\n",
      "step i 3 action= 1 \n",
      "\n",
      "Next observation= [-0.01428814  0.7917232  -0.04028353 -1.1595047 ] reward= 1.0 done= False \n",
      "\n",
      "step i 4 action= 0 \n",
      "\n",
      "Next observation= [ 0.00154633  0.5971486  -0.06347363 -0.87971973] reward= 1.0 done= False \n",
      "\n",
      "step i 5 action= 1 \n",
      "\n",
      "Next observation= [ 0.0134893   0.7930728  -0.08106802 -1.191663  ] reward= 1.0 done= False \n",
      "\n",
      "step i 6 action= 1 \n",
      "\n",
      "Next observation= [ 0.02935076  0.98914605 -0.10490128 -1.5086148 ] reward= 1.0 done= False \n",
      "\n",
      "step i 7 action= 1 \n",
      "\n",
      "Next observation= [ 0.04913368  1.1853715  -0.13507359 -1.8321195 ] reward= 1.0 done= False \n",
      "\n",
      "step i 8 action= 1 \n",
      "\n",
      "Next observation= [ 0.07284111  1.3817056  -0.17171597 -2.1635292 ] reward= 1.0 done= False \n",
      "\n",
      "step i 9 action= 1 \n",
      "\n",
      "Next observation= [ 0.10047522  1.5780424  -0.21498655 -2.5039396 ] reward= 1.0 done= True \n",
      "\n",
      "Iteration No: 37 TimeSteps: 9  Success: 2 Best Q: 1 Learning rate: 0.5 Total reward: 10.0\n",
      "Episode 37 terminated after 9 Iterations \n",
      "step i 0 action= 0 \n",
      "\n",
      "Next observation= [ 0.01857852 -0.14595728  0.009038    0.33096555] reward= 1.0 done= False \n",
      "\n",
      "step i 1 action= 1 \n",
      "\n",
      "Next observation= [0.01565937 0.04903486 0.01565731 0.04114641] reward= 1.0 done= False \n",
      "\n",
      "step i 2 action= 1 \n",
      "\n",
      "Next observation= [ 0.01664007  0.24392883  0.01648024 -0.24655564] reward= 1.0 done= False \n",
      "\n",
      "step i 3 action= 0 \n",
      "\n",
      "Next observation= [0.02151865 0.04857543 0.01154912 0.05127963] reward= 1.0 done= False \n",
      "\n",
      "step i 4 action= 1 \n",
      "\n",
      "Next observation= [ 0.02249015  0.2435299   0.01257472 -0.2377372 ] reward= 1.0 done= False \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step i 5 action= 1 \n",
      "\n",
      "Next observation= [ 0.02736075  0.43846998  0.00781997 -0.5264273 ] reward= 1.0 done= False \n",
      "\n",
      "step i 6 action= 0 \n",
      "\n",
      "Next observation= [ 0.03613015  0.24323885 -0.00270857 -0.23129055] reward= 1.0 done= False \n",
      "\n",
      "step i 7 action= 1 \n",
      "\n",
      "Next observation= [ 0.04099493  0.4383994  -0.00733438 -0.52482665] reward= 1.0 done= False \n",
      "\n",
      "step i 8 action= 1 \n",
      "\n",
      "Next observation= [ 0.04976292  0.6336238  -0.01783092 -0.81981164] reward= 1.0 done= False \n",
      "\n",
      "step i 9 action= 1 \n",
      "\n",
      "Next observation= [ 0.06243539  0.82898515 -0.03422715 -1.1180491 ] reward= 1.0 done= False \n",
      "\n",
      "step i 10 action= 0 \n",
      "\n",
      "Next observation= [ 0.0790151   0.63432866 -0.05658813 -0.8362962 ] reward= 1.0 done= False \n",
      "\n",
      "step i 11 action= 1 \n",
      "\n",
      "Next observation= [ 0.09170167  0.830176   -0.07331406 -1.146225  ] reward= 1.0 done= False \n",
      "\n",
      "step i 12 action= 1 \n",
      "\n",
      "Next observation= [ 0.10830519  1.0261748  -0.09623855 -1.4609685 ] reward= 1.0 done= False \n",
      "\n",
      "step i 13 action= 1 \n",
      "\n",
      "Next observation= [ 0.12882869  1.2223358  -0.12545793 -1.7820989 ] reward= 1.0 done= False \n",
      "\n",
      "step i 14 action= 1 \n",
      "\n",
      "Next observation= [ 0.1532754  1.4186256 -0.1610999 -2.1110075] reward= 1.0 done= False \n",
      "\n",
      "step i 15 action= 1 \n",
      "\n",
      "Next observation= [ 0.18164791  1.6149513  -0.20332006 -2.4488413 ] reward= 1.0 done= False \n",
      "\n",
      "step i 16 action= 1 \n",
      "\n",
      "Next observation= [ 0.21394694  1.8111427  -0.25229686 -2.7964318 ] reward= 1.0 done= True \n",
      "\n",
      "Iteration No: 38 TimeSteps: 16  Success: 0 Best Q: 1 Learning rate: 0.5 Total reward: 17.0\n",
      "Episode 38 terminated after 16 Iterations \n",
      "step i 0 action= 1 \n",
      "\n",
      "Next observation= [ 0.03795569  0.22570476  0.01919848 -0.31831393] reward= 1.0 done= False \n",
      "\n",
      "step i 1 action= 1 \n",
      "\n",
      "Next observation= [ 0.04246978  0.42054808  0.0128322  -0.60488105] reward= 1.0 done= False \n",
      "\n",
      "step i 2 action= 1 \n",
      "\n",
      "Next observation= [ 5.0880741e-02  6.1548823e-01  7.3457585e-04 -8.9349467e-01] reward= 1.0 done= False \n",
      "\n",
      "step i 3 action= 1 \n",
      "\n",
      "Next observation= [ 0.0631905   0.8106002  -0.01713532 -1.1859466 ] reward= 1.0 done= False \n",
      "\n",
      "step i 4 action= 0 \n",
      "\n",
      "Next observation= [ 0.07940251  0.61570466 -0.04085425 -0.8986837 ] reward= 1.0 done= False \n",
      "\n",
      "step i 5 action= 1 \n",
      "\n",
      "Next observation= [ 0.0917166   0.8113558  -0.05882793 -1.2039233 ] reward= 1.0 done= False \n",
      "\n",
      "step i 6 action= 1 \n",
      "\n",
      "Next observation= [ 0.10794372  1.0071868  -0.08290639 -1.5144472 ] reward= 1.0 done= False \n",
      "\n",
      "step i 7 action= 1 \n",
      "\n",
      "Next observation= [ 0.12808746  1.2032087  -0.11319534 -1.8318166 ] reward= 1.0 done= False \n",
      "\n",
      "step i 8 action= 1 \n",
      "\n",
      "Next observation= [ 0.15215163  1.3993872  -0.14983167 -2.1574097 ] reward= 1.0 done= False \n",
      "\n",
      "step i 9 action= 1 \n",
      "\n",
      "Next observation= [ 0.18013938  1.5956283  -0.19297986 -2.492359  ] reward= 1.0 done= False \n",
      "\n",
      "step i 10 action= 1 \n",
      "\n",
      "Next observation= [ 0.21205194  1.7917595  -0.24282704 -2.8374794 ] reward= 1.0 done= True \n",
      "\n",
      "Iteration No: 39 TimeSteps: 10  Success: 0 Best Q: 1 Learning rate: 0.5 Total reward: 11.0\n",
      "Episode 39 terminated after 10 Iterations \n",
      "step i 0 action= 1 \n",
      "\n",
      "Next observation= [ 0.01346157  0.16975679  0.02526141 -0.2522427 ] reward= 1.0 done= False \n",
      "\n",
      "step i 1 action= 1 \n",
      "\n",
      "Next observation= [ 0.01685671  0.36450908  0.02021656 -0.5368519 ] reward= 1.0 done= False \n",
      "\n",
      "step i 2 action= 0 \n",
      "\n",
      "Next observation= [ 0.02414689  0.16910881  0.00947952 -0.2378681 ] reward= 1.0 done= False \n",
      "\n",
      "step i 3 action= 1 \n",
      "\n",
      "Next observation= [ 0.02752906  0.36409405  0.00472216 -0.52754587] reward= 1.0 done= False \n",
      "\n",
      "step i 4 action= 1 \n",
      "\n",
      "Next observation= [ 0.03481095  0.55914927 -0.00582876 -0.8187371 ] reward= 1.0 done= False \n",
      "\n",
      "step i 5 action= 1 \n",
      "\n",
      "Next observation= [ 0.04599393  0.7543505  -0.0222035  -1.1132476 ] reward= 1.0 done= False \n",
      "\n",
      "step i 6 action= 0 \n",
      "\n",
      "Next observation= [ 0.06108094  0.55952704 -0.04446846 -0.8276118 ] reward= 1.0 done= False \n",
      "\n",
      "step i 7 action= 1 \n",
      "\n",
      "Next observation= [ 0.07227148  0.7552279  -0.06102069 -1.1339424 ] reward= 1.0 done= False \n",
      "\n",
      "step i 8 action= 1 \n",
      "\n",
      "Next observation= [ 0.08737604  0.951093   -0.08369954 -1.4451221 ] reward= 1.0 done= False \n",
      "\n",
      "step i 9 action= 1 \n",
      "\n",
      "Next observation= [ 0.1063979   1.1471392  -0.11260198 -1.7627409 ] reward= 1.0 done= False \n",
      "\n",
      "step i 10 action= 1 \n",
      "\n",
      "Next observation= [ 0.12934068  1.3433405  -0.1478568  -2.0882142 ] reward= 1.0 done= False \n",
      "\n",
      "step i 11 action= 1 \n",
      "\n",
      "Next observation= [ 0.15620749  1.5396137  -0.18962108 -2.4227233 ] reward= 1.0 done= False \n",
      "\n",
      "step i 12 action= 0 \n",
      "\n",
      "Next observation= [ 0.18699977  1.346569   -0.23807554 -2.1937616 ] reward= 1.0 done= True \n",
      "\n",
      "Iteration No: 40 TimeSteps: 12  Success: 0 Best Q: 1 Learning rate: 0.5 Total reward: 13.0\n",
      "Episode 40 terminated after 12 Iterations \n",
      "step i 0 action= 1 \n",
      "\n",
      "Next observation= [ 0.02196483  0.16886419  0.03445112 -0.2540283 ] reward= 1.0 done= False \n",
      "\n",
      "step i 1 action= 1 \n",
      "\n",
      "Next observation= [ 0.02534211  0.3634777   0.02937056 -0.53564876] reward= 1.0 done= False \n",
      "\n",
      "step i 2 action= 1 \n",
      "\n",
      "Next observation= [ 0.03261167  0.5581746   0.01865758 -0.8189345 ] reward= 1.0 done= False \n",
      "\n",
      "step i 3 action= 1 \n",
      "\n",
      "Next observation= [ 0.04377516  0.7530363   0.00227889 -1.1056911 ] reward= 1.0 done= False \n",
      "\n",
      "step i 4 action= 0 \n",
      "\n",
      "Next observation= [ 0.05883589  0.55788445 -0.01983493 -0.8122941 ] reward= 1.0 done= False \n",
      "\n",
      "step i 5 action= 1 \n",
      "\n",
      "Next observation= [ 0.06999358  0.7532724  -0.03608081 -1.1111494 ] reward= 1.0 done= False \n",
      "\n",
      "step i 6 action= 0 \n",
      "\n",
      "Next observation= [ 0.08505902  0.55864257 -0.0583038  -0.83000016] reward= 1.0 done= False \n",
      "\n",
      "step i 7 action= 1 \n",
      "\n",
      "Next observation= [ 0.09623188  0.75451094 -0.0749038  -1.1404351 ] reward= 1.0 done= False \n",
      "\n",
      "step i 8 action= 1 \n",
      "\n",
      "Next observation= [ 0.1113221   0.95052785 -0.09771251 -1.4556371 ] reward= 1.0 done= False \n",
      "\n",
      "step i 9 action= 1 \n",
      "\n",
      "Next observation= [ 0.13033265  1.146704   -0.12682524 -1.7771794 ] reward= 1.0 done= False \n",
      "\n",
      "step i 10 action= 1 \n",
      "\n",
      "Next observation= [ 0.15326673  1.3430058  -0.16236883 -2.106454  ] reward= 1.0 done= False \n",
      "\n",
      "step i 11 action= 1 \n",
      "\n",
      "Next observation= [ 0.18012685  1.5393404  -0.20449792 -2.4446094 ] reward= 1.0 done= False \n",
      "\n",
      "step i 12 action= 1 \n",
      "\n",
      "Next observation= [ 0.21091366  1.735538   -0.2533901  -2.7924778 ] reward= 1.0 done= True \n",
      "\n",
      "Iteration No: 41 TimeSteps: 12  Success: 0 Best Q: 1 Learning rate: 0.5 Total reward: 13.0\n",
      "Episode 41 terminated after 12 Iterations \n",
      "step i 0 action= 1 \n",
      "\n",
      "Next observation= [ 0.02207054  0.15229343  0.00270298 -0.26147303] reward= 1.0 done= False \n",
      "\n",
      "step i 1 action= 1 \n",
      "\n",
      "Next observation= [ 0.02511641  0.3473767  -0.00252648 -0.55330217] reward= 1.0 done= False \n",
      "\n",
      "step i 2 action= 0 \n",
      "\n",
      "Next observation= [ 0.03206395  0.15229031 -0.01359253 -0.26141632] reward= 1.0 done= False \n",
      "\n",
      "step i 3 action= 1 \n",
      "\n",
      "Next observation= [ 0.03510975  0.34760362 -0.01882085 -0.55835533] reward= 1.0 done= False \n",
      "\n",
      "step i 4 action= 0 \n",
      "\n",
      "Next observation= [ 0.04206182  0.15275085 -0.02998796 -0.27166092] reward= 1.0 done= False \n",
      "\n",
      "step i 5 action= 1 \n",
      "\n",
      "Next observation= [ 0.04511684  0.3482876  -0.03542118 -0.5736493 ] reward= 1.0 done= False \n",
      "\n",
      "step i 6 action= 1 \n",
      "\n",
      "Next observation= [ 0.05208259  0.5438878  -0.04689417 -0.8772772 ] reward= 1.0 done= False \n",
      "\n",
      "step i 7 action= 0 \n",
      "\n",
      "Next observation= [ 0.06296035  0.34943345 -0.06443971 -0.5996982 ] reward= 1.0 done= False \n",
      "\n",
      "step i 8 action= 1 \n",
      "\n",
      "Next observation= [ 0.06994902  0.54539496 -0.07643367 -0.9119625 ] reward= 1.0 done= False \n",
      "\n",
      "step i 9 action= 1 \n",
      "\n",
      "Next observation= [ 0.08085692  0.74146324 -0.09467293 -1.2276559 ] reward= 1.0 done= False \n",
      "\n",
      "step i 10 action= 1 \n",
      "\n",
      "Next observation= [ 0.09568618  0.93766755 -0.11922604 -1.5484366 ] reward= 1.0 done= False \n",
      "\n",
      "step i 11 action= 1 \n",
      "\n",
      "Next observation= [ 0.11443953  1.1340017  -0.15019478 -1.8758168 ] reward= 1.0 done= False \n",
      "\n",
      "step i 12 action= 1 \n",
      "\n",
      "Next observation= [ 0.13711956  1.33041    -0.1877111  -2.211104  ] reward= 1.0 done= False \n",
      "\n",
      "step i 13 action= 1 \n",
      "\n",
      "Next observation= [ 0.16372776  1.5267708  -0.23193319 -2.5553346 ] reward= 1.0 done= True \n",
      "\n",
      "Iteration No: 42 TimeSteps: 13  Success: 0 Best Q: 1 Learning rate: 0.5 Total reward: 14.0\n",
      "Episode 42 terminated after 13 Iterations \n",
      "step i 0 action= 1 \n",
      "\n",
      "Next observation= [-0.04100757  0.20263986 -0.0285522  -0.3102673 ] reward= 1.0 done= False \n",
      "\n",
      "step i 1 action= 1 \n",
      "\n",
      "Next observation= [-0.03695478  0.39815673 -0.03475755 -0.6118163 ] reward= 1.0 done= False \n",
      "\n",
      "step i 2 action= 1 \n",
      "\n",
      "Next observation= [-0.02899164  0.5937468  -0.04699387 -0.9152408 ] reward= 1.0 done= False \n",
      "\n",
      "step i 3 action= 1 \n",
      "\n",
      "Next observation= [-0.0171167   0.7894717  -0.06529869 -1.2223152 ] reward= 1.0 done= False \n",
      "\n",
      "step i 4 action= 1 \n",
      "\n",
      "Next observation= [-1.3272711e-03  9.8537135e-01 -8.9744993e-02 -1.5347226e+00] reward= 1.0 done= False \n",
      "\n",
      "step i 5 action= 1 \n",
      "\n",
      "Next observation= [ 0.01838016  1.1814523  -0.12043945 -1.8540099 ] reward= 1.0 done= False \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step i 6 action= 1 \n",
      "\n",
      "Next observation= [ 0.0420092   1.3776747  -0.15751964 -2.181535  ] reward= 1.0 done= False \n",
      "\n",
      "step i 7 action= 1 \n",
      "\n",
      "Next observation= [ 0.0695627   1.5739367  -0.20115034 -2.5184028 ] reward= 1.0 done= False \n",
      "\n",
      "step i 8 action= 0 \n",
      "\n",
      "Next observation= [ 0.10104143  1.3809499  -0.2515184  -2.2934992 ] reward= 1.0 done= True \n",
      "\n",
      "Iteration No: 43 TimeSteps: 8  Success: 0 Best Q: 1 Learning rate: 0.5 Total reward: 9.0\n",
      "Episode 43 terminated after 8 Iterations \n",
      "step i 0 action= 1 \n",
      "\n",
      "Next observation= [ 0.0155891   0.14797373  0.00051503 -0.26215377] reward= 1.0 done= False \n",
      "\n",
      "step i 1 action= 1 \n",
      "\n",
      "Next observation= [ 0.01854857  0.34308833 -0.00472804 -0.5546742 ] reward= 1.0 done= False \n",
      "\n",
      "step i 2 action= 1 \n",
      "\n",
      "Next observation= [ 0.02541034  0.5382763  -0.01582153 -0.848843  ] reward= 1.0 done= False \n",
      "\n",
      "step i 3 action= 1 \n",
      "\n",
      "Next observation= [ 0.03617587  0.73361045 -0.03279839 -1.1464589 ] reward= 1.0 done= False \n",
      "\n",
      "step i 4 action= 0 \n",
      "\n",
      "Next observation= [ 0.05084807  0.5389318  -0.05572756 -0.8642389 ] reward= 1.0 done= False \n",
      "\n",
      "step i 5 action= 1 \n",
      "\n",
      "Next observation= [ 0.06162671  0.7347663  -0.07301234 -1.17391   ] reward= 1.0 done= False \n",
      "\n",
      "step i 6 action= 1 \n",
      "\n",
      "Next observation= [ 0.07632203  0.9307572  -0.09649055 -1.4885598 ] reward= 1.0 done= False \n",
      "\n",
      "step i 7 action= 1 \n",
      "\n",
      "Next observation= [ 0.09493718  1.126913   -0.12626174 -1.8097489 ] reward= 1.0 done= False \n",
      "\n",
      "step i 8 action= 1 \n",
      "\n",
      "Next observation= [ 0.11747544  1.3231964  -0.16245672 -2.1388526 ] reward= 1.0 done= False \n",
      "\n",
      "step i 9 action= 0 \n",
      "\n",
      "Next observation= [ 0.14393936  1.1300114  -0.20523377 -1.9004433 ] reward= 1.0 done= False \n",
      "\n",
      "step i 10 action= 1 \n",
      "\n",
      "Next observation= [ 0.1665396   1.326679   -0.24324264 -2.2491696 ] reward= 1.0 done= True \n",
      "\n",
      "Iteration No: 44 TimeSteps: 10  Success: 0 Best Q: 1 Learning rate: 0.5 Total reward: 11.0\n",
      "Episode 44 terminated after 10 Iterations \n",
      "step i 0 action= 1 \n",
      "\n",
      "Next observation= [-0.04904507  0.20078644 -0.02964505 -0.33742306] reward= 1.0 done= False \n",
      "\n",
      "step i 1 action= 0 \n",
      "\n",
      "Next observation= [-0.04502934  0.00609862 -0.03639351 -0.05423401] reward= 1.0 done= False \n",
      "\n",
      "step i 2 action= 1 \n",
      "\n",
      "Next observation= [-0.04490736  0.201723   -0.03747819 -0.35817358] reward= 1.0 done= False \n",
      "\n",
      "step i 3 action= 0 \n",
      "\n",
      "Next observation= [-0.04087291  0.00715333 -0.04464166 -0.07754005] reward= 1.0 done= False \n",
      "\n",
      "step i 4 action= 1 \n",
      "\n",
      "Next observation= [-0.04072984  0.20288588 -0.04619246 -0.38396665] reward= 1.0 done= False \n",
      "\n",
      "step i 5 action= 0 \n",
      "\n",
      "Next observation= [-0.03667212  0.00844914 -0.0538718  -0.10619841] reward= 1.0 done= False \n",
      "\n",
      "step i 6 action= 1 \n",
      "\n",
      "Next observation= [-0.03650314  0.20430006 -0.05599576 -0.41537923] reward= 1.0 done= False \n",
      "\n",
      "step i 7 action= 0 \n",
      "\n",
      "Next observation= [-0.03241714  0.0100146  -0.06430335 -0.14086197] reward= 1.0 done= False \n",
      "\n",
      "step i 8 action= 0 \n",
      "\n",
      "Next observation= [-0.03221685 -0.18413027 -0.06712059  0.13086131] reward= 1.0 done= False \n",
      "\n",
      "step i 9 action= 0 \n",
      "\n",
      "Next observation= [-0.03589945 -0.37822974 -0.06450336  0.40163627] reward= 1.0 done= False \n",
      "\n",
      "step i 10 action= 1 \n",
      "\n",
      "Next observation= [-0.04346405 -0.182255   -0.05647064  0.08933466] reward= 1.0 done= False \n",
      "\n",
      "step i 11 action= 0 \n",
      "\n",
      "Next observation= [-0.04710915 -0.37652397 -0.05468394  0.36368003] reward= 1.0 done= False \n",
      "\n",
      "step i 12 action= 0 \n",
      "\n",
      "Next observation= [-0.05463963 -0.5708278  -0.04741034  0.63863105] reward= 1.0 done= False \n",
      "\n",
      "step i 13 action= 0 \n",
      "\n",
      "Next observation= [-0.06605618 -0.7652578  -0.03463772  0.91601485] reward= 1.0 done= False \n",
      "\n",
      "step i 14 action= 0 \n",
      "\n",
      "Next observation= [-0.08136134 -0.9598946  -0.01631742  1.1976136 ] reward= 1.0 done= False \n",
      "\n",
      "step i 15 action= 1 \n",
      "\n",
      "Next observation= [-0.10055923 -0.76456535  0.00763485  0.8998615 ] reward= 1.0 done= False \n",
      "\n",
      "step i 16 action= 0 \n",
      "\n",
      "Next observation= [-0.11585053 -0.9597899   0.02563208  1.1949345 ] reward= 1.0 done= False \n",
      "\n",
      "step i 17 action= 1 \n",
      "\n",
      "Next observation= [-0.13504633 -0.76500905  0.04953077  0.9103942 ] reward= 1.0 done= False \n",
      "\n",
      "step i 18 action= 0 \n",
      "\n",
      "Next observation= [-0.15034652 -0.96076506  0.06773865  1.2182242 ] reward= 1.0 done= False \n",
      "\n",
      "step i 19 action= 1 \n",
      "\n",
      "Next observation= [-0.16956182 -0.7665787   0.09210314  0.94751257] reward= 1.0 done= False \n",
      "\n",
      "step i 20 action= 1 \n",
      "\n",
      "Next observation= [-0.18489338 -0.5728095   0.11105338  0.68513083] reward= 1.0 done= False \n",
      "\n",
      "step i 21 action= 1 \n",
      "\n",
      "Next observation= [-0.19634958 -0.37939027  0.12475601  0.4293718 ] reward= 1.0 done= False \n",
      "\n",
      "step i 22 action= 1 \n",
      "\n",
      "Next observation= [-0.20393738 -0.18623544  0.13334344  0.17847455] reward= 1.0 done= False \n",
      "\n",
      "step i 23 action= 1 \n",
      "\n",
      "Next observation= [-0.20766209  0.00675129  0.13691293 -0.06934892] reward= 1.0 done= False \n",
      "\n",
      "step i 24 action= 1 \n",
      "\n",
      "Next observation= [-0.20752707  0.19967183  0.13552596 -0.31589496] reward= 1.0 done= False \n",
      "\n",
      "step i 25 action= 1 \n",
      "\n",
      "Next observation= [-0.20353363  0.39262927  0.12920806 -0.56295437] reward= 1.0 done= False \n",
      "\n",
      "step i 26 action= 0 \n",
      "\n",
      "Next observation= [-0.19568105  0.1959536   0.11794896 -0.23251843] reward= 1.0 done= False \n",
      "\n",
      "step i 27 action= 1 \n",
      "\n",
      "Next observation= [-0.19176197  0.38921002  0.11329859 -0.48579234] reward= 1.0 done= False \n",
      "\n",
      "step i 28 action= 0 \n",
      "\n",
      "Next observation= [-0.18397777  0.1926869   0.10358275 -0.15965907] reward= 1.0 done= False \n",
      "\n",
      "step i 29 action= 1 \n",
      "\n",
      "Next observation= [-0.18012403  0.38618514  0.10038957 -0.41795182] reward= 1.0 done= False \n",
      "\n",
      "step i 30 action= 0 \n",
      "\n",
      "Next observation= [-0.17240033  0.18979453  0.09203053 -0.0953841 ] reward= 1.0 done= False \n",
      "\n",
      "step i 31 action= 1 \n",
      "\n",
      "Next observation= [-0.16860445  0.38348517  0.09012285 -0.3576718 ] reward= 1.0 done= False \n",
      "\n",
      "step i 32 action= 1 \n",
      "\n",
      "Next observation= [-0.16093473  0.57721806  0.08296942 -0.62063146] reward= 1.0 done= False \n",
      "\n",
      "step i 33 action= 1 \n",
      "\n",
      "Next observation= [-0.14939037  0.77108926  0.07055679 -0.8860729 ] reward= 1.0 done= False \n",
      "\n",
      "step i 34 action= 1 \n",
      "\n",
      "Next observation= [-0.13396859  0.96518606  0.05283533 -1.1557672 ] reward= 1.0 done= False \n",
      "\n",
      "step i 35 action= 1 \n",
      "\n",
      "Next observation= [-0.11466487  1.1595808   0.02971998 -1.4314262 ] reward= 1.0 done= False \n",
      "\n",
      "step i 36 action= 1 \n",
      "\n",
      "Next observation= [-9.1473252e-02  1.3543236e+00  1.0914620e-03 -1.7146749e+00] reward= 1.0 done= False \n",
      "\n",
      "step i 37 action= 1 \n",
      "\n",
      "Next observation= [-0.06438678  1.5494331  -0.03320204 -2.007018  ] reward= 1.0 done= False \n",
      "\n",
      "step i 38 action= 1 \n",
      "\n",
      "Next observation= [-0.03339812  1.7448846  -0.0733424  -2.3097932 ] reward= 1.0 done= False \n",
      "\n",
      "step i 39 action= 1 \n",
      "\n",
      "Next observation= [ 1.4995735e-03  1.9405963e+00 -1.1953826e-01 -2.6241150e+00] reward= 1.0 done= False \n",
      "\n",
      "step i 40 action= 1 \n",
      "\n",
      "Next observation= [ 0.0403115   2.1364114  -0.17202057 -2.9508023 ] reward= 1.0 done= False \n",
      "\n",
      "step i 41 action= 1 \n",
      "\n",
      "Next observation= [ 0.08303973  2.3320785  -0.2310366  -3.290296  ] reward= 1.0 done= True \n",
      "\n",
      "Iteration No: 45 TimeSteps: 41  Success: 0 Best Q: 1 Learning rate: 0.5 Total reward: 42.0\n",
      "Episode 45 terminated after 41 Iterations \n",
      "step i 0 action= 0 \n",
      "\n",
      "Next observation= [ 0.00452255 -0.15844433  0.0045591   0.34012157] reward= 1.0 done= False \n",
      "\n",
      "step i 1 action= 0 \n",
      "\n",
      "Next observation= [ 0.00135366 -0.35363084  0.01136153  0.63423866] reward= 1.0 done= False \n",
      "\n",
      "step i 2 action= 0 \n",
      "\n",
      "Next observation= [-0.00571896 -0.5489094   0.02404631  0.93047786] reward= 1.0 done= False \n",
      "\n",
      "step i 3 action= 0 \n",
      "\n",
      "Next observation= [-0.01669715 -0.7443475   0.04265587  1.2306192 ] reward= 1.0 done= False \n",
      "\n",
      "step i 4 action= 0 \n",
      "\n",
      "Next observation= [-0.0315841  -0.9399914   0.06726825  1.5363551 ] reward= 1.0 done= False \n",
      "\n",
      "step i 5 action= 1 \n",
      "\n",
      "Next observation= [-0.05038393 -0.7457407   0.09799535  1.2654    ] reward= 1.0 done= False \n",
      "\n",
      "step i 6 action= 1 \n",
      "\n",
      "Next observation= [-0.06529874 -0.5519981   0.12330335  1.0049448 ] reward= 1.0 done= False \n",
      "\n",
      "step i 7 action= 1 \n",
      "\n",
      "Next observation= [-0.0763387  -0.35871935  0.14340225  0.7533873 ] reward= 1.0 done= False \n",
      "\n",
      "step i 8 action= 1 \n",
      "\n",
      "Next observation= [-0.08351309 -0.16583502  0.15846999  0.5090465 ] reward= 1.0 done= False \n",
      "\n",
      "step i 9 action= 1 \n",
      "\n",
      "Next observation= [-0.08682979  0.02674108  0.16865093  0.27019727] reward= 1.0 done= False \n",
      "\n",
      "step i 10 action= 1 \n",
      "\n",
      "Next observation= [-0.08629496  0.21910532  0.17405488  0.03509344] reward= 1.0 done= False \n",
      "\n",
      "step i 11 action= 1 \n",
      "\n",
      "Next observation= [-0.08191286  0.41135952  0.17475674 -0.19801643] reward= 1.0 done= False \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step i 12 action= 0 \n",
      "\n",
      "Next observation= [-0.07368567  0.21422471  0.17079641  0.14429924] reward= 1.0 done= False \n",
      "\n",
      "step i 13 action= 0 \n",
      "\n",
      "Next observation= [-0.06940117  0.01712074  0.17368239  0.4856237 ] reward= 1.0 done= False \n",
      "\n",
      "step i 14 action= 0 \n",
      "\n",
      "Next observation= [-0.06905876 -0.17997184  0.18339486  0.827621  ] reward= 1.0 done= False \n",
      "\n",
      "step i 15 action= 0 \n",
      "\n",
      "Next observation= [-0.0726582  -0.37706387  0.1999473   1.1719176 ] reward= 1.0 done= False \n",
      "\n",
      "step i 16 action= 1 \n",
      "\n",
      "Next observation= [-0.08019947 -0.18502185  0.22338565  0.9479872 ] reward= 1.0 done= True \n",
      "\n",
      "Iteration No: 46 TimeSteps: 16  Success: 1 Best Q: 1 Learning rate: 0.5 Total reward: 17.0\n",
      "Episode 46 terminated after 16 Iterations \n",
      "step i 0 action= 1 \n",
      "\n",
      "Next observation= [ 0.0267866   0.19656187 -0.02585054 -0.27312678] reward= 1.0 done= False \n",
      "\n",
      "step i 1 action= 1 \n",
      "\n",
      "Next observation= [ 0.03071784  0.39204296 -0.03131307 -0.5738496 ] reward= 1.0 done= False \n",
      "\n",
      "step i 2 action= 1 \n",
      "\n",
      "Next observation= [ 0.0385587   0.5875896  -0.04279006 -0.87623036] reward= 1.0 done= False \n",
      "\n",
      "step i 3 action= 1 \n",
      "\n",
      "Next observation= [ 0.05031049  0.78326625 -0.06031467 -1.1820531 ] reward= 1.0 done= False \n",
      "\n",
      "step i 4 action= 1 \n",
      "\n",
      "Next observation= [ 0.06597582  0.97911686 -0.08395573 -1.4930166 ] reward= 1.0 done= False \n",
      "\n",
      "step i 5 action= 0 \n",
      "\n",
      "Next observation= [ 0.08555815  0.7851108  -0.11381607 -1.2276864 ] reward= 1.0 done= False \n",
      "\n",
      "step i 6 action= 1 \n",
      "\n",
      "Next observation= [ 0.10126036  0.9814986  -0.1383698  -1.553752  ] reward= 1.0 done= False \n",
      "\n",
      "step i 7 action= 1 \n",
      "\n",
      "Next observation= [ 0.12089033  1.1779814  -0.16944483 -1.8862102 ] reward= 1.0 done= False \n",
      "\n",
      "step i 8 action= 1 \n",
      "\n",
      "Next observation= [ 0.14444996  1.3744929  -0.20716904 -2.2263348 ] reward= 1.0 done= False \n",
      "\n",
      "step i 9 action= 1 \n",
      "\n",
      "Next observation= [ 0.17193982  1.570899   -0.25169572 -2.5751173 ] reward= 1.0 done= True \n",
      "\n",
      "Iteration No: 47 TimeSteps: 9  Success: 0 Best Q: 1 Learning rate: 0.5 Total reward: 10.0\n",
      "Episode 47 terminated after 9 Iterations \n",
      "step i 0 action= 0 \n",
      "\n",
      "Next observation= [-0.00144582 -0.18369436  0.00844921  0.33908218] reward= 1.0 done= False \n",
      "\n",
      "step i 1 action= 0 \n",
      "\n",
      "Next observation= [-0.0051197  -0.37893552  0.01523085  0.6344175 ] reward= 1.0 done= False \n",
      "\n",
      "step i 2 action= 0 \n",
      "\n",
      "Next observation= [-0.01269841 -0.57426655  0.0279192   0.93185776] reward= 1.0 done= False \n",
      "\n",
      "step i 3 action= 0 \n",
      "\n",
      "Next observation= [-0.02418374 -0.76975393  0.04655636  1.2331817 ] reward= 1.0 done= False \n",
      "\n",
      "step i 4 action= 0 \n",
      "\n",
      "Next observation= [-0.03957882 -0.96544254  0.07121999  1.5400792 ] reward= 1.0 done= False \n",
      "\n",
      "step i 5 action= 1 \n",
      "\n",
      "Next observation= [-0.05888767 -0.77124584  0.10202157  1.2704436 ] reward= 1.0 done= False \n",
      "\n",
      "step i 6 action= 1 \n",
      "\n",
      "Next observation= [-0.07431259 -0.57756346  0.12743044  1.011373  ] reward= 1.0 done= False \n",
      "\n",
      "step i 7 action= 1 \n",
      "\n",
      "Next observation= [-0.08586386 -0.38435087  0.1476579   0.7612673 ] reward= 1.0 done= False \n",
      "\n",
      "step i 8 action= 1 \n",
      "\n",
      "Next observation= [-0.09355088 -0.1915381   0.16288325  0.5184491 ] reward= 1.0 done= False \n",
      "\n",
      "step i 9 action= 1 \n",
      "\n",
      "Next observation= [-0.09738164  0.00096109  0.17325224  0.2811985 ] reward= 1.0 done= False \n",
      "\n",
      "step i 10 action= 1 \n",
      "\n",
      "Next observation= [-0.09736241  0.19324297  0.1788762   0.04777529] reward= 1.0 done= False \n",
      "\n",
      "step i 11 action= 1 \n",
      "\n",
      "Next observation= [-0.09349756  0.3854092   0.17983171 -0.18356524] reward= 1.0 done= False \n",
      "\n",
      "step i 12 action= 0 \n",
      "\n",
      "Next observation= [-0.08578938  0.18823092  0.17616041  0.16001862] reward= 1.0 done= False \n",
      "\n",
      "step i 13 action= 1 \n",
      "\n",
      "Next observation= [-0.08202475  0.38045081  0.17936078 -0.07232529] reward= 1.0 done= False \n",
      "\n",
      "step i 14 action= 0 \n",
      "\n",
      "Next observation= [-0.07441574  0.18327148  0.17791428  0.27114874] reward= 1.0 done= False \n",
      "\n",
      "step i 15 action= 1 \n",
      "\n",
      "Next observation= [-0.07075031  0.37546763  0.18333724  0.03943658] reward= 1.0 done= False \n",
      "\n",
      "step i 16 action= 0 \n",
      "\n",
      "Next observation= [-0.06324095  0.17825477  0.18412597  0.38389784] reward= 1.0 done= False \n",
      "\n",
      "step i 17 action= 0 \n",
      "\n",
      "Next observation= [-0.05967586 -0.01893855  0.19180393  0.7285157 ] reward= 1.0 done= False \n",
      "\n",
      "step i 18 action= 1 \n",
      "\n",
      "Next observation= [-0.06005463  0.17308773  0.20637424  0.5018036 ] reward= 1.0 done= False \n",
      "\n",
      "step i 19 action= 0 \n",
      "\n",
      "Next observation= [-0.05659288 -0.02425396  0.21641032  0.85177904] reward= 1.0 done= True \n",
      "\n",
      "Iteration No: 48 TimeSteps: 19  Success: 0 Best Q: 1 Learning rate: 0.5 Total reward: 20.0\n",
      "Episode 48 terminated after 19 Iterations \n",
      "step i 0 action= 1 \n",
      "\n",
      "Next observation= [-0.00138066  0.1636304   0.00494259 -0.2574612 ] reward= 1.0 done= False \n",
      "\n",
      "step i 1 action= 1 \n",
      "\n",
      "Next observation= [ 1.8919442e-03  3.5868144e-01 -2.0663315e-04 -5.4858106e-01] reward= 1.0 done= False \n",
      "\n",
      "step i 2 action= 1 \n",
      "\n",
      "Next observation= [ 0.00906557  0.5538063  -0.01117825 -0.8413291 ] reward= 1.0 done= False \n",
      "\n",
      "step i 3 action= 1 \n",
      "\n",
      "Next observation= [ 0.0201417   0.74907905 -0.02800484 -1.1375062 ] reward= 1.0 done= False \n",
      "\n",
      "step i 4 action= 0 \n",
      "\n",
      "Next observation= [ 0.03512328  0.55433434 -0.05075496 -0.8537361 ] reward= 1.0 done= False \n",
      "\n",
      "step i 5 action= 0 \n",
      "\n",
      "Next observation= [ 0.04620996  0.35993958 -0.06782968 -0.57743496] reward= 1.0 done= False \n",
      "\n",
      "step i 6 action= 1 \n",
      "\n",
      "Next observation= [ 0.05340876  0.55594337 -0.07937838 -0.8906913 ] reward= 1.0 done= False \n",
      "\n",
      "step i 7 action= 1 \n",
      "\n",
      "Next observation= [ 0.06452762  0.75204736 -0.09719221 -1.2072338 ] reward= 1.0 done= False \n",
      "\n",
      "step i 8 action= 1 \n",
      "\n",
      "Next observation= [ 0.07956857  0.9482812  -0.12133688 -1.5287249 ] reward= 1.0 done= False \n",
      "\n",
      "step i 9 action= 1 \n",
      "\n",
      "Next observation= [ 0.0985342   1.1446398  -0.15191138 -1.8566829 ] reward= 1.0 done= False \n",
      "\n",
      "step i 10 action= 1 \n",
      "\n",
      "Next observation= [ 0.12142699  1.341069   -0.18904504 -2.1924238 ] reward= 1.0 done= False \n",
      "\n",
      "step i 11 action= 1 \n",
      "\n",
      "Next observation= [ 0.14824837  1.5374492  -0.23289351 -2.536995  ] reward= 1.0 done= True \n",
      "\n",
      "Iteration No: 49 TimeSteps: 11  Success: 0 Best Q: 1 Learning rate: 0.5 Total reward: 12.0\n",
      "Episode 49 terminated after 11 Iterations \n",
      "step i 0 action= 1 \n",
      "\n",
      "Next observation= [-0.00146553  0.2449859  -0.02253609 -0.34382904] reward= 1.0 done= False \n",
      "\n",
      "step i 1 action= 0 \n",
      "\n",
      "Next observation= [ 0.00343418  0.05019168 -0.02941267 -0.05833694] reward= 1.0 done= False \n",
      "\n",
      "step i 2 action= 1 \n",
      "\n",
      "Next observation= [ 0.00443802  0.24572274 -0.03057941 -0.36015275] reward= 1.0 done= False \n",
      "\n",
      "step i 3 action= 0 \n",
      "\n",
      "Next observation= [ 0.00935247  0.05104851 -0.03778246 -0.07726687] reward= 1.0 done= False \n",
      "\n",
      "step i 4 action= 1 \n",
      "\n",
      "Next observation= [ 0.01037344  0.24669115 -0.0393278  -0.3816268 ] reward= 1.0 done= False \n",
      "\n",
      "step i 5 action= 0 \n",
      "\n",
      "Next observation= [ 0.01530727  0.05214906 -0.04696034 -0.10159869] reward= 1.0 done= False \n",
      "\n",
      "step i 6 action= 1 \n",
      "\n",
      "Next observation= [ 0.01635025  0.24791147 -0.04899231 -0.40871984] reward= 1.0 done= False \n",
      "\n",
      "step i 7 action= 0 \n",
      "\n",
      "Next observation= [ 0.02130848  0.05351714 -0.05716671 -0.13187622] reward= 1.0 done= False \n",
      "\n",
      "step i 8 action= 0 \n",
      "\n",
      "Next observation= [ 0.02237882 -0.1407413  -0.05980423  0.14223759] reward= 1.0 done= False \n",
      "\n",
      "step i 9 action= 0 \n",
      "\n",
      "Next observation= [ 0.01956399 -0.33495805 -0.05695948  0.4154699 ] reward= 1.0 done= False \n",
      "\n",
      "step i 10 action= 1 \n",
      "\n",
      "Next observation= [ 0.01286483 -0.13907701 -0.04865009  0.10538784] reward= 1.0 done= False \n",
      "\n",
      "step i 11 action= 1 \n",
      "\n",
      "Next observation= [ 0.01008329  0.05670715 -0.04654233 -0.20223841] reward= 1.0 done= False \n",
      "\n",
      "step i 12 action= 1 \n",
      "\n",
      "Next observation= [ 0.01121743  0.25246277 -0.0505871  -0.50923234] reward= 1.0 done= False \n",
      "\n",
      "step i 13 action= 0 \n",
      "\n",
      "Next observation= [ 0.01626669  0.05808864 -0.06077174 -0.23291042] reward= 1.0 done= False \n",
      "\n",
      "step i 14 action= 0 \n",
      "\n",
      "Next observation= [ 0.01742846 -0.1361147  -0.06542995  0.04000093] reward= 1.0 done= False \n",
      "\n",
      "step i 15 action= 0 \n",
      "\n",
      "Next observation= [ 0.01470617 -0.33024034 -0.06462993  0.31134364] reward= 1.0 done= False \n",
      "\n",
      "step i 16 action= 1 \n",
      "\n",
      "Next observation= [ 0.00810136 -0.13426    -0.05840306 -0.0010011 ] reward= 1.0 done= False \n",
      "\n",
      "step i 17 action= 0 \n",
      "\n",
      "Next observation= [ 0.00541616 -0.32849786 -0.05842308  0.27269816] reward= 1.0 done= False \n",
      "\n",
      "step i 18 action= 1 \n",
      "\n",
      "Next observation= [-0.00115379 -0.13259304 -0.05296912 -0.03782429] reward= 1.0 done= False \n",
      "\n",
      "step i 19 action= 1 \n",
      "\n",
      "Next observation= [-0.00380566  0.06324691 -0.0537256  -0.34673783] reward= 1.0 done= False \n",
      "\n",
      "step i 20 action= 0 \n",
      "\n",
      "Next observation= [-0.00254072 -0.13107133 -0.06066036 -0.07146879] reward= 1.0 done= False \n",
      "\n",
      "step i 21 action= 0 \n",
      "\n",
      "Next observation= [-0.00516214 -0.3252735  -0.06208974  0.20147552] reward= 1.0 done= False \n",
      "\n",
      "step i 22 action= 1 \n",
      "\n",
      "Next observation= [-0.01166761 -0.12932107 -0.05806023 -0.11012945] reward= 1.0 done= False \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step i 23 action= 0 \n",
      "\n",
      "Next observation= [-0.01425404 -0.32356504 -0.06026281  0.16368544] reward= 1.0 done= False \n",
      "\n",
      "step i 24 action= 1 \n",
      "\n",
      "Next observation= [-0.02072534 -0.12763451 -0.05698911 -0.14738342] reward= 1.0 done= False \n",
      "\n",
      "step i 25 action= 0 \n",
      "\n",
      "Next observation= [-0.02327803 -0.32189602 -0.05993678  0.12679006] reward= 1.0 done= False \n",
      "\n",
      "step i 26 action= 1 \n",
      "\n",
      "Next observation= [-0.02971595 -0.12596892 -0.05740098 -0.18418372] reward= 1.0 done= False \n",
      "\n",
      "step i 27 action= 0 \n",
      "\n",
      "Next observation= [-0.03223532 -0.32022458 -0.06108465  0.08985325] reward= 1.0 done= False \n",
      "\n",
      "step i 28 action= 1 \n",
      "\n",
      "Next observation= [-0.03863982 -0.12428267 -0.05928759 -0.22145917] reward= 1.0 done= False \n",
      "\n",
      "step i 29 action= 0 \n",
      "\n",
      "Next observation= [-0.04112547 -0.31850928 -0.06371677  0.05194854] reward= 1.0 done= False \n",
      "\n",
      "step i 30 action= 1 \n",
      "\n",
      "Next observation= [-0.04749566 -0.12253431 -0.06267779 -0.26013747] reward= 1.0 done= False \n",
      "\n",
      "step i 31 action= 0 \n",
      "\n",
      "Next observation= [-0.04994634 -0.31670812 -0.06788055  0.01213612] reward= 1.0 done= False \n",
      "\n",
      "step i 32 action= 1 \n",
      "\n",
      "Next observation= [-0.0562805  -0.12068165 -0.06763782 -0.30116796] reward= 1.0 done= False \n",
      "\n",
      "step i 33 action= 0 \n",
      "\n",
      "Next observation= [-0.05869414 -0.31477764 -0.07366119 -0.03056007] reward= 1.0 done= False \n",
      "\n",
      "step i 34 action= 0 \n",
      "\n",
      "Next observation= [-0.06498969 -0.50877017 -0.07427239  0.23800285] reward= 1.0 done= False \n",
      "\n",
      "step i 35 action= 1 \n",
      "\n",
      "Next observation= [-0.07516509 -0.31267008 -0.06951233 -0.07715236] reward= 1.0 done= False \n",
      "\n",
      "step i 36 action= 0 \n",
      "\n",
      "Next observation= [-0.08141849 -0.50673026 -0.07105538  0.19281471] reward= 1.0 done= False \n",
      "\n",
      "step i 37 action= 1 \n",
      "\n",
      "Next observation= [-0.0915531  -0.31066754 -0.06719908 -0.12140991] reward= 1.0 done= False \n",
      "\n",
      "step i 38 action= 1 \n",
      "\n",
      "Next observation= [-0.09776645 -0.11465041 -0.06962728 -0.4345137 ] reward= 1.0 done= False \n",
      "\n",
      "step i 39 action= 0 \n",
      "\n",
      "Next observation= [-0.10005946 -0.3087211  -0.07831755 -0.16456686] reward= 1.0 done= False \n",
      "\n",
      "step i 40 action= 1 \n",
      "\n",
      "Next observation= [-0.10623388 -0.11257047 -0.08160889 -0.48089275] reward= 1.0 done= False \n",
      "\n",
      "step i 41 action= 0 \n",
      "\n",
      "Next observation= [-0.10848529 -0.30645135 -0.09122674 -0.21500574] reward= 1.0 done= False \n",
      "\n",
      "step i 42 action= 1 \n",
      "\n",
      "Next observation= [-0.11461432 -0.11015163 -0.09552686 -0.5350144 ] reward= 1.0 done= False \n",
      "\n",
      "step i 43 action= 0 \n",
      "\n",
      "Next observation= [-0.11681735 -0.3038096  -0.10622714 -0.27389398] reward= 1.0 done= False \n",
      "\n",
      "step i 44 action= 1 \n",
      "\n",
      "Next observation= [-0.12289354 -0.10734502 -0.11170503 -0.5981018 ] reward= 1.0 done= False \n",
      "\n",
      "step i 45 action= 0 \n",
      "\n",
      "Next observation= [-0.12504044 -0.3007413  -0.12366706 -0.34258842] reward= 1.0 done= False \n",
      "\n",
      "step i 46 action= 0 \n",
      "\n",
      "Next observation= [-0.13105527 -0.4939068  -0.13051882 -0.09131854] reward= 1.0 done= False \n",
      "\n",
      "step i 47 action= 1 \n",
      "\n",
      "Next observation= [-0.14093341 -0.29717907 -0.1323452  -0.42216387] reward= 1.0 done= False \n",
      "\n",
      "step i 48 action= 0 \n",
      "\n",
      "Next observation= [-0.14687699 -0.4902021  -0.14078848 -0.1739573 ] reward= 1.0 done= False \n",
      "\n",
      "step i 49 action= 1 \n",
      "\n",
      "Next observation= [-0.15668103 -0.29337534 -0.14426762 -0.50753146] reward= 1.0 done= False \n",
      "\n",
      "step i 50 action= 0 \n",
      "\n",
      "Next observation= [-0.16254854 -0.48620138 -0.15441826 -0.26356485] reward= 1.0 done= False \n",
      "\n",
      "step i 51 action= 1 \n",
      "\n",
      "Next observation= [-0.17227256 -0.2892512  -0.15968955 -0.60069364] reward= 1.0 done= False \n",
      "\n",
      "step i 52 action= 1 \n",
      "\n",
      "Next observation= [-0.17805758 -0.09229817 -0.17170343 -0.9391138 ] reward= 1.0 done= False \n",
      "\n",
      "step i 53 action= 1 \n",
      "\n",
      "Next observation= [-0.17990355  0.10467038 -0.1904857  -1.2804552 ] reward= 1.0 done= False \n",
      "\n",
      "step i 54 action= 1 \n",
      "\n",
      "Next observation= [-0.17781015  0.30163905 -0.2160948  -1.6262289 ] reward= 1.0 done= True \n",
      "\n",
      "Iteration No: 50 TimeSteps: 54  Success: 0 Best Q: 1 Learning rate: 0.5 Total reward: 55.0\n",
      "Episode 50 terminated after 54 Iterations \n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "actions=[]\n",
    "states=[]\n",
    "rewards=[]\n",
    "\n",
    "#constants chosen by us\n",
    "alpha = 0.5\n",
    "time_steps = 100\n",
    "success_to_end = 10\n",
    "num_success = 0\n",
    "success_time=20\n",
    "gamma=0.9\n",
    "\n",
    "  \n",
    "for i in range(50):\n",
    "  obser= env.reset()\n",
    "\n",
    "  total_reward = 0\n",
    "  state_0 = tuple(assignBins(obser,bins))\n",
    "  for t1 in range(time_steps):\n",
    "    env.render()\n",
    "    action1 = np.random.choice(actionSpace, p=b[state_0])\n",
    "\n",
    "    obser, rew, done1, info1 = env.step(action1)\n",
    "    print(\"step i\",t1,\"action=\",action1,\"\\n\")\n",
    "    print(\"Next observation=\",obser,\"reward=\",rew,\"done=\",done1,\"\\n\")\n",
    "\n",
    "    state = tuple(assignBins(obser,bins)) \n",
    "    best_q = max(q[state])# update the Q table \n",
    "    q[state_0][action1] = q[state_0][action1]  + alpha * (rew + gamma * (best_q) - q[state_0][action1]) \n",
    "    state_0 = state\n",
    "    total_reward += rew\n",
    "    \n",
    "    if done1:\n",
    "      print(\"Iteration No:\", i+1,\"TimeSteps:\",t1,\" Success:\",num_success, \"Best Q:\",best_q,\"Learning rate:\",alpha,\"Total reward:\", total_reward)\n",
    "      if t1>=success_time:\n",
    "        num_success+=1\n",
    "      else:\n",
    "        num_success=0\n",
    "      break\n",
    "    \n",
    "  # break when it's solved over num_success=10 times consecutively\n",
    "    if num_success > success_to_end:\n",
    "        break\n",
    "  print(\"Episode\",i+1 ,\"terminated after\",t1,\"Iterations \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sb6tJjVzjUGL"
   },
   "source": [
    "## c. Off-policy Expected SARSA with an epsilon-greedy policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "Eb-usYGkje6r",
    "outputId": "c60e9636-dfb4-499b-eae7-5c83036e5eaf"
   },
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m       greedy_actions \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     29\u001b[0m p1 \u001b[38;5;241m=\u001b[39m epsilon \u001b[38;5;241m/\u001b[39m actions \u001b[38;5;66;03m#non-greedy action prob\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m p2 \u001b[38;5;241m=\u001b[39m (\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mgreedy_actions\u001b[49m) \u001b[38;5;241m+\u001b[39m p1 \u001b[38;5;66;03m#greedy_action_probability\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(actions):\n\u001b[0;32m     32\u001b[0m       \u001b[38;5;28;01mif\u001b[39;00m q[new_state][k] \u001b[38;5;241m==\u001b[39m qmax:\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "epsilon = 0.1 # Exploration rate\n",
    "alpha = 0.5 # Step size \n",
    "gamma = 0.9\n",
    "max_steps=200\n",
    "epreward=0\n",
    "totreward=[]\n",
    "e=0.1\n",
    "actions=2\n",
    "\n",
    "num_episodes=10\n",
    "for i in range(num_episodes):\n",
    "  prev_state = env.reset()\n",
    "  prev_state=tuple(assignBins(prev_state,bins))\n",
    "  prev_action = np.random.choice(actionSpace, p=b[state_0])\n",
    "  epreward = 0\n",
    "\n",
    "  for t2 in range(max_steps):\n",
    "    new_state, reward1, done2, _ = env.step(prev_action)\n",
    "    new_state=tuple(assignBins(new_state, bins))\n",
    "    new_action = np.random.choice(actionSpace, p=b[new_state])\n",
    "    oldQ=q[prev_state][prev_action]\n",
    "    expected_q=0\n",
    "    qmax = max(q[prev_state])\n",
    "    greedy_actions = 0\n",
    "    for j in range(actions):\n",
    "        if q[new_state][j] == qmax:\n",
    "          greedy_actions += 1\n",
    "    \n",
    "    p1 = epsilon / actions #non-greedy action prob\n",
    "    p2 = ((1 - epsilon) / greedy_actions) + p1 #greedy_action_probability\n",
    "    for k in range(actions):\n",
    "          if q[new_state][k] == qmax:\n",
    "              expected_q += q[new_state][k] * p2\n",
    "          else:\n",
    "              expected_q += q[new_state][k] * p1\n",
    "    q[prev_state, prev_action] += alpha * (reward1 + gamma * expected_q - oldQ)\n",
    "    prev_state=new_state\n",
    "    prev_action=new_action\n",
    "    epreward+=reward1\n",
    "    print(\"Episode no.:\",i,\"Timestep:\",t2)\n",
    "\n",
    "    if done2:\n",
    "      break\n",
    "totreward.append(epreward)    \n",
    "print(\"Mean Expected return\",np.mean(totreward))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "RL exercise 3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
