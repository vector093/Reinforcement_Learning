{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A1_DynProg final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Hruday Vishal Kanna Anand**\n",
        "\n",
        "**1006874517**"
      ],
      "metadata": {
        "id": "0KvELfya9KSx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhOUhNHtn6PO"
      },
      "source": [
        "1. What is the purpose of OpenAI gyms and how is it going to help us in our RL education?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8w0HGkFwoVHb"
      },
      "source": [
        "OpenAI Gyms gives programmers an environment to run and visualize there algorithms on multiple types of models, there by allowing them to come up with more efficient agents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsGqubiDkQnd"
      },
      "source": [
        "import gym\n",
        "import random \n",
        "import numpy as np\n",
        "import copy"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iS40R9okStg"
      },
      "source": [
        "gym.envs.register(\n",
        "    id='FrozenLakeNotSlippery-v0',\n",
        "    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
        "    kwargs={'map_name' : '4x4', 'is_slippery': False},\n",
        "    max_episode_steps=100,\n",
        "    reward_threshold=0.74\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVX1AjRWkueO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f71a03b1-9d34-4236-d433-d21005c2fb67"
      },
      "source": [
        "# Create the gridworld-like environment\n",
        "env=gym.make('FrozenLakeNotSlippery-v0')\n",
        "# Let's look at the model of the environment (i.e., P):\n",
        "env.env.P\n",
        "# Question: what is the data in this structure saying? Relate this to the course\n",
        "# presentation of P"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:330: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: {0: [(1.0, 0, 0.0, False)],\n",
              "  1: [(1.0, 4, 0.0, False)],\n",
              "  2: [(1.0, 1, 0.0, False)],\n",
              "  3: [(1.0, 0, 0.0, False)]},\n",
              " 1: {0: [(1.0, 0, 0.0, False)],\n",
              "  1: [(1.0, 5, 0.0, True)],\n",
              "  2: [(1.0, 2, 0.0, False)],\n",
              "  3: [(1.0, 1, 0.0, False)]},\n",
              " 2: {0: [(1.0, 1, 0.0, False)],\n",
              "  1: [(1.0, 6, 0.0, False)],\n",
              "  2: [(1.0, 3, 0.0, False)],\n",
              "  3: [(1.0, 2, 0.0, False)]},\n",
              " 3: {0: [(1.0, 2, 0.0, False)],\n",
              "  1: [(1.0, 7, 0.0, True)],\n",
              "  2: [(1.0, 3, 0.0, False)],\n",
              "  3: [(1.0, 3, 0.0, False)]},\n",
              " 4: {0: [(1.0, 4, 0.0, False)],\n",
              "  1: [(1.0, 8, 0.0, False)],\n",
              "  2: [(1.0, 5, 0.0, True)],\n",
              "  3: [(1.0, 0, 0.0, False)]},\n",
              " 5: {0: [(1.0, 5, 0, True)],\n",
              "  1: [(1.0, 5, 0, True)],\n",
              "  2: [(1.0, 5, 0, True)],\n",
              "  3: [(1.0, 5, 0, True)]},\n",
              " 6: {0: [(1.0, 5, 0.0, True)],\n",
              "  1: [(1.0, 10, 0.0, False)],\n",
              "  2: [(1.0, 7, 0.0, True)],\n",
              "  3: [(1.0, 2, 0.0, False)]},\n",
              " 7: {0: [(1.0, 7, 0, True)],\n",
              "  1: [(1.0, 7, 0, True)],\n",
              "  2: [(1.0, 7, 0, True)],\n",
              "  3: [(1.0, 7, 0, True)]},\n",
              " 8: {0: [(1.0, 8, 0.0, False)],\n",
              "  1: [(1.0, 12, 0.0, True)],\n",
              "  2: [(1.0, 9, 0.0, False)],\n",
              "  3: [(1.0, 4, 0.0, False)]},\n",
              " 9: {0: [(1.0, 8, 0.0, False)],\n",
              "  1: [(1.0, 13, 0.0, False)],\n",
              "  2: [(1.0, 10, 0.0, False)],\n",
              "  3: [(1.0, 5, 0.0, True)]},\n",
              " 10: {0: [(1.0, 9, 0.0, False)],\n",
              "  1: [(1.0, 14, 0.0, False)],\n",
              "  2: [(1.0, 11, 0.0, True)],\n",
              "  3: [(1.0, 6, 0.0, False)]},\n",
              " 11: {0: [(1.0, 11, 0, True)],\n",
              "  1: [(1.0, 11, 0, True)],\n",
              "  2: [(1.0, 11, 0, True)],\n",
              "  3: [(1.0, 11, 0, True)]},\n",
              " 12: {0: [(1.0, 12, 0, True)],\n",
              "  1: [(1.0, 12, 0, True)],\n",
              "  2: [(1.0, 12, 0, True)],\n",
              "  3: [(1.0, 12, 0, True)]},\n",
              " 13: {0: [(1.0, 12, 0.0, True)],\n",
              "  1: [(1.0, 13, 0.0, False)],\n",
              "  2: [(1.0, 14, 0.0, False)],\n",
              "  3: [(1.0, 9, 0.0, False)]},\n",
              " 14: {0: [(1.0, 13, 0.0, False)],\n",
              "  1: [(1.0, 14, 0.0, False)],\n",
              "  2: [(1.0, 15, 1.0, True)],\n",
              "  3: [(1.0, 10, 0.0, False)]},\n",
              " 15: {0: [(1.0, 15, 0, True)],\n",
              "  1: [(1.0, 15, 0, True)],\n",
              "  2: [(1.0, 15, 0, True)],\n",
              "  3: [(1.0, 15, 0, True)]}}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ouq6OFc9V2kP"
      },
      "source": [
        "The elements of the data structure above all have a unique meaning-\n",
        "\n",
        "1.   The first element of the data structure ranging from 0-15 is the key of the dictionary that tells use the current position.(state space)\n",
        "2.   The second element of the data structure which is the value of the key which is another dictionary containing all the possible actions that can be taken from that position as the key. Such as 0,1,2,3 (action space)\n",
        "3.   The third element is the value of the internal dictionary which gives all the information of the action in the form of an array.\n",
        "  *   first element- probability of getting the mentioned reward and moving to the mentioned state p(S',R(t+1)|S,a)\n",
        "  *   second element- the state it will go to after taking that action- S'\n",
        "  *   third element- reward for taking that action- R(t+1)\n",
        "  *   fourth element- If the episode is over after taking that action\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gyn_w3ulkyZI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e60feb75-ecdc-42ea-e61b-ec9b3dc04ec4"
      },
      "source": [
        "# Now let's investigate the observation space (i.e., S using our nomenclature),\n",
        "# and confirm we see it is a discrete space with 16 locations\n",
        "print(env.observation_space)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Discrete(16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zND5ArI8k_qQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db3e846b-0aec-4b90-8617-74b309797513"
      },
      "source": [
        "stateSpaceSize = env.observation_space.n\n",
        "print(stateSpaceSize)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_tp9YzRljnj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb0d9d1e-fe00-4f5b-aa95-f9f1224d75f9"
      },
      "source": [
        "# Now let's investigate the action space (i.e., A) for the agent->environment\n",
        "# channel\n",
        "print(env.action_space)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Discrete(4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFGNZNowluz2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc8856cf-0745-42bd-c089-ec4d16599dca"
      },
      "source": [
        "# The gym environment has ...sample() functions that allow us to sample\n",
        "# from the above spaces:\n",
        "for g in range(1,10,1):\n",
        "  print(\"sample from S:\",env.observation_space.sample(),\" ... \",\"sample from A:\",env.action_space.sample())\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample from S: 4  ...  sample from A: 0\n",
            "sample from S: 0  ...  sample from A: 2\n",
            "sample from S: 8  ...  sample from A: 2\n",
            "sample from S: 11  ...  sample from A: 3\n",
            "sample from S: 11  ...  sample from A: 3\n",
            "sample from S: 6  ...  sample from A: 1\n",
            "sample from S: 3  ...  sample from A: 2\n",
            "sample from S: 3  ...  sample from A: 2\n",
            "sample from S: 6  ...  sample from A: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOQL5JxsmcEd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        },
        "outputId": "e7d914c9-3ad9-453c-ec7d-c1d40bcb7a6d"
      },
      "source": [
        "# The enviroment also provides a helper to render (visualize) the environment\n",
        "env.reset()\n",
        "env.render()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:58: DeprecationWarning: \u001b[33mWARN: You are calling render method, but you didn't specified the argument render_mode at environment initialization. To maintain backward compatibility, the environment will render in human mode.\n",
            "If you want to render in human mode, initialize the environment in this way: gym.make('EnvName', render_mode='human') and don't call the render method.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"You are calling render method, \"\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "DependencyNotInstalled",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/envs/toy_text/frozen_lake.py\u001b[0m in \u001b[0;36m_render_gui\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m             \u001b[0;32mimport\u001b[0m \u001b[0mpygame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pygame'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mDependencyNotInstalled\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-210f2ff72547>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# The enviroment also provides a helper to render (visualize) the environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m                 )\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mrender_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    431\u001b[0m     ) -> Optional[Union[RenderFrame, List[RenderFrame]]]:\n\u001b[1;32m    432\u001b[0m         \u001b[0;34m\"\"\"Renders the environment.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m                 )\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mrender_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/wrappers/order_enforcing.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0;34m\"set `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             )\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m                 )\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mrender_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    431\u001b[0m     ) -> Optional[Union[RenderFrame, List[RenderFrame]]]:\n\u001b[1;32m    432\u001b[0m         \u001b[0;34m\"\"\"Renders the environment.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m                 )\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mrender_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/wrappers/env_checker.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchecked_render\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchecked_render\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0menv_render_passive_checker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py\u001b[0m in \u001b[0;36menv_render_passive_checker\u001b[0;34m(env, *args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m             )\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;31m# TODO: Check that the result is correct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m                 )\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mrender_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/envs/toy_text/frozen_lake.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_renders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"human\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/envs/toy_text/frozen_lake.py\u001b[0m in \u001b[0;36m_render\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"human\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rgb_array\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"single_rgb_array\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_gui\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_render_gui\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/envs/toy_text/frozen_lake.py\u001b[0m in \u001b[0;36m_render_gui\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m             raise DependencyNotInstalled(\n\u001b[0;32m--> 292\u001b[0;31m                 \u001b[0;34m\"pygame is not installed, run `pip install gym[toy_text]`\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m             )\n\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mDependencyNotInstalled\u001b[0m: pygame is not installed, run `pip install gym[toy_text]`"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLV6e43mmwx1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6440f8d5-45e5-415c-bd76-bbaa6fa63ad3"
      },
      "source": [
        "# We can act as the agent, by selecting actions and stepping the environment\n",
        "# through time to see its responses to our actions\n",
        "env.reset()\n",
        "exitCommand=False\n",
        "env.render()\n",
        "while not(exitCommand):\n",
        "  print(\"Enter the action as an integer from 0 to\",env.action_space.n-1,\" (or exit): \")\n",
        "  userInput=input()\n",
        "  if userInput==\"exit\":\n",
        "    break\n",
        "  action=int(userInput)\n",
        "  (observation, reward, compute, probability) = env.step(action)\n",
        "  print(\"--> The result of taking action\",action,\"is:\")\n",
        "  print(\"     S=\",observation)\n",
        "  print(\"     R=\",reward)\n",
        "  print(\"     p=\",probability)\n",
        "\n",
        "  env.render()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Enter the action as an integer from 0 to 3  (or exit): \n",
            "0\n",
            "--> The result of taking action 0 is:\n",
            "     S= 0\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Enter the action as an integer from 0 to 3  (or exit): \n",
            "1\n",
            "--> The result of taking action 1 is:\n",
            "     S= 4\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "  (Down)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Enter the action as an integer from 0 to 3  (or exit): \n",
            "3\n",
            "--> The result of taking action 3 is:\n",
            "     S= 0\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "  (Up)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Enter the action as an integer from 0 to 3  (or exit): \n",
            "2\n",
            "--> The result of taking action 2 is:\n",
            "     S= 1\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "  (Right)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Enter the action as an integer from 0 to 3  (or exit): \n",
            "exit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tBpeiuRnyih"
      },
      "source": [
        "# Question: draw a table indicating the correspondence between the action\n",
        "# you input (a number) and the logic action performed.\n",
        "# Question: draw a table that illustrates what the symbols on the render image\n",
        "# mean?\n",
        "# Question: Explain what the objective of the agent is in this environment?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjvIthZwUCPh"
      },
      "source": [
        "**Action Table-**\n",
        "\n",
        "0 - left\n",
        "\n",
        "1 - down\n",
        "\n",
        "2 - right\n",
        "\n",
        "3 - up\n",
        "\n",
        "**Symbols on the render-**\n",
        "\n",
        "S - starting point of the agent\n",
        "\n",
        "G - the goal destination of the agent\n",
        "\n",
        "F - positions where the agent is free to move\n",
        "\n",
        "H - positions where the agent gets stuck\n",
        "\n",
        "**Objective of the agent -** the agent should move from the starting point to the goal witout getting stuck."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWI3h6s7qqdq"
      },
      "source": [
        "# Practical: Code up an AI that will employ random action selection in order\n",
        "# to drive the agent. Test this random action selection agent with the\n",
        "# above environment (i.e., code up a loop as I did above, but instead\n",
        "# of taking input from a human user, take it from the AI you coded)."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmRwGwPoqw0F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e040fc5-f2ed-44c4-d345-9f539f7baeff"
      },
      "source": [
        "env.reset()\n",
        "exitCommand=False\n",
        "env.render()\n",
        "while not(exitCommand):\n",
        "  userInput=env.action_space.sample()\n",
        "  print(\"The action choosen is - \",userInput)\n",
        "  action=int(userInput)\n",
        "  (observation, reward, compute, probability) = env.step(action)\n",
        "  print(\"--> The result of taking action\",action,\"is:\")\n",
        "  print(\"     S=\",observation)\n",
        "  print(\"     R=\",reward)\n",
        "  print(\"     p=\",probability)\n",
        "\n",
        "  env.render()\n",
        "  #checks if the episode has ended or not\n",
        "  if compute==True:\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "The action choosen is -  1\n",
            "--> The result of taking action 1 is:\n",
            "     S= 4\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "  (Down)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "The action choosen is -  1\n",
            "--> The result of taking action 1 is:\n",
            "     S= 8\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "The action choosen is -  1\n",
            "--> The result of taking action 1 is:\n",
            "     S= 12\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "\u001b[41mH\u001b[0mFFG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQLKuubzrl4L"
      },
      "source": [
        "# Now towards dynamic programming. Note that env.env.P has the model\n",
        "# of the environment.\n",
        "#\n",
        "# Question: How would you represent the agent's policy function and value function?\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XekyPXOObJAx"
      },
      "source": [
        "The agents policy function is represented as an array(list) of 4 probabilities (for each action) for each possible state the agent can be in. So the policy is also a dictionary with the states as the key and the value will be an array(list) of 4 elements that tell us the probability of each action.\n",
        "\n",
        "The value function in an array(list) of 16 elements whose index signifies the state and the value is the value of the state."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HntLq1J9CVGw"
      },
      "source": [
        "# Practical: revise the above AI solver to use a policy function in which you\n",
        "# code the random action selections in the policy function. Test this.\n",
        "# Practical: Code the C-4 Policy Evaluation (Prediction) algorithm. You may use\n",
        "# either the inplace or ping-pong buffer (as described in the lecture). Now\n",
        "# randomly initialize your policy function, and compute its value function.\n",
        "# Report your results: policy and value function. Ensure your prediction\n",
        "# algo reports how many iterations it took.\n",
        "#\n",
        "# (Optional): Repeat the above for q.\n",
        "#\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lppyx5Nf4Jqf",
        "outputId": "2e8f210e-8a92-4060-e0cf-421a74fec925"
      },
      "source": [
        "#initilize a randomized policy that is randomly weighted for all actions\n",
        "pi={}\n",
        "for i in range (0,16):\n",
        "  pi[i]=np.random.random(4)\n",
        "\n",
        "#we run the ai with the policy we initilized above\n",
        "observation=env.reset()\n",
        "exitCommand=False\n",
        "env.render()\n",
        "while not(exitCommand):\n",
        "  #randomly choose an action based on the weights\n",
        "  userInput=random.choices((0,1,2,3),weights=pi[observation],k=1)\n",
        "  print(\"The action choosen is - \",userInput)\n",
        "  action=int(userInput[0])\n",
        "  (observation, reward, compute, probability) = env.step(action)\n",
        "  print(\"--> The result of taking action\",action,\"is:\")\n",
        "  print(\"     S=\",observation)\n",
        "  print(\"     R=\",reward)\n",
        "  print(\"     p=\",probability)\n",
        "\n",
        "  env.render()\n",
        "  #checks if the episode has ended or not\n",
        "  if compute==True:\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "The action choosen is -  [3]\n",
            "--> The result of taking action 3 is:\n",
            "     S= 0\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "  (Up)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "The action choosen is -  [1]\n",
            "--> The result of taking action 1 is:\n",
            "     S= 4\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "  (Down)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "The action choosen is -  [3]\n",
            "--> The result of taking action 3 is:\n",
            "     S= 0\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "  (Up)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "The action choosen is -  [1]\n",
            "--> The result of taking action 1 is:\n",
            "     S= 4\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "  (Down)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "The action choosen is -  [2]\n",
            "--> The result of taking action 2 is:\n",
            "     S= 5\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "  (Right)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_PbIiCl9DJBR",
        "outputId": "8b12f104-5dae-4cef-cb52-1a3d6fe7c42d"
      },
      "source": [
        "#policy evaluation\n",
        "\n",
        "#initilize parameters to perform policy evaluation\n",
        "delta=1\n",
        "gamma=0.9\n",
        "iter=0\n",
        "v=np.zeros(16)\n",
        "\n",
        "#get the model of the environment\n",
        "lake=env.env.P\n",
        "\n",
        "#initialize the value function arbitarily\n",
        "for i in lake.keys():\n",
        "  v[i]=random.random()\n",
        "v[15]=0\n",
        "\n",
        "while delta>0.00001:\n",
        "  delta=0\n",
        "  for i in lake.keys():\n",
        "    t=v[i]\n",
        "    sum=0\n",
        "    for a in [0,1,2,3]:\n",
        "      sum=sum+ (pi[i][a]/np.sum(pi[i]))*lake[i][a][0][0]*(lake[i][a][0][2]+ gamma*v[lake[i][a][0][1]])\n",
        "    v[i]=sum\n",
        "    delta=max(delta,abs(t-v[i]))\n",
        "  iter=iter+1\n",
        "print('policy is -')\n",
        "dictionary_items = pi.items()\n",
        "for item in dictionary_items:\n",
        "  print(item)\n",
        "print('the value function is -')\n",
        "print(v)\n",
        "print('number of iterations -',iter)\n",
        "\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "policy is -\n",
            "(0, array([0.07519211, 0.79844905, 0.33665976, 0.44116947]))\n",
            "(1, array([0.72115789, 0.73605523, 0.12270313, 0.67698254]))\n",
            "(2, array([0.50514257, 0.05779971, 0.34394013, 0.59053567]))\n",
            "(3, array([0.85941686, 0.95496945, 0.51772382, 0.15734225]))\n",
            "(4, array([0.37844992, 0.94274417, 0.21595725, 0.92456433]))\n",
            "(5, array([0.38161744, 0.45021688, 0.71424303, 0.56828732]))\n",
            "(6, array([0.60955609, 0.2172288 , 0.41982878, 0.29712073]))\n",
            "(7, array([0.32993819, 0.6148518 , 0.59276764, 0.46789157]))\n",
            "(8, array([0.00846003, 0.81073851, 0.73765472, 0.52890883]))\n",
            "(9, array([0.4164536 , 0.51971577, 0.12043761, 0.62499805]))\n",
            "(10, array([0.26530068, 0.99472237, 0.04539243, 0.3792612 ]))\n",
            "(11, array([0.91105345, 0.29968833, 0.13129563, 0.50097267]))\n",
            "(12, array([0.32685073, 0.89981949, 0.77884258, 0.90280541]))\n",
            "(13, array([0.21777336, 0.09859577, 0.10249531, 0.53558594]))\n",
            "(14, array([0.9178621 , 0.03251223, 0.94600904, 0.29941887]))\n",
            "(15, array([0.68070841, 0.91028395, 0.77903824, 0.31829969]))\n",
            "the value function is -\n",
            "[5.89501958e-03 2.59407467e-03 3.66804128e-03 1.55249989e-03\n",
            " 8.62980670e-03 5.27269308e-05 3.60895538e-02 8.37423306e-05\n",
            " 1.57704031e-02 4.30821158e-02 2.79618340e-01 5.16284627e-05\n",
            " 7.40368466e-05 7.73817924e-02 5.00928868e-01 0.00000000e+00]\n",
            "number of iterations - 87\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcqWU24V4Itt"
      },
      "source": [
        "# Policy Improvement:\n",
        "# Question: How would you use P and your value function to improve an arbitrary\n",
        "# policy, pi, per Chapter 4?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPHGyrXriEty"
      },
      "source": [
        "To improve an arbitary policy pi we apply policy improvment- we set the new policy by selecting the most greedy actions for every state.By greedy actions we imply actions that will take the agent to a state of greater value. this can be determined by the models probability p(S',R(t+1)|S,a) , Reward and the Value function. The probability and Reward can be obtain in the environment model P. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6eLc7t5iE_-"
      },
      "source": [
        "# Practical: Code the policy iteration process, and employ it to arrive at a\n",
        "# policy that solves this problem. Show your testing results, and ensure\n",
        "# it reports the number of iterations for each step: (a) overall policy\n",
        "# iteration steps and (b) evaluation steps.\n",
        "# Practical: Code the value iteration process, and employ it to arrive at a\n",
        "# policy that solves this problem. Show your testing results, reporting\n",
        "# the iteration counts.\n",
        "# Comment on the difference between the iterations required for policy vs\n",
        "# value iteration.\n",
        "#\n",
        "# Optional: instead of the above environment, use the \"slippery\" Frozen Lake via\n",
        "# env = gym.make(\"FrozenLake-v0\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gkI5kwgBmJyF",
        "outputId": "dac4ebc5-2f4a-46e5-bda8-26fe9c967553"
      },
      "source": [
        "#policy iteration\n",
        "\n",
        "#initilize parameters to perform policy evaluation\n",
        "delta=1\n",
        "gamma=0.9\n",
        "iter=0\n",
        "iter2=0\n",
        "pstable=False\n",
        "v=np.zeros(16)\n",
        "\n",
        "#get the model of the environment\n",
        "lake=env.env.P\n",
        "\n",
        "#initialize the value function arbitarily\n",
        "for i in lake.keys():\n",
        "  v[i]=random.random()\n",
        "v[15]=0\n",
        "\n",
        "while (pstable==False):\n",
        "  while delta>0.00001:\n",
        "    delta=0\n",
        "    for i in lake.keys():\n",
        "      t=v[i]\n",
        "      sum=0\n",
        "      for a in [0,1,2,3]:\n",
        "        sum=sum+ (pi[i][a]/np.sum(pi[i]))*lake[i][a][0][0]*(lake[i][a][0][2]+ gamma*v[lake[i][a][0][1]])\n",
        "      v[i]=sum\n",
        "      delta=max(delta,abs(t-v[i]))\n",
        "    iter=iter+1\n",
        "  print('number of iterations for policy eval -',iter)\n",
        "\n",
        "  pstable=True\n",
        "  for i in lake.keys():\n",
        "    t=copy.deepcopy(pi[i])\n",
        "    act=np.zeros(4)\n",
        "    for a in [0,1,2,3]:\n",
        "      act[a]=lake[i][a][0][0]*(lake[i][a][0][2]+ gamma*v[lake[i][a][0][1]])\n",
        "    actf=np.argwhere(act == np.amax(act))\n",
        "    for a in [0,1,2,3]:\n",
        "      if a in actf:\n",
        "        continue\n",
        "      else:\n",
        "        pi[i][a]=0\n",
        "    if all(np.equal(t,pi[i])):\n",
        "      continue\n",
        "    else:\n",
        "      pstable=False\n",
        "  iter2=iter2+1\n",
        "\n",
        "print('optimal policy is -')\n",
        "dictionary_items = pi.items()\n",
        "for item in dictionary_items:\n",
        "  print(item)\n",
        "print('the optimal value function is -')\n",
        "print(v)\n",
        "print('number of iterations of policy iteration is -',iter2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of iterations for policy eval - 86\n",
            "number of iterations for policy eval - 86\n",
            "optimal policy is -\n",
            "(0, array([0.        , 0.79844905, 0.        , 0.        ]))\n",
            "(1, array([0.72115789, 0.        , 0.        , 0.        ]))\n",
            "(2, array([0.        , 0.05779971, 0.        , 0.        ]))\n",
            "(3, array([0.85941686, 0.        , 0.        , 0.        ]))\n",
            "(4, array([0.        , 0.94274417, 0.        , 0.        ]))\n",
            "(5, array([0.38161744, 0.45021688, 0.71424303, 0.56828732]))\n",
            "(6, array([0.       , 0.2172288, 0.       , 0.       ]))\n",
            "(7, array([0.32993819, 0.6148518 , 0.59276764, 0.46789157]))\n",
            "(8, array([0.        , 0.        , 0.73765472, 0.        ]))\n",
            "(9, array([0.        , 0.        , 0.12043761, 0.        ]))\n",
            "(10, array([0.        , 0.99472237, 0.        , 0.        ]))\n",
            "(11, array([0.91105345, 0.29968833, 0.13129563, 0.50097267]))\n",
            "(12, array([0.32685073, 0.89981949, 0.77884258, 0.90280541]))\n",
            "(13, array([0.        , 0.        , 0.10249531, 0.        ]))\n",
            "(14, array([0.        , 0.        , 0.94600904, 0.        ]))\n",
            "(15, array([0.68070841, 0.91028395, 0.77903824, 0.31829969]))\n",
            "the optimal value function is -\n",
            "[5.86525155e-03 2.56763628e-03 3.65162843e-03 1.54657994e-03\n",
            " 8.59863475e-03 2.49629498e-05 3.60752150e-02 8.57883145e-05\n",
            " 1.57341543e-02 4.30549034e-02 2.79602809e-01 5.02380477e-06\n",
            " 2.42204886e-05 7.73535337e-02 5.00916142e-01 0.00000000e+00]\n",
            "number of iterations of policy iteration is - 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gX_uLL4Gwnm",
        "outputId": "10daa0ae-ed40-4a50-9e35-6914c5bd7cba"
      },
      "source": [
        "#value iteration\n",
        "\n",
        "#initilize parameters to perform policy evaluation\n",
        "delta=1\n",
        "gamma=0.9\n",
        "v=np.zeros(16)\n",
        "iter=0\n",
        "iter2=0\n",
        "\n",
        "#get the model of the environment\n",
        "lake=env.env.P\n",
        "\n",
        "#initialize the value function arbitarily\n",
        "for i in lake.keys():\n",
        "  v[i]=random.random()\n",
        "v[15]=0\n",
        "\n",
        "#initilize another randomized policy\n",
        "pi2={}\n",
        "for i in range (0,16):\n",
        "  pi2[i]=np.random.random(4)\n",
        "\n",
        "while delta>0.00001:\n",
        "  delta=0\n",
        "  for i in lake.keys():\n",
        "    t=v[i]\n",
        "    sum=np.zeros(4)\n",
        "    for a in [0,1,2,3]:\n",
        "      sum[a]= lake[i][a][0][0]*(lake[i][a][0][2]+ gamma*v[lake[i][a][0][1]])\n",
        "    v[i]=np.amax(sum)\n",
        "    delta=max(delta,abs(t-v[i]))\n",
        "  iter=iter+1\n",
        "\n",
        "for i in lake.keys():\n",
        "  act=np.zeros(4)\n",
        "  for a in [0,1,2,3]:\n",
        "    act[a]=lake[i][a][0][0]*(lake[i][a][0][2]+ gamma*v[lake[i][a][0][1]])\n",
        "  actf=np.argwhere(act == np.amax(act))\n",
        "  for a in [0,1,2,3]:\n",
        "    if a in actf:\n",
        "      continue\n",
        "    else:\n",
        "      pi2[i][a]=0\n",
        "\n",
        "print('optimal policy is -')\n",
        "dictionary_items = pi2.items()\n",
        "for item in dictionary_items:\n",
        "  print(item)\n",
        "print('the optimal value function is -')\n",
        "print(v)\n",
        "print('number of iterations for value eval -',iter)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "optimal policy is -\n",
            "(0, array([0.       , 0.7284041, 0.846667 , 0.       ]))\n",
            "(1, array([0.        , 0.        , 0.80054689, 0.        ]))\n",
            "(2, array([0.        , 0.59962053, 0.        , 0.        ]))\n",
            "(3, array([0.42755683, 0.        , 0.        , 0.        ]))\n",
            "(4, array([0.        , 0.46574072, 0.        , 0.        ]))\n",
            "(5, array([0.57100158, 0.23111283, 0.86816903, 0.42743008]))\n",
            "(6, array([0.      , 0.212586, 0.      , 0.      ]))\n",
            "(7, array([0.09324707, 0.29399932, 0.19733997, 0.05226935]))\n",
            "(8, array([0.        , 0.        , 0.67540654, 0.        ]))\n",
            "(9, array([0.        , 0.55886402, 0.29805347, 0.        ]))\n",
            "(10, array([0.        , 0.98100735, 0.        , 0.        ]))\n",
            "(11, array([0.43540658, 0.98592829, 0.83427157, 0.63779671]))\n",
            "(12, array([0.80541176, 0.04964795, 0.89761233, 0.45859569]))\n",
            "(13, array([0.        , 0.        , 0.88417979, 0.        ]))\n",
            "(14, array([0.        , 0.        , 0.95538488, 0.        ]))\n",
            "(15, array([0.18172176, 0.9555936 , 0.43075695, 0.22877406]))\n",
            "the optimal value function is -\n",
            "[5.90490000e-01 6.56100000e-01 7.29000000e-01 6.56100000e-01\n",
            " 6.56100000e-01 8.35237263e-05 8.10000000e-01 3.91012489e-06\n",
            " 7.29000000e-01 8.10000000e-01 9.00000000e-01 6.76193440e-05\n",
            " 3.50056204e-05 9.00000000e-01 1.00000000e+00 0.00000000e+00]\n",
            "number of iterations for value eval - 89\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtpxhfVnK8ac"
      },
      "source": [
        "**From the results of policy iteration we can see it went through 2 steps of policy iteration, where in each step had 86 iterations of policy evaluation to come to the optimal policy. But while using value iteration to approximate the policy we can see it only needed 89 iterations of the modified value evaluation to come up with an approximate optimal policy. This shows us that value iteration is much faster than policy iteration**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2TICbi6Mm2d"
      },
      "source": [
        "# Testing both the policies out"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZLT1-Sx-p_W",
        "outputId": "94145923-09cb-4f05-cbc8-a98081c7461e"
      },
      "source": [
        "#policy using policy iteration\n",
        "observation=env.reset()\n",
        "exitCommand=False\n",
        "env.render()\n",
        "while not(exitCommand):\n",
        "  userInput=random.choices((0,1,2,3),weights=pi[observation],k=1)\n",
        "  print(\"The action choosen is - \",userInput)\n",
        "  action=int(userInput[0])\n",
        "  (observation, reward, compute, probability) = env.step(action)\n",
        "  print(\"--> The result of taking action\",action,\"is:\")\n",
        "  print(\"     S=\",observation)\n",
        "  print(\"     R=\",reward)\n",
        "  print(\"     p=\",probability)\n",
        "\n",
        "  env.render()\n",
        "  if compute==True:\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "The action choosen is -  [1]\n",
            "--> The result of taking action 1 is:\n",
            "     S= 4\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "  (Down)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "The action choosen is -  [1]\n",
            "--> The result of taking action 1 is:\n",
            "     S= 8\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "The action choosen is -  [2]\n",
            "--> The result of taking action 2 is:\n",
            "     S= 9\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "F\u001b[41mF\u001b[0mFH\n",
            "HFFG\n",
            "The action choosen is -  [2]\n",
            "--> The result of taking action 2 is:\n",
            "     S= 10\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FF\u001b[41mF\u001b[0mH\n",
            "HFFG\n",
            "The action choosen is -  [1]\n",
            "--> The result of taking action 1 is:\n",
            "     S= 14\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HF\u001b[41mF\u001b[0mG\n",
            "The action choosen is -  [2]\n",
            "--> The result of taking action 2 is:\n",
            "     S= 15\n",
            "     R= 1.0\n",
            "     p= {'prob': 1.0}\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1U3oCtQKPAwx",
        "outputId": "7a655e8f-2907-4a57-fed6-34f26ec0de6d"
      },
      "source": [
        "#policy using value iteration\n",
        "observation=env.reset()\n",
        "exitCommand=False\n",
        "env.render()\n",
        "while not(exitCommand):\n",
        "  userInput=random.choices((0,1,2,3),weights=pi2[observation],k=1)\n",
        "  print(\"The action choosen is - \",userInput)\n",
        "  action=int(userInput[0])\n",
        "  (observation, reward, compute, probability) = env.step(action)\n",
        "  print(\"--> The result of taking action\",action,\"is:\")\n",
        "  print(\"     S=\",observation)\n",
        "  print(\"     R=\",reward)\n",
        "  print(\"     p=\",probability)\n",
        "\n",
        "  env.render()\n",
        "  if compute==True:\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "The action choosen is -  [2]\n",
            "--> The result of taking action 2 is:\n",
            "     S= 1\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "  (Right)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "The action choosen is -  [2]\n",
            "--> The result of taking action 2 is:\n",
            "     S= 2\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "  (Right)\n",
            "SF\u001b[41mF\u001b[0mF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "The action choosen is -  [1]\n",
            "--> The result of taking action 1 is:\n",
            "     S= 6\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "  (Down)\n",
            "SFFF\n",
            "FH\u001b[41mF\u001b[0mH\n",
            "FFFH\n",
            "HFFG\n",
            "The action choosen is -  [1]\n",
            "--> The result of taking action 1 is:\n",
            "     S= 10\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FF\u001b[41mF\u001b[0mH\n",
            "HFFG\n",
            "The action choosen is -  [1]\n",
            "--> The result of taking action 1 is:\n",
            "     S= 14\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HF\u001b[41mF\u001b[0mG\n",
            "The action choosen is -  [2]\n",
            "--> The result of taking action 2 is:\n",
            "     S= 15\n",
            "     R= 1.0\n",
            "     p= {'prob': 1.0}\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Csuf443OPJyB"
      },
      "source": [
        "**we can see that when either policy is used the agent is able to reach the goal. Hence we have verified both the policies work**"
      ]
    }
  ]
}