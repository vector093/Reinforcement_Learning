{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4b317e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16618a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\SOFTWARE\\Anaconda3\\lib\\site-packages\\gym\\core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "D:\\SOFTWARE\\Anaconda3\\lib\\site-packages\\gym\\wrappers\\step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "numberOfActions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d8a7316",
   "metadata": {},
   "outputs": [],
   "source": [
    "actionSpace = [0, 1, 2]\n",
    "numberOfStates = 20 ** 2\n",
    "GAMMA = 0.9\n",
    "ALPHA = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "beff5c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createBins():\n",
    "\n",
    "    bins = np.zeros((2, 20))\n",
    "    bins[0] = np.linspace(-1.20, 0.6, 20)\n",
    "    bins[1] = np.linspace(-0.07, 0.07, 20)\n",
    "    return bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a233d7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assignBins(observation, bins):\n",
    "    state = np.zeros(2)\n",
    "    for i in range(2):\n",
    "        state[i] = np.digitize(observation[i], bins[i])\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebb77d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_dict(d):\n",
    "    max_v = float('-inf')\n",
    "    for key, val in d.items():\n",
    "        if val > max_v:\n",
    "            max_v = val\n",
    "            max_key = key\n",
    "    return max_key, max_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5e6d489",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initilizing the action value function\n",
    "def initQ():\n",
    "  Q = {}\n",
    "\n",
    "  for i in range(20):\n",
    "    for j in range(20):\n",
    "      Q[(i,j)] = {}\n",
    "      for action in range(env.action_space.n):\n",
    "        Q[(i,j)][action] = float(0)\n",
    "  return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "04ef6212",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initilize a random deterministic target policy\n",
    "def initTargetPolicy():\n",
    "  pi = {}\n",
    "\n",
    "  for i in range(20):\n",
    "    for j in range(20):\n",
    "      pi[(i,j)]= np.random.choice(actionSpace)\n",
    "  return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6dfc39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initReturns():\n",
    "    returns = {}\n",
    "    \n",
    "    for i in range(20):\n",
    "        for j in range(20):\n",
    "            returns[(i,j)] = {}\n",
    "            for action in range(env.action_space.n):\n",
    "                returns[(i,j)][action] = []\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ecd9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def onPolicy_SARSA(env, Q, bins, eps):\n",
    "    obs = env.reset()\n",
    "    state = tuple(assignBins(obs, bins))\n",
    "    # prev_screen = env.render(mode='rgb_array')\n",
    "    # plt.imshow(prev_screen)\n",
    "    count = 0\n",
    "    complete = 0\n",
    "    while True:\n",
    "        count += 1\n",
    "        if np.random.uniform() < eps:\n",
    "            action = env.action_space.sample()  # epsilon greedy\n",
    "        else:\n",
    "            action = max_dict(Q[state])[0]\n",
    "\n",
    "        obs, reward, done, info = env.step(action)\n",
    "\n",
    "        if done == True:\n",
    "            if count < 200:\n",
    "                reward = 1000\n",
    "                complete = 1\n",
    "\n",
    "        new_state = tuple(assignBins(obs, bins))\n",
    "        if np.random.uniform() < eps:\n",
    "            new_action = env.action_space.sample()  # epsilon greedy\n",
    "        else:\n",
    "            new_action = max_dict(Q[state])[0]\n",
    "\n",
    "        Q[state][action] += ALPHA * (reward + GAMMA * Q[new_state][new_action] - Q[state][action])\n",
    "        state = new_state\n",
    "        if done == True:\n",
    "            break;\n",
    "    return complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "00c666af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_and_plot_parameters(cumulative_completion, training_episodes):\n",
    "    title = f'total episodes vs completed episodes'\n",
    "    print(f'Evaluated {title}')\n",
    "    line, = plt.plot(\n",
    "        np.arange(1, training_episodes + 1),\n",
    "        cumulative_completion,\n",
    "        label=title)\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5d661bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = createBins()\n",
    "Q = initQ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f4a32471",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "onPolicy_SARSA_completeList = []\n",
    "completed = 0\n",
    "eps = 0.05\n",
    "mem = 0\n",
    "training_episodes = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a36633",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(training_episodes):\n",
    "    complete = onPolicy_SARSA(env, Q, bins, eps)\n",
    "    if i % 100 == 0 and i != 0:\n",
    "        print(\"i = \" + str(i) + \", and completed over past 100 episode \" + str(completed - mem))\n",
    "        mem = completed\n",
    "\n",
    "    completed += complete\n",
    "    onPolicy_SARSA_completeList.append(completed)\n",
    "\n",
    "evaluate_and_plot_parameters(onPolicy_SARSA_completeList, training_episodes)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e481a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0\n",
    "avg = 0\n",
    "for i in range(100):\n",
    "    complete = Expected_SARSA(env, Q, bins, eps)\n",
    "    avg += complete\n",
    "print(avg / 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
