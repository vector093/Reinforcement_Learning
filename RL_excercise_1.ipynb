{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL excercise 1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5v9jkFGAQpF"
      },
      "source": [
        "# **Tic Tac Toe Agent**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZR896p2uOhbr"
      },
      "source": [
        "Important aspects of class state- maintains the state of the board\n",
        "\n",
        "*   The function hash assigns a unique values to every possible state the agent comes accross.\n",
        "*   the function is_end checks for a winner or if the game is tied and also sets the winner.\n",
        "\n",
        "*   get_all_states and get_all_states_impl gets all the possible states the board can be in\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iW-3okQWTIzs"
      },
      "source": [
        "#######################################################################\n",
        "# Copyright (C)                                                       #\n",
        "# 2016 - 2018 Shangtong Zhang(zhangshangtong.cpp@gmail.com)           #\n",
        "# 2016 Jan Hakenberg(jan.hakenberg@gmail.com)                         #\n",
        "# 2016 Tian Jun(tianjun.cpp@gmail.com)                                #\n",
        "# 2016 Kenta Shimada(hyperkentakun@gmail.com)                         #\n",
        "# Permission given to modify the code as long as you keep this        #\n",
        "# declaration at the top                                              #\n",
        "#######################################################################\n",
        "\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "BOARD_ROWS = 3\n",
        "BOARD_COLS = 3\n",
        "BOARD_SIZE = BOARD_ROWS * BOARD_COLS\n",
        "\n",
        "\n",
        "class State:\n",
        "    def __init__(self):\n",
        "        # the board is represented by an n * n array,\n",
        "        # 1 represents a chessman of the player who moves first,\n",
        "        # -1 represents a chessman of another player\n",
        "        # 0 represents an empty position\n",
        "        self.data = np.zeros((BOARD_ROWS, BOARD_COLS))\n",
        "        self.winner = None\n",
        "        self.hash_val = None\n",
        "        self.end = None\n",
        "\n",
        "    # compute the hash value for one state, it's unique\n",
        "    def hash(self):\n",
        "        if self.hash_val is None:\n",
        "            self.hash_val = 0\n",
        "            for i in np.nditer(self.data):\n",
        "                self.hash_val = self.hash_val * 3 + i + 1\n",
        "        return self.hash_val\n",
        "\n",
        "    # check whether a player has won the game, or it's a tie\n",
        "    def is_end(self):\n",
        "        if self.end is not None:\n",
        "            return self.end\n",
        "        results = []\n",
        "        # check row\n",
        "        for i in range(BOARD_ROWS):\n",
        "            results.append(np.sum(self.data[i, :]))\n",
        "        # check columns\n",
        "        for i in range(BOARD_COLS):\n",
        "            results.append(np.sum(self.data[:, i]))\n",
        "\n",
        "        # check diagonals\n",
        "        trace = 0\n",
        "        reverse_trace = 0\n",
        "        for i in range(BOARD_ROWS):\n",
        "            trace += self.data[i, i]\n",
        "            reverse_trace += self.data[i, BOARD_ROWS - 1 - i]\n",
        "        results.append(trace)\n",
        "        results.append(reverse_trace)\n",
        "\n",
        "        for result in results:\n",
        "            if result == 3:\n",
        "                self.winner = 1\n",
        "                self.end = True\n",
        "                return self.end\n",
        "            if result == -3:\n",
        "                self.winner = -1\n",
        "                self.end = True\n",
        "                return self.end\n",
        "\n",
        "        # whether it's a tie\n",
        "        sum_values = np.sum(np.abs(self.data))\n",
        "        if sum_values == BOARD_SIZE:\n",
        "            self.winner = 0\n",
        "            self.end = True\n",
        "            return self.end\n",
        "\n",
        "        # game is still going on\n",
        "        self.end = False\n",
        "        return self.end\n",
        "\n",
        "    # @symbol: 1 or -1\n",
        "    # put chessman symbol in position (i, j)\n",
        "    def next_state(self, i, j, symbol):\n",
        "        new_state = State()\n",
        "        new_state.data = np.copy(self.data)\n",
        "        new_state.data[i, j] = symbol\n",
        "        return new_state\n",
        "\n",
        "    # print the board\n",
        "    def print_state(self):\n",
        "        for i in range(BOARD_ROWS):\n",
        "            print('-------------')\n",
        "            out = '| '\n",
        "            for j in range(BOARD_COLS):\n",
        "                if self.data[i, j] == 1:\n",
        "                    token = '*'\n",
        "                elif self.data[i, j] == -1:\n",
        "                    token = 'x'\n",
        "                else:\n",
        "                    token = '0'\n",
        "                out += token + ' | '\n",
        "            print(out)\n",
        "        print('-------------')\n",
        "\n",
        "\n",
        "def get_all_states_impl(current_state, current_symbol, all_states):\n",
        "    for i in range(BOARD_ROWS):\n",
        "        for j in range(BOARD_COLS):\n",
        "            if current_state.data[i][j] == 0:\n",
        "                new_state = current_state.next_state(i, j, current_symbol)\n",
        "                new_hash = new_state.hash()\n",
        "                if new_hash not in all_states:\n",
        "                    is_end = new_state.is_end()\n",
        "                    all_states[new_hash] = (new_state, is_end)\n",
        "                    if not is_end:\n",
        "                        get_all_states_impl(new_state, -current_symbol, all_states)\n",
        "\n",
        "\n",
        "def get_all_states():\n",
        "    current_symbol = 1\n",
        "    current_state = State()\n",
        "    all_states = dict()\n",
        "    all_states[current_state.hash()] = (current_state, current_state.is_end())\n",
        "    get_all_states_impl(current_state, current_symbol, all_states)\n",
        "    return all_states\n",
        "\n",
        "\n",
        "# all possible board configurations\n",
        "all_states = get_all_states()   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQxGkBlgdvXx"
      },
      "source": [
        "Important aspects of class Judger- maintains the game by alternating turns between the players and checks if anyone has won"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qShGDXjjcU6c"
      },
      "source": [
        "class Judger:\n",
        "    # @player1: the player who will move first, its chessman will be 1\n",
        "    # @player2: another player with a chessman -1\n",
        "    def __init__(self, player1, player2):\n",
        "        self.p1 = player1\n",
        "        self.p2 = player2\n",
        "        self.current_player = None\n",
        "        self.p1_symbol = 1\n",
        "        self.p2_symbol = -1\n",
        "        self.p1.set_symbol(self.p1_symbol)\n",
        "        self.p2.set_symbol(self.p2_symbol)\n",
        "        self.current_state = State()\n",
        "\n",
        "    def reset(self):\n",
        "        self.p1.reset()\n",
        "        self.p2.reset()\n",
        "\n",
        "    def alternate(self):\n",
        "        while True:\n",
        "            yield self.p1\n",
        "            yield self.p2\n",
        "\n",
        "    # @print_state: if True, print each board during the game\n",
        "    def play(self, print_state=False):\n",
        "        alternator = self.alternate()\n",
        "        self.reset()\n",
        "        current_state = State()\n",
        "        self.p1.set_state(current_state)\n",
        "        self.p2.set_state(current_state)\n",
        "        if print_state:\n",
        "            current_state.print_state()\n",
        "        while True:\n",
        "            player = next(alternator)\n",
        "            i, j, symbol = player.act()\n",
        "            next_state_hash = current_state.next_state(i, j, symbol).hash()\n",
        "            current_state, is_end = all_states[next_state_hash]\n",
        "            self.p1.set_state(current_state)\n",
        "            self.p2.set_state(current_state)\n",
        "            if print_state:\n",
        "                current_state.print_state()\n",
        "            if is_end:\n",
        "                return current_state.winner"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7qJUShOfTlT"
      },
      "source": [
        "Important aspects of class player- maintains the agent\n",
        "\n",
        "*   The function set_symbol initilizes the the intial rewards of each state.\n",
        "*   the function backup updates the reward estimates with temporal difference learning.\n",
        "\n",
        "*   The Act function first takes into account all the available positions, then chooses whether to exploit or explore based on epsilon value- if random value genrated is less that epsilon it will explore else it will exploit. If we are to exploit we will take the action with the highest reward estimation. We modify this function to display when it exploits and when it explores, along with the value of the state"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNfeIVM2cbTU"
      },
      "source": [
        "# AI player\n",
        "class Player:\n",
        "    # @step_size: the step size to update estimations\n",
        "    # @epsilon: the probability to explore\n",
        "    def __init__(self, step_size=0.1, epsilon=0.1):\n",
        "        self.estimations = dict()\n",
        "        self.step_size = step_size\n",
        "        self.epsilon = epsilon\n",
        "        self.states = []\n",
        "        self.greedy = []\n",
        "        self.symbol = 0\n",
        "\n",
        "    def reset(self):\n",
        "        self.states = []\n",
        "        self.greedy = []\n",
        "\n",
        "    def set_state(self, state):\n",
        "        self.states.append(state)\n",
        "        self.greedy.append(True)\n",
        "\n",
        "    def set_symbol(self, symbol):\n",
        "        self.symbol = symbol\n",
        "        for hash_val in all_states:\n",
        "            state, is_end = all_states[hash_val]\n",
        "            if is_end:\n",
        "                if state.winner == self.symbol:\n",
        "                    self.estimations[hash_val] = 1.0\n",
        "                elif state.winner == 0:\n",
        "                    # we need to distinguish between a tie and a lose\n",
        "                    self.estimations[hash_val] = 0.5\n",
        "                else:\n",
        "                    self.estimations[hash_val] = 0\n",
        "            else:\n",
        "                self.estimations[hash_val] = 0.5\n",
        "\n",
        "    # update value estimation\n",
        "    def backup(self):\n",
        "        states = [state.hash() for state in self.states]\n",
        "\n",
        "        for i in reversed(range(len(states) - 1)):\n",
        "            state = states[i]\n",
        "            td_error = self.greedy[i] * (\n",
        "                self.estimations[states[i + 1]] - self.estimations[state]\n",
        "            )\n",
        "            self.estimations[state] += self.step_size * td_error\n",
        "\n",
        "    # choose an action based on the state\n",
        "    def act(self):\n",
        "        state = self.states[-1]\n",
        "        next_states = []\n",
        "        next_positions = []\n",
        "        for i in range(BOARD_ROWS):\n",
        "            for j in range(BOARD_COLS):\n",
        "                if state.data[i, j] == 0:\n",
        "                    next_positions.append([i, j])\n",
        "                    next_states.append(state.next_state(\n",
        "                        i, j, self.symbol).hash())\n",
        "\n",
        "        if np.random.rand() < self.epsilon:\n",
        "            action = next_positions[np.random.randint(len(next_positions))]\n",
        "            action.append(self.symbol)\n",
        "            self.greedy[-1] = False\n",
        "            print(\"the agent is exploring\")\n",
        "            return action\n",
        "\n",
        "        values = []\n",
        "        for hash_val, pos in zip(next_states, next_positions):\n",
        "            values.append((self.estimations[hash_val], pos))\n",
        "        # to select one of the actions of equal value at random due to Python's sort is stable\n",
        "        np.random.shuffle(values)\n",
        "        values.sort(key=lambda x: x[0], reverse=True)\n",
        "        action = values[0][1]\n",
        "        action.append(self.symbol)\n",
        "        print(\"the agent is exploiting\")\n",
        "        print('the value is-',values[0][0])\n",
        "        return action\n",
        "\n",
        "    def save_policy(self):\n",
        "        with open('policy_%s.bin' % ('first' if self.symbol == 1 else 'second'), 'wb') as f:\n",
        "            pickle.dump(self.estimations, f)\n",
        "\n",
        "    def load_policy(self):\n",
        "        with open('policy_%s.bin' % ('first' if self.symbol == 1 else 'second'), 'rb') as f:\n",
        "            self.estimations = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZqFcuzQhuGu"
      },
      "source": [
        "Important aspects of class humanplayer- takes input from the human player and sets the board"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oqM1IxhcoCi"
      },
      "source": [
        "# human interface\n",
        "# input a number to put a chessman\n",
        "# | q | w | e |\n",
        "# | a | s | d |\n",
        "# | z | x | c |\n",
        "class HumanPlayer:\n",
        "    def __init__(self, **kwargs):\n",
        "        self.symbol = None\n",
        "        self.keys = ['q', 'w', 'e', 'a', 's', 'd', 'z', 'x', 'c']\n",
        "        self.state = None\n",
        "\n",
        "    def reset(self):\n",
        "        pass\n",
        "\n",
        "    def set_state(self, state):\n",
        "        self.state = state\n",
        "\n",
        "    def set_symbol(self, symbol):\n",
        "        self.symbol = symbol\n",
        "\n",
        "    def act(self):\n",
        "        self.state.print_state()\n",
        "        key = input(\"Input your position:\")\n",
        "        data = self.keys.index(key)\n",
        "        i = data // BOARD_COLS\n",
        "        j = data % BOARD_COLS\n",
        "        return i, j, self.symbol"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJatcxiWpH1k"
      },
      "source": [
        "Train func- trains the agent by playing against it self over a large number of epochs.\n",
        "We modify the train function so the agent will train against a human player.\n",
        "\n",
        "Compete func- the two agents that played against each other compete against one another."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zK5HYpZCcxXC"
      },
      "source": [
        "def train(epochs, print_every_n=1):\n",
        "    player1 = Player(epsilon=0.1)\n",
        "    player2 = HumanPlayer()\n",
        "    judger = Judger(player1, player2)\n",
        "    player1_win = 0.0\n",
        "    player2_win = 0.0\n",
        "    for i in range(1, epochs + 1):\n",
        "        winner = judger.play(print_state=False)\n",
        "        if winner == 1:\n",
        "            player1_win += 1\n",
        "        if winner == -1:\n",
        "            player2_win += 1\n",
        "        if i % print_every_n == 0:\n",
        "            print('Epoch %d, player 1 winrate: %.02f, player 2 winrate: %.02f' % (i, player1_win / i, player2_win / i))\n",
        "        player1.backup()\n",
        "        #player2.backup()\n",
        "        judger.reset()\n",
        "    player1.save_policy()\n",
        "    #player2.save_policy()\n",
        "\n",
        "\n",
        "def compete(turns):\n",
        "    player1 = Player(epsilon=0)\n",
        "    player2 = Player(epsilon=0)\n",
        "    judger = Judger(player1, player2)\n",
        "    player1.load_policy()\n",
        "    player2.load_policy()\n",
        "    player1_win = 0.0\n",
        "    player2_win = 0.0\n",
        "    for _ in range(turns):\n",
        "        winner = judger.play()\n",
        "        if winner == 1:\n",
        "            player1_win += 1\n",
        "        if winner == -1:\n",
        "            player2_win += 1\n",
        "        judger.reset()\n",
        "    print('%d turns, player 1 win %.02f, player 2 win %.02f' % (turns, player1_win / turns, player2_win / turns))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaORQXrndEpi"
      },
      "source": [
        "# The game is a zero sum game. If both players are playing with an optimal strategy, every game will end in a tie.\n",
        "# So we test whether the AI can guarantee at least a tie if it goes second.\n",
        "def play():\n",
        "    while True:\n",
        "        player2 = HumanPlayer()\n",
        "        player1 = Player(epsilon=0)\n",
        "        judger = Judger(player1, player2)\n",
        "        player1.load_policy()\n",
        "        winner = judger.play()\n",
        "        if winner == player1.symbol:\n",
        "            print(\"You lose!\")\n",
        "        elif winner == player2.symbol:\n",
        "            print(\"You win!\")\n",
        "        else:\n",
        "            print(\"It is a tie!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzXaCz7udLHA",
        "outputId": "0b22e30a-86d3-47b1-d290-807782e9b163"
      },
      "source": [
        "train(int(10))\n",
        "#compete(int(100))\n",
        "#play()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the agent is exploring\n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| * | 0 | 0 | \n",
            "-------------\n",
            "Input your position:a\n",
            "the agent is exploring\n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| x | * | 0 | \n",
            "-------------\n",
            "| * | 0 | 0 | \n",
            "-------------\n",
            "Input your position:e\n",
            "the agent is exploiting\n",
            "the value is- 0.5\n",
            "-------------\n",
            "| 0 | 0 | x | \n",
            "-------------\n",
            "| x | * | 0 | \n",
            "-------------\n",
            "| * | 0 | * | \n",
            "-------------\n",
            "Input your position:x\n",
            "the agent is exploiting\n",
            "the value is- 1.0\n",
            "Epoch 1, player 1 winrate: 1.00, player 2 winrate: 0.00\n",
            "the agent is exploiting\n",
            "the value is- 0.5\n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| 0 | * | 0 | \n",
            "-------------\n",
            "Input your position:s\n",
            "the agent is exploiting\n",
            "the value is- 0.5\n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| 0 | x | * | \n",
            "-------------\n",
            "| 0 | * | 0 | \n",
            "-------------\n",
            "Input your position:c\n",
            "the agent is exploiting\n",
            "the value is- 0.5\n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| 0 | x | * | \n",
            "-------------\n",
            "| * | * | x | \n",
            "-------------\n",
            "Input your position:q\n",
            "Epoch 2, player 1 winrate: 0.50, player 2 winrate: 0.50\n",
            "the agent is exploiting\n",
            "the value is- 0.5\n",
            "-------------\n",
            "| 0 | 0 | * | \n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "Input your position:s\n",
            "the agent is exploiting\n",
            "the value is- 0.5\n",
            "-------------\n",
            "| 0 | 0 | * | \n",
            "-------------\n",
            "| 0 | x | * | \n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "Input your position:c\n",
            "the agent is exploring\n",
            "-------------\n",
            "| 0 | 0 | * | \n",
            "-------------\n",
            "| * | x | * | \n",
            "-------------\n",
            "| 0 | 0 | x | \n",
            "-------------\n",
            "Input your position:q\n",
            "Epoch 3, player 1 winrate: 0.33, player 2 winrate: 0.67\n",
            "the agent is exploring\n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| * | 0 | 0 | \n",
            "-------------\n",
            "Input your position:x\n",
            "the agent is exploiting\n",
            "the value is- 0.5\n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| 0 | * | 0 | \n",
            "-------------\n",
            "| * | x | 0 | \n",
            "-------------\n",
            "Input your position:e\n",
            "the agent is exploiting\n",
            "the value is- 0.5\n",
            "-------------\n",
            "| 0 | 0 | x | \n",
            "-------------\n",
            "| 0 | * | 0 | \n",
            "-------------\n",
            "| * | x | * | \n",
            "-------------\n",
            "Input your position:q\n",
            "the agent is exploiting\n",
            "the value is- 0.5\n",
            "-------------\n",
            "| x | * | x | \n",
            "-------------\n",
            "| 0 | * | 0 | \n",
            "-------------\n",
            "| * | x | * | \n",
            "-------------\n",
            "Input your position:a\n",
            "the agent is exploiting\n",
            "the value is- 0.5\n",
            "Epoch 4, player 1 winrate: 0.25, player 2 winrate: 0.50\n",
            "the agent is exploiting\n",
            "the value is- 0.5\n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| * | 0 | 0 | \n",
            "-------------\n",
            "Input your position:x\n",
            "the agent is exploiting\n",
            "the value is- 0.5\n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| * | 0 | 0 | \n",
            "-------------\n",
            "| * | x | 0 | \n",
            "-------------\n",
            "Input your position:s\n",
            "the agent is exploiting\n",
            "the value is- 1.0\n",
            "Epoch 5, player 1 winrate: 0.40, player 2 winrate: 0.40\n",
            "the agent is exploring\n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| 0 | 0 | * | \n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "Input your position:a\n",
            "the agent is exploring\n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| x | 0 | * | \n",
            "-------------\n",
            "| 0 | * | 0 | \n",
            "-------------\n",
            "Input your position:s\n",
            "the agent is exploiting\n",
            "the value is- 0.5\n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| x | x | * | \n",
            "-------------\n",
            "| * | * | 0 | \n",
            "-------------\n",
            "Input your position:c\n",
            "the agent is exploiting\n",
            "the value is- 0.5\n",
            "-------------\n",
            "| * | 0 | 0 | \n",
            "-------------\n",
            "| x | x | * | \n",
            "-------------\n",
            "| * | * | x | \n",
            "-------------\n",
            "Input your position:w\n",
            "the agent is exploiting\n",
            "the value is- 0.5\n",
            "Epoch 6, player 1 winrate: 0.33, player 2 winrate: 0.33\n",
            "the agent is exploiting\n",
            "the value is- 0.50005\n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| * | 0 | 0 | \n",
            "-------------\n",
            "Input your position:s\n",
            "the agent is exploring\n",
            "-------------\n",
            "| * | 0 | 0 | \n",
            "-------------\n",
            "| 0 | x | 0 | \n",
            "-------------\n",
            "| * | 0 | 0 | \n",
            "-------------\n",
            "Input your position:a\n",
            "the agent is exploiting\n",
            "the value is- 0.5\n",
            "-------------\n",
            "| * | 0 | * | \n",
            "-------------\n",
            "| x | x | 0 | \n",
            "-------------\n",
            "| * | 0 | 0 | \n",
            "-------------\n",
            "Input your position:d\n",
            "Epoch 7, player 1 winrate: 0.29, player 2 winrate: 0.43\n",
            "the agent is exploiting\n",
            "the value is- 0.500045\n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| * | 0 | 0 | \n",
            "-------------\n",
            "Input your position:a\n",
            "the agent is exploiting\n",
            "the value is- 0.50005\n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| x | * | 0 | \n",
            "-------------\n",
            "| * | 0 | 0 | \n",
            "-------------\n",
            "Input your position:e\n",
            "the agent is exploiting\n",
            "the value is- 0.505\n",
            "-------------\n",
            "| 0 | 0 | x | \n",
            "-------------\n",
            "| x | * | 0 | \n",
            "-------------\n",
            "| * | 0 | * | \n",
            "-------------\n",
            "Input your position:q\n",
            "the agent is exploiting\n",
            "the value is- 1.0\n",
            "Epoch 8, player 1 winrate: 0.38, player 2 winrate: 0.38\n",
            "the agent is exploiting\n",
            "the value is- 0.50004235\n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| * | 0 | 0 | \n",
            "-------------\n",
            "Input your position:s\n",
            "the agent is exploiting\n",
            "the value is- 0.5\n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| 0 | x | 0 | \n",
            "-------------\n",
            "| * | 0 | * | \n",
            "-------------\n",
            "Input your position:x\n",
            "the agent is exploiting\n",
            "the value is- 0.5\n",
            "-------------\n",
            "| 0 | * | 0 | \n",
            "-------------\n",
            "| 0 | x | 0 | \n",
            "-------------\n",
            "| * | x | * | \n",
            "-------------\n",
            "Input your position:d\n",
            "the agent is exploiting\n",
            "the value is- 0.5\n",
            "-------------\n",
            "| * | * | 0 | \n",
            "-------------\n",
            "| 0 | x | x | \n",
            "-------------\n",
            "| * | x | * | \n",
            "-------------\n",
            "Input your position:e\n",
            "the agent is exploiting\n",
            "the value is- 1.0\n",
            "Epoch 9, player 1 winrate: 0.44, player 2 winrate: 0.33\n",
            "the agent is exploiting\n",
            "the value is- 0.5000381199999999\n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| * | 0 | 0 | \n",
            "-------------\n",
            "Input your position:s\n",
            "the agent is exploiting\n",
            "the value is- 0.5000005\n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| 0 | x | 0 | \n",
            "-------------\n",
            "| * | 0 | * | \n",
            "-------------\n",
            "Input your position:x\n",
            "the agent is exploiting\n",
            "the value is- 0.50005\n",
            "-------------\n",
            "| 0 | * | 0 | \n",
            "-------------\n",
            "| 0 | x | 0 | \n",
            "-------------\n",
            "| * | x | * | \n",
            "-------------\n",
            "Input your position:d\n",
            "the agent is exploiting\n",
            "the value is- 0.505\n",
            "-------------\n",
            "| * | * | 0 | \n",
            "-------------\n",
            "| 0 | x | x | \n",
            "-------------\n",
            "| * | x | * | \n",
            "-------------\n",
            "Input your position:e\n",
            "the agent is exploiting\n",
            "the value is- 1.0\n",
            "Epoch 10, player 1 winrate: 0.50, player 2 winrate: 0.30\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZxaIubrEKB6"
      },
      "source": [
        "With higher epoch we can see the agents starts learning more possible states and actions that have a greater value. This can atrribute to the greater win rate of the agent."
      ]
    }
  ]
}